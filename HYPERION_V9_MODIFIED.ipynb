{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 146519,
          "status": "ok",
          "timestamp": 1763628605747,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "JRvsEFHImK7Y",
        "outputId": "7dc1b6df-9178-4713-b7cc-b34944500cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-20 08:48:52\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:55\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/profile/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/profile/CVS?apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:55\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 1_Profile\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:55\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/quote/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/quote/CVS?apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:55\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-20 08:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/income-statement/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/income-statement/CVS?period=annual&limit=10&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/balance-sheet-statement/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/balance-sheet-statement/CVS?period=annual&limit=10&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/cash-flow-statement/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/cash-flow-statement/CVS?period=annual&limit=10&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/key-metrics-ttm/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/key-metrics-ttm/CVS?limit=40&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/financial-growth/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/financial-growth/CVS?period=annual&limit=20&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/CVS?apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/CVS?apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/historical/earning_calendar/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/historical/earning_calendar/CVS?limit=12&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/institutional-holder/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/institutional-holder/CVS?limit=100&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/institutional-ownership/list: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/institutional-ownership/list?apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/senate-disclosure: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/senate-disclosure?symbol=CVS&limit=100&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/senate-trading: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/senate-trading?symbol=CVS&limit=100&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/house-disclosure: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/house-disclosure?symbol=CVS&limit=100&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/analyst-estimates/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/analyst-estimates/CVS?period=annual&limit=30&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 17_Analyst_Est\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/upgrades-downgrades: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/upgrades-downgrades?symbol=CVS&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/price-target: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/price-target?symbol=CVS&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/price-target-consensus: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/price-target-consensus?symbol=CVS&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data?symbol=CVS&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings?symbol=CVS&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/stable/commitment-of-traders-report: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/stable/commitment-of-traders-report?apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/stock_peers: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/stock_peers?symbol=CVS&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/stock_news: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/stock_news?tickers=CVS&limit=100&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v4/general_news: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v4/general_news?page=0&limit=50&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/press-releases/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/press-releases/CVS?limit=100&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/stable/sec-filings-search/form-type: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/stable/sec-filings-search/form-type?formType=8-K&limit=100&from=2024-11-20&to=2025-11-20&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/stable/sec-filings-search/form-type: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/stable/sec-filings-search/form-type?formType=10-K&limit=10&from=2024-11-20&to=2025-11-20&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/stable/sec-filings-search/form-type: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/stable/sec-filings-search/form-type?formType=10-Q&limit=10&from=2024-11-20&to=2025-11-20&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/stable/sec-filings-search/symbol: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/stable/sec-filings-search/symbol?symbol=CVS&limit=100&from=2024-11-20&to=2025-11-20&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mSin CIK; omitiendo búsqueda por CIK.\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:05\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2025&quarter=4&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:06\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2025&quarter=3&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:06\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2025&quarter=2&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:06\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2025&quarter=1&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:06\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2024&quarter=4&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:06\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2024&quarter=3&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:06\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2024&quarter=2&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:06\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS: 403 Client Error: Forbidden for url: https://financialmodelingprep.com/api/v3/earning_call_transcript/CVS?year=2024&quarter=1&apikey=16NIcRN6k424f0xRP7QlSiUwDk0vsRAp\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-1582635950.py:759: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-20 08:49:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121C00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121C00057500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251121C00057500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121C00057500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220C00085000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220C00085000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121C00082000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121C00082000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00055000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00055000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00055000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121C00077500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121C00077500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260102C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260102C00083000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260102C00083000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260102C00083000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00075000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00075000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00047500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00047500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00047500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00047500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00074000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00074000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00074000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121P00100000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121P00100000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251121P00100000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121P00100000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121P00076000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121P00076000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220C00075000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220C00075000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00072500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00072500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00072500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00077000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00077000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00072500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00072500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00072500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00077000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00077000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00076000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00076000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00080000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00080000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121C00080000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121C00080000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121C00047500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121C00047500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251121C00047500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121C00047500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00077000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00077000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00074000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00074000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00074000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00084000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00084000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205C00084000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00084000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00070000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00070000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251121C00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251121C00040000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251121C00040000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251121C00040000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00030000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00030000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00030000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:49:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00030000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:50:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:50:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00083000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:50:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00083000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:50:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-20 08:50:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00075000\u001b[0m\n",
            "/tmp/ipython-input-1582635950.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-20 08:50:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00075000\u001b[0m\n",
            "\u001b[32m2025-11-20 08:50:01\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-20 08:50:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 67.75 s\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# Polygon REST Client\n",
        "from polygon import RESTClient\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "polygon_client = RESTClient(POLY_KEY)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "\ndef get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt):\n    \"\"\"\n    Usa polygon.RESTClient para traer trades y quotes de un contrato de opciones\n    entre start_dt y end_dt (ambos datetime con tzinfo).\n    Devuelve (df_trades, df_quotes) como DataFrames de pandas.\n    \"\"\"\n    import pytz\n    start_utc = start_dt.astimezone(pytz.UTC).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    end_utc = end_dt.astimezone(pytz.UTC).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n    trades_list = []\n    for tr in polygon_client.list_trades(\n        options_ticker,\n        timestamp_gte=start_utc,\n        timestamp_lte=end_utc,\n        order=\"asc\",\n        limit=1000,\n    ):\n        trades_list.append({\n            \"price\": tr.price,\n            \"size\": tr.size,\n            \"exchange\": tr.exchange,\n            \"participant_timestamp\": tr.participant_timestamp,\n        })\n\n    quotes_list = []\n    for qt in polygon_client.list_quotes(\n        options_ticker,\n        timestamp_gte=start_utc,\n        timestamp_lte=end_utc,\n        order=\"asc\",\n        limit=1000,\n    ):\n        quotes_list.append({\n            \"bid_price\": qt.bid_price,\n            \"ask_price\": qt.ask_price,\n            \"bid_size\": qt.bid_size,\n            \"ask_size\": qt.ask_size,\n            \"exchange\": qt.bid_exchange,\n            \"sip_timestamp\": qt.sip_timestamp,\n        })\n\n    df_trades = pd.DataFrame(trades_list)\n    df_quotes = pd.DataFrame(quotes_list)\n    return df_trades, df_quotes\n\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "    # Si Polygon no devuelve trades para este contrato en la ventana consultada,\n",
        "    # no añadimos una fila falsa llena de ceros.\n",
        "    contracts_no_quotes += 1\n",
        "    continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            flow_results.append({\n",
        "                'contract': options_ticker,\n",
        "                'type': contract.get('contract_type'),\n",
        "                'strike': contract.get('strike_price'),\n",
        "                'expiration': contract.get('expiration_date'),\n",
        "                'imbalance': np.nan, 'buy_volume': 0, 'sell_volume': 0,\n",
        "                'total_volume_trades': 0,\n",
        "                'Volumen_Lit_%': 0.0, 'Volumen_Off_%': 0.0, 'Volumen_Unknown_%': 0.0,\n",
        "                'quotes_present': False, 'session_source': session_source\n",
        "            })\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra, spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain, spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 664,
          "status": "ok",
          "timestamp": 1763076829415,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "2SDeMStVGjy5",
        "outputId": "ecb37164-ed6c-4fdb-b7eb-3013ff47d16c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ HYPERION V9 Enhancement Module Loaded\n",
            "   All 10 advanced features implemented:\n",
            "   1. Expected Move Calculator\n",
            "   2. Comprehensive Greeks (1st, 2nd, 3rd order)\n",
            "   3. GEX Analysis\n",
            "   4. IV Term Structure\n",
            "   5. IV Skew 25-Delta\n",
            "   6. Smile Regression (SVI)\n",
            "   7. Unusual Options Activity Detection\n",
            "   8. Expected Earnings Gap\n",
            "   9. IV Crush History\n",
            "   10. Quantitative Scorecard\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "HYPERION V9 COMPLETE - Enhanced Options Analytics\n",
        "All 10 Advanced Features Implementation\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Any, Tuple, List\n",
        "from scipy import stats, optimize\n",
        "from scipy.stats import norm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 1: Expected Move Calculator\n",
        "# =========================\n",
        "def calculate_expected_move(\n",
        "    df_options: pd.DataFrame,\n",
        "    spot_price: float,\n",
        "    earnings_date: str = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate expected price moves using multiple methods:\n",
        "    1. ATM straddle pricing method\n",
        "    2. Implied volatility method\n",
        "    3. Multiple time horizons (daily, weekly, monthly, until earnings)\n",
        "\n",
        "    Args:\n",
        "        df_options: Options chain dataframe\n",
        "        spot_price: Current stock price\n",
        "        earnings_date: Next earnings date (optional)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with expected moves for different time horizons\n",
        "    \"\"\"\n",
        "    print(\"📊 Calculating Expected Move...\")\n",
        "\n",
        "    if df_options.empty or not spot_price:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Get unique expirations sorted by date\n",
        "    df_options = df_options.copy()\n",
        "    df_options['exp_date'] = pd.to_datetime(df_options['expiration_date'], errors='coerce')\n",
        "    expirations = (\n",
        "        df_options.dropna(subset=['exp_date'])\n",
        "        .sort_values('exp_date')['exp_date']\n",
        "        .unique()\n",
        "    )\n",
        "\n",
        "    earnings_dt = pd.to_datetime(earnings_date, errors='coerce') if earnings_date else pd.NaT\n",
        "\n",
        "    for exp_date in expirations:\n",
        "        exp_opts = df_options[df_options['exp_date'] == exp_date].copy()\n",
        "\n",
        "        # Calculate days to expiration\n",
        "        days_to_exp = (pd.to_datetime(exp_date) - datetime.now()).days\n",
        "        if days_to_exp <= 0:\n",
        "            continue\n",
        "\n",
        "        # Time to expiration in years\n",
        "        T = days_to_exp / 365.25\n",
        "\n",
        "        # Method 1: ATM Straddle Price\n",
        "        # Find ATM options (closest to spot)\n",
        "        exp_opts['distance_from_spot'] = (exp_opts['strike_price'] - spot_price).abs()\n",
        "        atm_idx = exp_opts['distance_from_spot'].idxmin() if not exp_opts.empty else None\n",
        "\n",
        "        if atm_idx is not None and pd.notna(atm_idx):\n",
        "            atm_strike = exp_opts.loc[atm_idx, 'strike_price']\n",
        "\n",
        "            # Get ATM call and put prices\n",
        "            atm_call = exp_opts[\n",
        "                (exp_opts['strike_price'] == atm_strike) &\n",
        "                (exp_opts['contract_type'].isin(['call', 'C']))\n",
        "            ]['close'].mean()\n",
        "\n",
        "            atm_put = exp_opts[\n",
        "                (exp_opts['strike_price'] == atm_strike) &\n",
        "                (exp_opts['contract_type'].isin(['put', 'P']))\n",
        "            ]['close'].mean()\n",
        "\n",
        "            if pd.notna(atm_call) and pd.notna(atm_put):\n",
        "                straddle_price = atm_call + atm_put\n",
        "                # Expected move ≈ 0.85 * straddle (aprox 1σ)\n",
        "                expected_move_dollar_straddle = straddle_price * 0.85\n",
        "                expected_move_pct_straddle = (expected_move_dollar_straddle / spot_price) * 100\n",
        "            else:\n",
        "                expected_move_dollar_straddle = np.nan\n",
        "                expected_move_pct_straddle = np.nan\n",
        "        else:\n",
        "            atm_strike = np.nan\n",
        "            expected_move_dollar_straddle = np.nan\n",
        "            expected_move_pct_straddle = np.nan\n",
        "\n",
        "        # Method 2: Implied Volatility (ATM por delta)\n",
        "        calls = exp_opts[exp_opts['contract_type'].isin(['call', 'C'])]\n",
        "        puts  = exp_opts[exp_opts['contract_type'].isin(['put', 'P'])]\n",
        "\n",
        "        iv_call = np.nan\n",
        "        iv_put  = np.nan\n",
        "\n",
        "        if not calls.empty and 'delta' in calls.columns:\n",
        "            atm_call_iv = calls.iloc[(calls['delta'] - 0.5).abs().argsort()[:1]]['iv'].values\n",
        "            if len(atm_call_iv) > 0:\n",
        "                iv_call = atm_call_iv[0]\n",
        "                if iv_call > 5:  # si viene en porcentaje (p.ej. 28)\n",
        "                    iv_call = iv_call / 100.0\n",
        "\n",
        "        if not puts.empty and 'delta' in puts.columns:\n",
        "            atm_put_iv = puts.iloc[(puts['delta'] + 0.5).abs().argsort()[:1]]['iv'].values\n",
        "            if len(atm_put_iv) > 0:\n",
        "                iv_put = atm_put_iv[0]\n",
        "                if iv_put > 5:  # si viene en porcentaje\n",
        "                    iv_put = iv_put / 100.0\n",
        "\n",
        "        # ATM IV promedio\n",
        "        if pd.notna(iv_call) and pd.notna(iv_put):\n",
        "            atm_iv = (iv_call + iv_put) / 2\n",
        "        elif pd.notna(iv_call):\n",
        "            atm_iv = iv_call\n",
        "        elif pd.notna(iv_put):\n",
        "            atm_iv = iv_put\n",
        "        else:\n",
        "            atm_iv = np.nan\n",
        "\n",
        "        # Expected move por IV (1σ)\n",
        "        if pd.notna(atm_iv) and atm_iv > 0:\n",
        "            expected_move_dollar_iv = spot_price * atm_iv * np.sqrt(T)\n",
        "            expected_move_pct_iv = atm_iv * np.sqrt(T) * 100\n",
        "        else:\n",
        "            expected_move_dollar_iv = np.nan\n",
        "            expected_move_pct_iv = np.nan\n",
        "\n",
        "        # Clasificación de horizonte\n",
        "        if days_to_exp <= 7:\n",
        "            horizon = 'Daily/Weekly'\n",
        "        elif days_to_exp <= 30:\n",
        "            horizon = 'Weekly/Monthly'\n",
        "        elif days_to_exp <= 45:\n",
        "            horizon = 'Monthly'\n",
        "        else:\n",
        "            horizon = 'Quarterly'\n",
        "\n",
        "        # ¿Expiración cercana a earnings?\n",
        "        is_earnings_expiry = False\n",
        "        if pd.notna(earnings_dt):\n",
        "            days_diff = abs((pd.to_datetime(exp_date) - earnings_dt).days)\n",
        "            is_earnings_expiry = days_diff <= 7\n",
        "\n",
        "        results.append({\n",
        "            'expiration_date': pd.to_datetime(exp_date).strftime('%Y-%m-%d'),\n",
        "            'days_to_expiration': days_to_exp,\n",
        "            'time_horizon': horizon,\n",
        "            'is_earnings_expiry': is_earnings_expiry,\n",
        "            'atm_strike': atm_strike if pd.notna(atm_strike) else np.nan,\n",
        "            'atm_iv': atm_iv if pd.notna(atm_iv) else np.nan,\n",
        "            'expected_move_$_straddle': round(expected_move_dollar_straddle, 2) if pd.notna(expected_move_dollar_straddle) else np.nan,\n",
        "            'expected_move_%_straddle': round(expected_move_pct_straddle, 2) if pd.notna(expected_move_pct_straddle) else np.nan,\n",
        "            'expected_move_$_iv': round(expected_move_dollar_iv, 2) if pd.notna(expected_move_dollar_iv) else np.nan,\n",
        "            'expected_move_%_iv': round(expected_move_pct_iv, 2) if pd.notna(expected_move_pct_iv) else np.nan,\n",
        "            'move_range_high_$': round(spot_price + expected_move_dollar_iv, 2) if pd.notna(expected_move_dollar_iv) else np.nan,\n",
        "            'move_range_low_$':  round(spot_price - expected_move_dollar_iv, 2) if pd.notna(expected_move_dollar_iv) else np.nan,\n",
        "        })\n",
        "\n",
        "    df_expected_move = pd.DataFrame(results)\n",
        "    if not df_expected_move.empty:\n",
        "        print(f\"✅ Expected Move calculated for {len(df_expected_move)} expirations\")\n",
        "    return df_expected_move\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 2: Second and Third Order Greeks (Enhanced)\n",
        "# =========================\n",
        "def calculate_comprehensive_greeks(\n",
        "    df_options: pd.DataFrame,\n",
        "    spot: float,\n",
        "    rfr: float = 0.05,\n",
        "    div_yield: float = 0.0\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate comprehensive Greeks including 1st, 2nd, and 3rd order:\n",
        "    - 1st Order: Delta, Vega, Theta, Rho\n",
        "    - 2nd Order: Gamma, Vanna, Volga (Vomma), Charm\n",
        "    - 3rd Order: Speed, Color, Ultima, Veta\n",
        "    Using Black-Scholes-Merton closed-form solutions for accuracy\n",
        "    \"\"\"\n",
        "    print(\"🔢 Calculating Comprehensive Greeks (1st, 2nd, 3rd order)...\")\n",
        "\n",
        "    if df_options.empty or not spot:\n",
        "        return df_options\n",
        "\n",
        "    df = df_options.copy()\n",
        "\n",
        "    # Ensure numeric columns\n",
        "    for col in ['strike_price', 'iv', 'close', 'open_interest', 'volume']:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Parse expiration dates\n",
        "    df['exp_date'] = pd.to_datetime(df['expiration_date'], errors='coerce')\n",
        "    df = df.dropna(subset=['exp_date', 'strike_price', 'iv'])\n",
        "\n",
        "    # Calculate time to expiration in years\n",
        "    df['T'] = (df['exp_date'] - datetime.now()).dt.total_seconds() / (365.25 * 24 * 3600)\n",
        "    df = df[df['T'] > 0]  # Remove expired options\n",
        "\n",
        "    # Normalize IV (convert percent → decimal if needed)\n",
        "    df['sigma'] = df['iv'].apply(lambda x: x / 100 if x >= 5 else x)\n",
        "\n",
        "    # Initialize columns\n",
        "    greek_cols = [\n",
        "        'delta', 'gamma', 'vega', 'theta', 'rho',        # 1st order\n",
        "        'vanna', 'vomma', 'charm', 'veta',               # 2nd order\n",
        "        'speed', 'color', 'ultima'                       # 3rd order\n",
        "    ]\n",
        "    for c in greek_cols:\n",
        "        df[c + '_calc'] = np.nan\n",
        "\n",
        "    # Row-wise computation\n",
        "    for idx, row in df.iterrows():\n",
        "        S = float(spot)\n",
        "        K = row['strike_price']\n",
        "        T = row['T']\n",
        "        r = float(rfr)\n",
        "        q = float(div_yield)\n",
        "        sigma = row['sigma']\n",
        "        opt_type = str(row.get('contract_type', 'call')).lower()\n",
        "\n",
        "        if any(pd.isna([S, K, T, r, sigma])) or T <= 0 or sigma <= 0:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
        "            d2 = d1 - sigma * np.sqrt(T)\n",
        "\n",
        "            N_d1 = norm.cdf(d1)\n",
        "            N_d2 = norm.cdf(d2)\n",
        "            n_d1 = norm.pdf(d1)\n",
        "\n",
        "            # 1st Order\n",
        "            if opt_type in ['call', 'c']:\n",
        "                delta = np.exp(-q * T) * N_d1\n",
        "                theta = ((-S * n_d1 * sigma * np.exp(-q * T)) / (2 * np.sqrt(T))\n",
        "                         - r * K * np.exp(-r * T) * N_d2\n",
        "                         + q * S * np.exp(-q * T) * N_d1) / 365.0\n",
        "                rho = K * T * np.exp(-r * T) * N_d2 / 100.0\n",
        "            else:  # put\n",
        "                delta = -np.exp(-q * T) * norm.cdf(-d1)\n",
        "                theta = ((-S * n_d1 * sigma * np.exp(-q * T)) / (2 * np.sqrt(T))\n",
        "                         + r * K * np.exp(-r * T) * norm.cdf(-d2)\n",
        "                         - q * S * np.exp(-q * T) * norm.cdf(-d1)) / 365.0\n",
        "                rho = -K * T * np.exp(-r * T) * norm.cdf(-d2) / 100.0\n",
        "\n",
        "            gamma = (np.exp(-q * T) * n_d1) / (S * sigma * np.sqrt(T))\n",
        "            vega  = S * np.exp(-q * T) * n_d1 * np.sqrt(T) / 100.0\n",
        "\n",
        "            # 2nd Order\n",
        "            vanna = -(np.exp(-q * T) * n_d1 * d2 / sigma) / 100.0\n",
        "            vomma = S * np.exp(-q * T) * n_d1 * np.sqrt(T) * (d1 * d2 / sigma) / 10000.0\n",
        "            charm = -q * np.exp(-q * T) * N_d1 - np.exp(-q * T) * n_d1 * (\n",
        "                2 * (r - q) * T - d2 * sigma * np.sqrt(T)\n",
        "            ) / (2 * T * sigma * np.sqrt(T))\n",
        "            charm = charm / 365.0\n",
        "            veta  = S * np.exp(-q * T) * n_d1 * np.sqrt(T) * (\n",
        "                q + ((r - q) * d1 / (sigma * np.sqrt(T))) - ((1 + d1 * d2) / (2 * T))\n",
        "            )\n",
        "            veta = veta / 365.0\n",
        "\n",
        "            # 3rd Order\n",
        "            speed = -(gamma / S) * (1 + (d1 / (sigma * np.sqrt(T))))\n",
        "            color = -np.exp(-q * T) * n_d1 / (2 * S * T * sigma * np.sqrt(T)) * (\n",
        "                2 * q * T + 1 + d1 * (2 * (r - q) * T - d2 * sigma * np.sqrt(T)) / (sigma * np.sqrt(T))\n",
        "            )\n",
        "            color = color / 365.0\n",
        "            ultima = -(vega / (sigma ** 2)) * (d1 * d2 * (1 - d1 * d2) + d1 ** 2 + d2 ** 2)\n",
        "\n",
        "            # Store\n",
        "            df.at[idx, 'delta_calc'] = delta\n",
        "            df.at[idx, 'gamma_calc'] = gamma\n",
        "            df.at[idx, 'vega_calc']  = vega\n",
        "            df.at[idx, 'theta_calc'] = theta\n",
        "            df.at[idx, 'rho_calc']   = rho\n",
        "            df.at[idx, 'vanna_calc'] = vanna\n",
        "            df.at[idx, 'vomma_calc'] = vomma\n",
        "            df.at[idx, 'charm_calc'] = charm\n",
        "            df.at[idx, 'veta_calc']  = veta\n",
        "            df.at[idx, 'speed_calc'] = speed\n",
        "            df.at[idx, 'color_calc'] = color\n",
        "            df.at[idx, 'ultima_calc'] = ultima\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Greeks calculated for {len(df)} contracts\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 3: GEX (Gamma Exposure) Analysis Enhanced\n",
        "# =========================\n",
        "def calculate_gex_comprehensive(df_options: pd.DataFrame, spot: float) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Comprehensive GEX analysis:\n",
        "    - Total GEX by expiration and strike\n",
        "    - Gamma flip identification\n",
        "    - Support/resistance levels\n",
        "    - Visualization data for GEX walls\n",
        "    \"\"\"\n",
        "    print(\"💥 Calculating Comprehensive GEX Analysis...\")\n",
        "\n",
        "    if df_options.empty or not spot:\n",
        "        return {}\n",
        "\n",
        "    df = df_options.copy()\n",
        "\n",
        "    # Ensure numeric columns\n",
        "    for col in ['gamma_calc', 'gamma', 'open_interest', 'strike_price']:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Use calculated gamma if available\n",
        "    if 'gamma_calc' in df.columns:\n",
        "        df['gamma_use'] = df['gamma_calc'].fillna(df.get('gamma', 0))\n",
        "    else:\n",
        "        df['gamma_use'] = df.get('gamma', 0)\n",
        "\n",
        "    df['open_interest'] = df.get('open_interest', 0)\n",
        "\n",
        "    # Dealer positioning sign (dealers short calls, long puts)\n",
        "    df['contract_type'] = df.get('contract_type', '').astype(str).str.lower()\n",
        "    df['dealer_sign'] = df['contract_type'].map({'call': -1, 'c': -1, 'put': 1, 'p': 1}).fillna(0)\n",
        "\n",
        "    # GEX in shares (por $1 de movimiento)\n",
        "    df['gex_shares'] = df['gamma_use'] * df['open_interest'] * 100.0 * df['dealer_sign']\n",
        "\n",
        "    # GEX notional (escala por spot^2; aquí en miles de millones)\n",
        "    df['gex_notional'] = df['gex_shares'] * float(spot) * float(spot) / 1e9  # Billions\n",
        "\n",
        "    # GEX by Strike\n",
        "    gex_by_strike = (\n",
        "        df.groupby('strike_price')\n",
        "        .agg({\n",
        "            'gex_shares': 'sum',\n",
        "            'gex_notional': 'sum',\n",
        "            'open_interest': 'sum'\n",
        "        })\n",
        "        .reset_index()\n",
        "        .sort_values('strike_price')\n",
        "    )\n",
        "    gex_by_strike['cumulative_gex'] = gex_by_strike['gex_shares'].cumsum()\n",
        "\n",
        "    # Gamma Flip (cruce de signo del acumulado)\n",
        "    gex_by_strike['sign_change'] = (\n",
        "        gex_by_strike['cumulative_gex'] * gex_by_strike['cumulative_gex'].shift(1)\n",
        "    ) < 0\n",
        "    flip_points = gex_by_strike[gex_by_strike['sign_change'] == True]\n",
        "\n",
        "    if not flip_points.empty:\n",
        "        gamma_flip = float(flip_points['strike_price'].iloc[0])\n",
        "        flip_interpretation = \"Above flip = negative gamma (volatility dampening), Below = positive gamma (volatility amplifying)\"\n",
        "    else:\n",
        "        gamma_flip = \"No flip found\"\n",
        "        flip_interpretation = \"No clear gamma flip level identified\"\n",
        "\n",
        "    # GEX by Expiration\n",
        "    gex_by_exp = (\n",
        "        df.groupby('expiration_date')\n",
        "        .agg({'gex_shares': 'sum', 'gex_notional': 'sum', 'open_interest': 'sum'})\n",
        "        .reset_index()\n",
        "    )\n",
        "    gex_by_exp['days_to_exp'] = (\n",
        "        pd.to_datetime(gex_by_exp['expiration_date']) - datetime.now()\n",
        "    ).dt.days\n",
        "    gex_by_exp = gex_by_exp.sort_values('days_to_exp')\n",
        "\n",
        "    # Major GEX walls\n",
        "    gex_by_strike['abs_gex'] = gex_by_strike['gex_shares'].abs()\n",
        "    gex_walls = gex_by_strike.nlargest(\n",
        "        10, 'abs_gex'\n",
        "    )[['strike_price', 'gex_shares', 'gex_notional', 'open_interest']]\n",
        "    gex_walls['wall_type'] = gex_walls['gex_shares'].apply(lambda x: 'Support' if x > 0 else 'Resistance')\n",
        "\n",
        "    # Totales\n",
        "    total_gex_shares = df['gex_shares'].sum()\n",
        "    total_gex_notional = df['gex_notional'].sum()\n",
        "    total_call_gex = df[df['contract_type'].isin(['call', 'c'])]['gex_shares'].sum()\n",
        "    total_put_gex  = df[df['contract_type'].isin(['put', 'p'])]['gex_shares'].sum()\n",
        "\n",
        "    # Posicionamiento del mercado\n",
        "    if total_gex_shares > 0:\n",
        "        positioning = \"Dealers LONG gamma (market stabilizing)\"\n",
        "    elif total_gex_shares < 0:\n",
        "        positioning = \"Dealers SHORT gamma (market volatile)\"\n",
        "    else:\n",
        "        positioning = \"Neutral gamma positioning\"\n",
        "\n",
        "    print(\"✅ GEX Analysis Complete:\")\n",
        "    print(f\"   Total GEX: {total_gex_shares:,.0f} shares ({total_gex_notional:.2f}B)\")\n",
        "    print(f\"   Gamma Flip: {gamma_flip}\")\n",
        "    print(f\"   Positioning: {positioning}\")\n",
        "\n",
        "    return {\n",
        "        'gex_by_strike': gex_by_strike,\n",
        "        'gex_by_expiration': gex_by_exp,\n",
        "        'gex_walls': gex_walls,\n",
        "        'summary': pd.DataFrame([{\n",
        "            'total_gex_shares': total_gex_shares,\n",
        "            'total_gex_notional_$B': total_gex_notional,\n",
        "            'total_call_gex': total_call_gex,\n",
        "            'total_put_gex': total_put_gex,\n",
        "            'gamma_flip_strike': gamma_flip,\n",
        "            'current_spot': float(spot),\n",
        "            'positioning': positioning,\n",
        "            'flip_interpretation': flip_interpretation\n",
        "        }])\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 4: IV Term Structure Enhanced\n",
        "# =========================\n",
        "def calculate_iv_term_structure_enhanced(df_options: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Enhanced IV Term Structure analysis:\n",
        "    - ATM IV across all expirations\n",
        "    - Contango/backwardation detection\n",
        "    - Term structure anomalies\n",
        "    - Calendar spread opportunities\n",
        "    \"\"\"\n",
        "    print(\"📈 Calculating Enhanced IV Term Structure...\")\n",
        "\n",
        "    if df_options.empty:\n",
        "        return {}\n",
        "\n",
        "    df = df_options.copy()\n",
        "\n",
        "    # Required cols\n",
        "    if ('iv' not in df.columns) or ('expiration_date' not in df.columns):\n",
        "        return {}\n",
        "\n",
        "    df['exp_date'] = pd.to_datetime(df['expiration_date'], errors='coerce')\n",
        "    df = df.dropna(subset=['exp_date'])\n",
        "    df['days_to_exp'] = (df['exp_date'] - datetime.now()).dt.days\n",
        "    df = df[df['days_to_exp'] > 0]\n",
        "\n",
        "    # Normalize IV\n",
        "    df['iv_normalized'] = df['iv'].apply(lambda x: x / 100.0 if x >= 5 else x)\n",
        "\n",
        "    # Calls / Puts\n",
        "    calls = df[df['contract_type'].isin(['call', 'C'])]\n",
        "    puts  = df[df['contract_type'].isin(['put',  'P'])]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    for exp_date in sorted(df['exp_date'].unique()):\n",
        "        exp_calls = calls[calls['exp_date'] == exp_date]\n",
        "        exp_puts  = puts[puts['exp_date']  == exp_date]\n",
        "        days_to_exp = (exp_date - datetime.now()).days\n",
        "\n",
        "        # ATM IV por delta\n",
        "        atm_iv_call = np.nan\n",
        "        atm_iv_put  = np.nan\n",
        "\n",
        "        if not exp_calls.empty and 'delta' in exp_calls.columns:\n",
        "            atm_call = exp_calls.iloc[(exp_calls['delta'] - 0.5).abs().argsort()[:1]]\n",
        "            if not atm_call.empty:\n",
        "                atm_iv_call = atm_call['iv_normalized'].iloc[0]\n",
        "\n",
        "        if not exp_puts.empty and 'delta' in exp_puts.columns:\n",
        "            atm_put = exp_puts.iloc[(exp_puts['delta'] + 0.5).abs().argsort()[:1]]\n",
        "            if not atm_put.empty:\n",
        "                atm_iv_put = atm_put['iv_normalized'].iloc[0]\n",
        "\n",
        "        # Promedio ATM\n",
        "        if pd.notna(atm_iv_call) and pd.notna(atm_iv_put):\n",
        "            atm_iv_avg = (atm_iv_call + atm_iv_put) / 2.0\n",
        "        elif pd.notna(atm_iv_call):\n",
        "            atm_iv_avg = atm_iv_call\n",
        "        elif pd.notna(atm_iv_put):\n",
        "            atm_iv_avg = atm_iv_put\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        results.append({\n",
        "            'expiration_date': pd.to_datetime(exp_date).strftime('%Y-%m-%d'),\n",
        "            'days_to_exp': days_to_exp,\n",
        "            'atm_iv_call': atm_iv_call,\n",
        "            'atm_iv_put': atm_iv_put,\n",
        "            'atm_iv_avg': atm_iv_avg,\n",
        "            'annualized_iv': atm_iv_avg\n",
        "        })\n",
        "\n",
        "    df_term = pd.DataFrame(results).sort_values('days_to_exp')\n",
        "    if df_term.empty:\n",
        "        return {}\n",
        "\n",
        "    # Slope\n",
        "    df_term['iv_slope'] = df_term['atm_iv_avg'].diff() / df_term['days_to_exp'].diff()\n",
        "\n",
        "    # Estructura\n",
        "    df_term['structure'] = 'Flat'\n",
        "    df_term.loc[df_term['iv_slope'] >  0.0001, 'structure'] = 'Contango'\n",
        "    df_term.loc[df_term['iv_slope'] < -0.0001, 'structure'] = 'Backwardation'\n",
        "\n",
        "    # Anomalías\n",
        "    iv_mean = df_term['atm_iv_avg'].mean()\n",
        "    iv_std  = df_term['atm_iv_avg'].std()\n",
        "    df_term['is_anomaly'] = (df_term['atm_iv_avg'] - iv_mean).abs() > (2 * iv_std)\n",
        "    df_term['anomaly_type'] = ''\n",
        "    df_term.loc[(df_term['is_anomaly']) & (df_term['atm_iv_avg'] > iv_mean), 'anomaly_type'] = 'IV Spike (Earnings?)'\n",
        "    df_term.loc[(df_term['is_anomaly']) & (df_term['atm_iv_avg'] < iv_mean), 'anomaly_type'] = 'IV Crush'\n",
        "\n",
        "    # Calendar spreads (estructura empinada)\n",
        "    if len(df_term) >= 2:\n",
        "        df_term['calendar_opp'] = False\n",
        "        for i in range(len(df_term) - 1):\n",
        "            iv_diff = df_term.iloc[i+1]['atm_iv_avg'] - df_term.iloc[i]['atm_iv_avg']\n",
        "            if abs(iv_diff) > 0.05:\n",
        "                df_term.loc[df_term.index[i],   'calendar_opp'] = True\n",
        "                df_term.loc[df_term.index[i+1], 'calendar_opp'] = True\n",
        "\n",
        "    summary = {\n",
        "        'current_short_term_iv': df_term.iloc[0]['atm_iv_avg']  if len(df_term) > 0 else np.nan,\n",
        "        'current_long_term_iv':  df_term.iloc[-1]['atm_iv_avg'] if len(df_term) > 0 else np.nan,\n",
        "        'term_structure_type':   (df_term['structure'].mode()[0] if not df_term.empty else 'Unknown'),\n",
        "        'avg_iv_slope':          df_term['iv_slope'].mean(),\n",
        "        'anomalies_detected':    int(df_term['is_anomaly'].sum()),\n",
        "        'calendar_opportunities': int(df_term.get('calendar_opp', pd.Series([False])).sum())\n",
        "    }\n",
        "\n",
        "    print(f\"✅ IV Term Structure: {summary['term_structure_type']}, {summary['anomalies_detected']} anomalies\")\n",
        "\n",
        "    return {\n",
        "        'term_structure': df_term,\n",
        "        'summary': pd.DataFrame([summary])\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 5: IV Skew 25-Delta Enhanced\n",
        "# =========================\n",
        "def calculate_iv_skew_25delta_enhanced(df_options: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Enhanced 25-Delta IV Skew analysis:\n",
        "    - Risk reversal (25d call IV - 25d put IV)\n",
        "    - Butterfly spread\n",
        "    - Skew evolution across expirations\n",
        "    - Put/call skew asymmetry\n",
        "    \"\"\"\n",
        "    print(\"📐 Calculating Enhanced 25-Delta IV Skew...\")\n",
        "\n",
        "    if df_options.empty or 'delta' not in df_options.columns:\n",
        "        return {}\n",
        "\n",
        "    df = df_options.copy()\n",
        "\n",
        "    # Normalize IV\n",
        "    df['iv_normalized'] = df['iv'].apply(lambda x: x / 100.0 if x >= 5 else x)\n",
        "\n",
        "    # Parse dates\n",
        "    df['exp_date'] = pd.to_datetime(df['expiration_date'], errors='coerce')\n",
        "    df = df.dropna(subset=['exp_date'])\n",
        "    df['days_to_exp'] = (df['exp_date'] - datetime.now()).dt.days\n",
        "    df = df[df['days_to_exp'] > 0]\n",
        "\n",
        "    calls = df[df['contract_type'].isin(['call', 'C'])]\n",
        "    puts  = df[df['contract_type'].isin(['put',  'P'])]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    for exp_date in sorted(df['exp_date'].unique()):\n",
        "        exp_calls = calls[calls['exp_date'] == exp_date]\n",
        "        exp_puts  = puts[puts['exp_date']  == exp_date]\n",
        "\n",
        "        days_to_exp = (exp_date - datetime.now()).days\n",
        "\n",
        "        # 25-delta options\n",
        "        call_25d = exp_calls.iloc[(exp_calls['delta'] - 0.25).abs().argsort()[:1]] if not exp_calls.empty else pd.DataFrame()\n",
        "        put_25d  = exp_puts.iloc[(exp_puts['delta']  + 0.25).abs().argsort()[:1]] if not exp_puts.empty  else pd.DataFrame()\n",
        "\n",
        "        # 50-delta (ATM)\n",
        "        call_50d = exp_calls.iloc[(exp_calls['delta'] - 0.50).abs().argsort()[:1]] if not exp_calls.empty else pd.DataFrame()\n",
        "        put_50d  = exp_puts.iloc[(exp_puts['delta']  + 0.50).abs().argsort()[:1]] if not exp_puts.empty  else pd.DataFrame()\n",
        "\n",
        "        if call_25d.empty or put_25d.empty:\n",
        "            continue\n",
        "\n",
        "        iv_call_25d = call_25d['iv_normalized'].iloc[0]\n",
        "        iv_put_25d  = put_25d['iv_normalized'].iloc[0]\n",
        "\n",
        "        # Risk Reversal\n",
        "        risk_reversal = iv_put_25d - iv_call_25d\n",
        "\n",
        "        # Butterfly (convexidad de smile)\n",
        "        if not call_50d.empty and not put_50d.empty:\n",
        "            iv_call_50d = call_50d['iv_normalized'].iloc[0]\n",
        "            iv_put_50d  = put_50d['iv_normalized'].iloc[0]\n",
        "            atm_iv = (iv_call_50d + iv_put_50d) / 2.0\n",
        "            butterfly = ((iv_call_25d + iv_put_25d) / 2.0) - atm_iv\n",
        "        else:\n",
        "            butterfly = np.nan\n",
        "            atm_iv = np.nan\n",
        "\n",
        "        # Skew ratio\n",
        "        skew_ratio = (iv_put_25d / iv_call_25d) if iv_call_25d > 0 else np.nan\n",
        "\n",
        "        # Interpretación\n",
        "        if risk_reversal > 0.05:\n",
        "            skew_signal = \"Bearish (Put demand high)\"\n",
        "        elif risk_reversal < -0.05:\n",
        "            skew_signal = \"Bullish (Call demand high)\"\n",
        "        else:\n",
        "            skew_signal = \"Neutral\"\n",
        "\n",
        "        results.append({\n",
        "            'expiration_date': pd.to_datetime(exp_date).strftime('%Y-%m-%d'),\n",
        "            'days_to_exp': days_to_exp,\n",
        "            'iv_call_25d': iv_call_25d,\n",
        "            'iv_put_25d': iv_put_25d,\n",
        "            'atm_iv_50d': atm_iv,\n",
        "            'risk_reversal_25d': risk_reversal,\n",
        "            'butterfly_spread': butterfly,\n",
        "            'skew_ratio': skew_ratio,\n",
        "            'skew_signal': skew_signal,\n",
        "            'call_25d_strike': call_25d['strike_price'].iloc[0] if not call_25d.empty else np.nan,\n",
        "            'put_25d_strike':  put_25d['strike_price'].iloc[0]  if not put_25d.empty  else np.nan\n",
        "        })\n",
        "\n",
        "    df_skew = pd.DataFrame(results).sort_values('days_to_exp')\n",
        "    if df_skew.empty:\n",
        "        return {}\n",
        "\n",
        "    # Evolución del skew\n",
        "    df_skew['skew_change'] = df_skew['risk_reversal_25d'].diff()\n",
        "    df_skew['skew_acceleration'] = df_skew['skew_change'].diff()\n",
        "\n",
        "    # Summary\n",
        "    summary = {\n",
        "        'current_risk_reversal': df_skew.iloc[0]['risk_reversal_25d'] if len(df_skew) > 0 else np.nan,\n",
        "        'avg_risk_reversal': df_skew['risk_reversal_25d'].mean(),\n",
        "        'current_butterfly': df_skew.iloc[0]['butterfly_spread'] if len(df_skew) > 0 else np.nan,\n",
        "        'avg_butterfly': df_skew['butterfly_spread'].mean(),\n",
        "        'dominant_skew_signal': (df_skew['skew_signal'].mode()[0] if not df_skew.empty else 'Unknown'),\n",
        "        'skew_trend': 'Increasing' if df_skew['skew_change'].mean() > 0 else 'Decreasing'\n",
        "    }\n",
        "\n",
        "    print(f\"✅ IV Skew: Risk Reversal = {summary['current_risk_reversal']:.4f}, Signal = {summary['dominant_skew_signal']}\")\n",
        "\n",
        "    return {\n",
        "        'skew_25delta': df_skew,\n",
        "        'summary': pd.DataFrame([summary])\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 6: Smile Regression (SVI Model)\n",
        "# =========================\n",
        "def calculate_smile_regression_svi(df_options: pd.DataFrame, spot: float) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Volatility Smile Regression using SVI (Stochastic Volatility Inspired) model\n",
        "    and polynomial regression as fallback\n",
        "    SVI Model: σ²(k) = a + b * (ρ * (k - m) + sqrt((k - m)² + σ²))\n",
        "    where k = log(K/S) is log-moneyness\n",
        "    \"\"\"\n",
        "    print(\"😊 Calculating Smile Regression (SVI Model)...\")\n",
        "\n",
        "    if df_options.empty or not spot:\n",
        "        return {}\n",
        "\n",
        "    df = df_options.copy()\n",
        "\n",
        "    # Moneyness\n",
        "    df['moneyness'] = df['strike_price'] / float(spot)\n",
        "    df['log_moneyness'] = np.log(df['moneyness'])\n",
        "\n",
        "    # Normalize IV → variance\n",
        "    df['iv_normalized'] = df['iv'].apply(lambda x: x / 100.0 if x >= 5 else x)\n",
        "    df['iv_variance'] = df['iv_normalized'] ** 2\n",
        "\n",
        "    # Dates\n",
        "    df['exp_date'] = pd.to_datetime(df['expiration_date'], errors='coerce')\n",
        "    df = df.dropna(subset=['exp_date', 'log_moneyness', 'iv_variance'])\n",
        "    df['days_to_exp'] = (df['exp_date'] - datetime.now()).dt.days\n",
        "    df = df[df['days_to_exp'] > 0]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    for exp_date in sorted(df['exp_date'].unique()):\n",
        "        exp_opts = df[df['exp_date'] == exp_date].copy()\n",
        "        if len(exp_opts) < 10:\n",
        "            continue\n",
        "\n",
        "        days_to_exp = (exp_date - datetime.now()).days\n",
        "        k = exp_opts['log_moneyness'].values\n",
        "        iv_var = exp_opts['iv_variance'].values\n",
        "\n",
        "        # Polynomial (quadratic) baseline\n",
        "        try:\n",
        "            poly_coeffs = np.polyfit(k, iv_var, 2)\n",
        "            a_poly, b_poly, c_poly = poly_coeffs[0], poly_coeffs[1], poly_coeffs[2]\n",
        "            iv_var_pred_poly = np.polyval(poly_coeffs, k)\n",
        "            r2_poly = 1 - (np.sum((iv_var - iv_var_pred_poly) ** 2) / np.sum((iv_var - np.mean(iv_var)) ** 2))\n",
        "            atm_iv_poly = np.sqrt(np.polyval(poly_coeffs, 0.0))  # IV at ATM (k=0)\n",
        "            skew_poly = b_poly\n",
        "            curvature_poly = a_poly\n",
        "        except Exception as e:\n",
        "            print(f\"   Polynomial fit failed for {exp_date}: {e}\")\n",
        "            atm_iv_poly = np.nan\n",
        "            skew_poly = np.nan\n",
        "            curvature_poly = np.nan\n",
        "            r2_poly = np.nan\n",
        "            iv_var_pred_poly = np.full_like(iv_var, np.nan, dtype=float)\n",
        "\n",
        "        # SVI model\n",
        "        def svi_func(k_arr, a, b, rho, m, sigma):\n",
        "            try:\n",
        "                return a + b * (rho * (k_arr - m) + np.sqrt((k_arr - m) ** 2 + sigma ** 2))\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        try:\n",
        "            from scipy.optimize import curve_fit\n",
        "            a_init = float(np.mean(iv_var))\n",
        "            b_init = 0.1\n",
        "            rho_init = 0.0\n",
        "            m_init = 0.0\n",
        "            sigma_init = 0.1\n",
        "            bounds = ([0, 0, -1, -1, 0.01], [1, 1, 1, 1, 1])\n",
        "            popt, _ = curve_fit(\n",
        "                svi_func, k, iv_var,\n",
        "                p0=[a_init, b_init, rho_init, m_init, sigma_init],\n",
        "                bounds=bounds, maxfev=5000\n",
        "            )\n",
        "            a_svi, b_svi, rho_svi, m_svi, sigma_svi = popt\n",
        "            iv_var_pred_svi = svi_func(k, *popt)\n",
        "            r2_svi = 1 - (np.sum((iv_var - iv_var_pred_svi) ** 2) / np.sum((iv_var - np.mean(iv_var)) ** 2))\n",
        "            atm_iv_svi = np.sqrt(svi_func(0.0, *popt))\n",
        "            svi_success = True\n",
        "        except Exception as e:\n",
        "            print(f\"   SVI fit failed for {exp_date}, using polynomial: {e}\")\n",
        "            a_svi = b_svi = rho_svi = m_svi = sigma_svi = np.nan\n",
        "            r2_svi = np.nan\n",
        "            atm_iv_svi = np.nan\n",
        "            iv_var_pred_svi = np.full_like(iv_var, np.nan, dtype=float)\n",
        "            svi_success = False\n",
        "\n",
        "        # Residuals / model selection\n",
        "        if svi_success:\n",
        "            residuals = iv_var - iv_var_pred_svi\n",
        "            model_used = 'SVI'\n",
        "            r2_final = r2_svi\n",
        "            atm_iv_final = atm_iv_svi\n",
        "        else:\n",
        "            residuals = iv_var - iv_var_pred_poly if not np.isnan(r2_poly) else np.zeros_like(iv_var)\n",
        "            model_used = 'Polynomial'\n",
        "            r2_final = r2_poly\n",
        "            atm_iv_final = atm_iv_poly\n",
        "\n",
        "        std_residual = np.std(residuals)\n",
        "        exp_opts['residual'] = residuals\n",
        "        exp_opts['is_mispriced'] = np.abs(residuals) > (2 * std_residual)\n",
        "        n_mispriced = int(exp_opts['is_mispriced'].sum())\n",
        "\n",
        "        results.append({\n",
        "            'expiration_date': pd.to_datetime(exp_date).strftime('%Y-%m-%d'),\n",
        "            'days_to_exp': days_to_exp,\n",
        "            'model_used': model_used,\n",
        "            'r_squared': r2_final,\n",
        "            'atm_iv': atm_iv_final,\n",
        "            'poly_skew': skew_poly,\n",
        "            'poly_curvature': curvature_poly,\n",
        "            'svi_a': a_svi if svi_success else np.nan,\n",
        "            'svi_b': b_svi if svi_success else np.nan,\n",
        "            'svi_rho': rho_svi if svi_success else np.nan,\n",
        "            'svi_m': m_svi if svi_success else np.nan,\n",
        "            'svi_sigma': sigma_svi if svi_success else np.nan,\n",
        "            'n_contracts_fitted': len(exp_opts),\n",
        "            'n_mispriced': n_mispriced,\n",
        "            'mispriced_pct': (n_mispriced / len(exp_opts) * 100.0) if len(exp_opts) > 0 else 0.0\n",
        "        })\n",
        "\n",
        "    df_smile = pd.DataFrame(results).sort_values('days_to_exp')\n",
        "    if df_smile.empty:\n",
        "        return {}\n",
        "\n",
        "    print(f\"✅ Smile Regression: {len(df_smile)} expirations fitted\")\n",
        "\n",
        "    return {\n",
        "        'smile_regression': df_smile,\n",
        "        'summary': pd.DataFrame([{\n",
        "            'avg_r_squared': df_smile['r_squared'].mean(),\n",
        "            'best_fit_r_squared': df_smile['r_squared'].max(),\n",
        "            'total_mispriced': int(df_smile['n_mispriced'].sum()),\n",
        "            'model_preference': (df_smile['model_used'].mode()[0] if not df_smile.empty else 'Unknown')\n",
        "        }])\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 7: LQS/OAI/Block Trades Detection\n",
        "# =========================\n",
        "def detect_unusual_options_activity(\n",
        "    df_options: pd.DataFrame,\n",
        "    df_flow: pd.DataFrame,\n",
        "    premium_threshold: float = 100000,\n",
        "    volume_oi_ratio: float = 2.0\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Detect unusual options activity:\n",
        "    - Large Quantity Sweeps (LQS): Trades > premium threshold\n",
        "    - Unusual Options Activity (UOI): Volume >> Open Interest\n",
        "    - Block Trades: Single large transactions\n",
        "    - Classify as bullish/bearish/neutral\n",
        "    \"\"\"\n",
        "    print(\"🔍 Detecting Unusual Options Activity...\")\n",
        "\n",
        "    if df_options.empty:\n",
        "        return {}\n",
        "\n",
        "    df = df_options.copy()\n",
        "\n",
        "    # Premium (vol * price * 100)\n",
        "    df['volume'] = pd.to_numeric(df.get('volume', 0), errors='coerce').fillna(0)\n",
        "    df['close']  = pd.to_numeric(df.get('close', 0),  errors='coerce').fillna(0)\n",
        "    df['open_interest'] = pd.to_numeric(df.get('open_interest', 0), errors='coerce').fillna(0)\n",
        "    df['premium'] = df['volume'] * df['close'] * 100.0\n",
        "\n",
        "    # Volume/OI ratio\n",
        "    def _ratio(row):\n",
        "        return (row['volume'] / row['open_interest']) if row['open_interest'] > 0 else 0.0\n",
        "    df['vol_oi_ratio'] = df.apply(_ratio, axis=1)\n",
        "\n",
        "    # 1) LQS\n",
        "    lqs = df[df['premium'] > premium_threshold].copy()\n",
        "    lqs['activity_type'] = 'LQS (Large Sweep)'\n",
        "    lqs['reason'] = lqs['premium'].apply(lambda x: f'Premium ${x:,.0f} > ${premium_threshold:,.0f}')\n",
        "\n",
        "    # 2) UOI\n",
        "    uoi = df[df['vol_oi_ratio'] > volume_oi_ratio].copy()\n",
        "    uoi['activity_type'] = 'UOI (Unusual Activity)'\n",
        "    uoi['reason'] = uoi['vol_oi_ratio'].apply(lambda x: f'Volume/OI = {x:.1f}x (> {volume_oi_ratio}x)')\n",
        "\n",
        "    # 3) Block trades\n",
        "    blocks = df[(df['volume'] >= 100) & (df['premium'] > 50000)].copy()\n",
        "    blocks['activity_type'] = 'Block Trade'\n",
        "    blocks['reason'] = 'Large single transaction'\n",
        "\n",
        "    # Combine\n",
        "    unusual = pd.concat([lqs, uoi, blocks], ignore_index=True)\n",
        "    unusual = unusual.drop_duplicates(subset=['options_ticker']) if 'options_ticker' in unusual.columns else unusual\n",
        "\n",
        "    if unusual.empty:\n",
        "        print(\"   No unusual activity detected\")\n",
        "        return {\n",
        "            'unusual_activity': pd.DataFrame(),\n",
        "            'summary': pd.DataFrame([{\n",
        "                'total_unusual': 0,\n",
        "                'lqs_count': 0,\n",
        "                'uoi_count': 0,\n",
        "                'block_count': 0,\n",
        "                'total_premium': 0.0,\n",
        "                'bullish_count': 0,\n",
        "                'bearish_count': 0\n",
        "            }])\n",
        "        }\n",
        "\n",
        "    # Clasificación de sentimiento\n",
        "    def classify_sentiment(row):\n",
        "        opt_type = str(row.get('contract_type', '')).lower()\n",
        "\n",
        "        # Si hay flow por contrato\n",
        "        if df_flow is not None and not df_flow.empty:\n",
        "            contract = row.get('options_ticker')\n",
        "            if contract is not None and 'contract' in df_flow.columns:\n",
        "                contract_flow = df_flow[df_flow['contract'] == contract]\n",
        "                if not contract_flow.empty:\n",
        "                    buy_vol  = contract_flow['buy_volume'].iloc[0]\n",
        "                    sell_vol = contract_flow['sell_volume'].iloc[0]\n",
        "                    if buy_vol > sell_vol * 1.5:\n",
        "                        return 'Bullish (Call Buying)' if opt_type in ['call', 'c'] else 'Bearish (Put Buying)'\n",
        "                    elif sell_vol > buy_vol * 1.5:\n",
        "                        return 'Bearish (Call Selling)' if opt_type in ['call', 'c'] else 'Bullish (Put Selling)'\n",
        "\n",
        "        # Fallback\n",
        "        if opt_type in ['call', 'c']:\n",
        "            return 'Bullish (Call Activity)'\n",
        "        elif opt_type in ['put', 'p']:\n",
        "            return 'Bearish (Put Activity)'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "\n",
        "    unusual['sentiment'] = unusual.apply(classify_sentiment, axis=1)\n",
        "    unusual['bullish'] = unusual['sentiment'].str.contains('Bullish', case=False, na=False)\n",
        "    unusual['bearish'] = unusual['sentiment'].str.contains('Bearish', case=False, na=False)\n",
        "\n",
        "    # Summary\n",
        "    summary = {\n",
        "        'total_unusual': len(unusual),\n",
        "        'lqs_count': len(lqs),\n",
        "        'uoi_count': len(uoi),\n",
        "        'block_count': len(blocks),\n",
        "        'total_premium': float(unusual['premium'].sum()),\n",
        "        'bullish_count': int(unusual['bullish'].sum()),\n",
        "        'bearish_count': int(unusual['bearish'].sum()),\n",
        "        'neutral_count': int((~unusual['bullish'] & ~unusual['bearish']).sum())\n",
        "    }\n",
        "\n",
        "    # Output columns\n",
        "    output_cols = [\n",
        "        'options_ticker', 'expiration_date', 'strike_price', 'contract_type',\n",
        "        'volume', 'open_interest', 'vol_oi_ratio', 'close', 'premium',\n",
        "        'activity_type', 'reason', 'sentiment'\n",
        "    ]\n",
        "    output_cols = [c for c in output_cols if c in unusual.columns]\n",
        "    unusual_output = unusual[output_cols].sort_values('premium', ascending=False)\n",
        "\n",
        "    print(f\"✅ Unusual Activity: {summary['total_unusual']} detected ({summary['bullish_count']} bullish, {summary['bearish_count']} bearish)\")\n",
        "    return {\n",
        "        'unusual_activity': unusual_output,\n",
        "        'summary': pd.DataFrame([summary])\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 8: Expected Earnings Gap (Historical Analysis)\n",
        "# =========================\n",
        "def calculate_expected_earnings_gap(\n",
        "    ticker: str,\n",
        "    df_earnings_hist: pd.DataFrame,\n",
        "    df_price_hist: pd.DataFrame = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate expected earnings gap based on historical price moves\n",
        "    Analyzes last 8-12 quarters of earnings reactions\n",
        "    Returns expected gap %, historical moves, and comparison to current implied move\n",
        "    \"\"\"\n",
        "    print(\"📊 Calculating Expected Earnings Gap (Historical)...\")\n",
        "\n",
        "    if df_earnings_hist.empty:\n",
        "        print(\"   No earnings history available\")\n",
        "        return {}\n",
        "\n",
        "    df = df_earnings_hist.copy()\n",
        "\n",
        "    # Date column\n",
        "    date_col = next((c for c in ['date', 'publishedDate', 'fillingDate'] if c in df.columns), None)\n",
        "    if not date_col:\n",
        "        return {}\n",
        "\n",
        "    df['earnings_date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "    df = df.dropna(subset=['earnings_date']).sort_values('earnings_date', ascending=False)\n",
        "\n",
        "    recent_earnings = df.head(12)\n",
        "    if len(recent_earnings) < 4:\n",
        "        print(f\"   Insufficient earnings history ({len(recent_earnings)} < 4)\")\n",
        "        return {}\n",
        "\n",
        "    historical_moves: List[Dict[str, Any]] = []\n",
        "\n",
        "    # If price history available\n",
        "    if df_price_hist is not None and not df_price_hist.empty:\n",
        "        price_df = df_price_hist.copy()\n",
        "\n",
        "        # Normalize date column\n",
        "        if 't' in price_df.columns:\n",
        "            price_df['date'] = pd.to_datetime(price_df['t'], unit='ms')\n",
        "        elif 'timestamp' in price_df.columns:\n",
        "            price_df['date'] = pd.to_datetime(price_df['timestamp'], unit='ms')\n",
        "        else:\n",
        "            price_df['date'] = pd.to_datetime(price_df.index)\n",
        "\n",
        "        price_df = price_df.set_index('date').sort_index()\n",
        "\n",
        "        for _, earnings_row in recent_earnings.iterrows():\n",
        "            earn_date = earnings_row['earnings_date']\n",
        "\n",
        "            # 1 day before (skip weekends)\n",
        "            pre_earn_date = earn_date - timedelta(days=1)\n",
        "            while pre_earn_date.weekday() >= 5:\n",
        "                pre_earn_date -= timedelta(days=1)\n",
        "\n",
        "            pre_price = price_df[price_df.index.date == pre_earn_date.date()]['c'].iloc[-1] if 'c' in price_df.columns else None\n",
        "\n",
        "            # 1 day after (skip weekends)\n",
        "            post_earn_date = earn_date + timedelta(days=1)\n",
        "            while post_earn_date.weekday() >= 5:\n",
        "                post_earn_date += timedelta(days=1)\n",
        "\n",
        "            post_price = price_df[price_df.index.date == post_earn_date.date()]['c'].iloc[0] if 'c' in price_df.columns else None\n",
        "\n",
        "            if pre_price and post_price:\n",
        "                move_pct = ((post_price - pre_price) / pre_price) * 100.0\n",
        "                move_abs = abs(move_pct)\n",
        "\n",
        "                eps_actual   = pd.to_numeric(earnings_row.get('eps', np.nan), errors='coerce')\n",
        "                eps_estimate = pd.to_numeric(earnings_row.get('epsEstimated', np.nan), errors='coerce')\n",
        "                eps_surprise = ((eps_actual - eps_estimate) / abs(eps_estimate) * 100.0) if pd.notna(eps_actual) and pd.notna(eps_estimate) and eps_estimate != 0 else np.nan\n",
        "\n",
        "                historical_moves.append({\n",
        "                    'earnings_date': earn_date.strftime('%Y-%m-%d'),\n",
        "                    'pre_price': pre_price,\n",
        "                    'post_price': post_price,\n",
        "                    'move_%': move_pct,\n",
        "                    'abs_move_%': move_abs,\n",
        "                    'eps_actual': eps_actual,\n",
        "                    'eps_estimate': eps_estimate,\n",
        "                    'eps_surprise_%': eps_surprise,\n",
        "                    'direction': 'UP' if move_pct > 0 else 'DOWN'\n",
        "                })\n",
        "    else:\n",
        "        # No price history: EPS surprise proxy\n",
        "        print(\"   No price history, using EPS surprise as proxy\")\n",
        "        for _, earnings_row in recent_earnings.iterrows():\n",
        "            eps_actual   = pd.to_numeric(earnings_row.get('eps', np.nan), errors='coerce')\n",
        "            eps_estimate = pd.to_numeric(earnings_row.get('epsEstimated', np.nan), errors='coerce')\n",
        "\n",
        "            if pd.notna(eps_actual) and pd.notna(eps_estimate) and eps_estimate != 0:\n",
        "                eps_surprise = ((eps_actual - eps_estimate) / abs(eps_estimate)) * 100.0\n",
        "                move_proxy = eps_surprise * 0.5  # Rough proxy: 1% EPS surprise ≈ 0.5% price move\n",
        "\n",
        "                historical_moves.append({\n",
        "                    'earnings_date': earnings_row['earnings_date'].strftime('%Y-%m-%d'),\n",
        "                    'eps_actual': eps_actual,\n",
        "                    'eps_estimate': eps_estimate,\n",
        "                    'eps_surprise_%': eps_surprise,\n",
        "                    'move_%_proxy': move_proxy,\n",
        "                    'abs_move_%': abs(move_proxy)\n",
        "                })\n",
        "\n",
        "    if not historical_moves:\n",
        "        print(\"   Could not calculate historical moves\")\n",
        "        return {}\n",
        "\n",
        "    df_moves = pd.DataFrame(historical_moves)\n",
        "\n",
        "    # Winsorize 5–95%\n",
        "    abs_moves = df_moves['abs_move_%'].values\n",
        "    lower = np.percentile(abs_moves, 5)\n",
        "    upper = np.percentile(abs_moves, 95)\n",
        "    winsorized_moves = np.clip(abs_moves, lower, upper)\n",
        "\n",
        "    expected_gap = {\n",
        "        'median_move_%': float(np.median(abs_moves)),\n",
        "        'mean_move_%': float(np.mean(abs_moves)),\n",
        "        'winsorized_median_%': float(np.median(winsorized_moves)),\n",
        "        'std_move_%': float(np.std(abs_moves)),\n",
        "        'min_move_%': float(np.min(abs_moves)),\n",
        "        'max_move_%': float(np.max(abs_moves)),\n",
        "        'n_earnings': int(len(df_moves)),\n",
        "        'avg_eps_surprise_%': float(df_moves['eps_surprise_%'].mean()) if 'eps_surprise_%' in df_moves.columns else np.nan,\n",
        "        'up_moves': int((df_moves.get('direction', pd.Series([''])) == 'UP').sum()),\n",
        "        'down_moves': int((df_moves.get('direction', pd.Series([''])) == 'DOWN').sum())\n",
        "    }\n",
        "\n",
        "    print(f\"✅ Expected Earnings Gap: {expected_gap['winsorized_median_%']:.2f}% (median of {expected_gap['n_earnings']} earnings)\")\n",
        "\n",
        "    return {\n",
        "        'historical_moves': df_moves,\n",
        "        'expected_gap_summary': pd.DataFrame([expected_gap])\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 9: IV Crush History (30-90 Days Pre-Earnings)\n",
        "# =========================\n",
        "def calculate_iv_crush_history(\n",
        "    ticker: str,\n",
        "    df_earnings_hist: pd.DataFrame,\n",
        "    FMP_KEY: str,\n",
        "    POLY_KEY: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate historical IV crush patterns\n",
        "    Retrieves IV data 30-90 days before past earnings to measure crush\n",
        "\n",
        "    Note: This requires historical options data which may not be available\n",
        "    We'll use a simplified approach based on current IV patterns\n",
        "    \"\"\"\n",
        "    print(\"📉 Calculating IV Crush History...\")\n",
        "\n",
        "    if df_earnings_hist.empty:\n",
        "        return {}\n",
        "\n",
        "    # Past earnings\n",
        "    df = df_earnings_hist.copy()\n",
        "    date_col = next((c for c in ['date', 'publishedDate', 'fillingDate'] if c in df.columns), None)\n",
        "    if not date_col:\n",
        "        return {}\n",
        "\n",
        "    df['earnings_date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "    df = df.dropna(subset=['earnings_date']).sort_values('earnings_date', ascending=False)\n",
        "\n",
        "    recent_earnings = df.head(8)  # last 8 earnings\n",
        "    iv_crush_data: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Typical ranges (framework)\n",
        "    typical_crush_patterns = {\n",
        "        'tech_growth': {'pre_iv': 0.45, 'post_iv': 0.28, 'crush_%': 37.8},\n",
        "        'stable_large_cap': {'pre_iv': 0.30, 'post_iv': 0.22, 'crush_%': 26.7},\n",
        "        'volatile_small_cap': {'pre_iv': 0.60, 'post_iv': 0.35, 'crush_%': 41.7},\n",
        "    }\n",
        "\n",
        "    # Estimated values (placeholder when no historical IV)\n",
        "    for _, earn_row in recent_earnings.iterrows():\n",
        "        earn_date = earn_row['earnings_date']\n",
        "        estimated_pre_iv = 0.40   # ~ 1 week before\n",
        "        estimated_post_iv = 0.25  # ~ 1 week after\n",
        "        estimated_crush = ((estimated_pre_iv - estimated_post_iv) / estimated_pre_iv) * 100.0\n",
        "\n",
        "        iv_crush_data.append({\n",
        "            'earnings_date': earn_date.strftime('%Y-%m-%d'),\n",
        "            'days_before_measured': 7,\n",
        "            'estimated_pre_earnings_iv': estimated_pre_iv,\n",
        "            'estimated_post_earnings_iv': estimated_post_iv,\n",
        "            'estimated_iv_crush_%': estimated_crush,\n",
        "            'data_type': 'Estimated (Historical data unavailable)'\n",
        "        })\n",
        "\n",
        "    df_iv_crush = pd.DataFrame(iv_crush_data)\n",
        "    if not df_iv_crush.empty:\n",
        "        summary = {\n",
        "            'avg_iv_crush_%': df_iv_crush['estimated_iv_crush_%'].mean(),\n",
        "            'median_iv_crush_%': df_iv_crush['estimated_iv_crush_%'].median(),\n",
        "            'min_crush_%': df_iv_crush['estimated_iv_crush_%'].min(),\n",
        "            'max_crush_%': df_iv_crush['estimated_iv_crush_%'].max(),\n",
        "            'n_earnings_analyzed': len(df_iv_crush),\n",
        "            'typical_range': f\"{df_iv_crush['estimated_iv_crush_%'].quantile(0.25):.1f}% - {df_iv_crush['estimated_iv_crush_%'].quantile(0.75):.1f}%\",\n",
        "            'note': 'Historical options IV data not available - using industry estimates'\n",
        "        }\n",
        "        print(f\"✅ IV Crush History: Avg {summary['avg_iv_crush_%']:.1f}% crush over {summary['n_earnings_analyzed']} earnings\")\n",
        "        print(\"   ⚠️  Note: Using estimated values (historical options data requires premium API)\")\n",
        "\n",
        "        return {\n",
        "            'iv_crush_history': df_iv_crush,\n",
        "            'summary': pd.DataFrame([summary])\n",
        "        }\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEATURE 10: Quantitative Scorecard (0-100 Scale)\n",
        "# =========================\n",
        "def calculate_quantitative_scorecard(\n",
        "    ticker: str,\n",
        "    spot_price: float,\n",
        "    iv_metrics: Dict,\n",
        "    flow_metrics: Dict,\n",
        "    gex_metrics: Dict,\n",
        "    earnings_metrics: Dict,\n",
        "    unusual_activity: Dict,\n",
        "    skew_metrics: Dict\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Comprehensive quantitative scorecard (0-100 scale)\n",
        "\n",
        "    Scoring factors:\n",
        "    1. IV Percentile Rank (0-20 points)\n",
        "    2. Put/Call Ratio (0-15 points)\n",
        "    3. Unusual Options Activity Score (0-15 points)\n",
        "    4. GEX Positioning (0-15 points)\n",
        "    5. IV Skew Signal (0-10 points)\n",
        "    6. Earnings Setup Quality (0-15 points)\n",
        "    7. Risk/Reward Ratio (0-10 points)\n",
        "\n",
        "    Final recommendation: Strong Buy, Buy, Hold, Sell, Strong Sell\n",
        "    \"\"\"\n",
        "    print(\"🎯 Calculating Quantitative Scorecard...\")\n",
        "\n",
        "    score = 0\n",
        "    max_score = 100\n",
        "    score_breakdown: Dict[str, Any] = {}\n",
        "\n",
        "    # 1) IV Percentile Rank (0-20) — Lower IV rank = better for buying options\n",
        "    iv_rank = iv_metrics.get('iv_rank', iv_metrics.get('iv_percentile', 50))\n",
        "    if pd.notna(iv_rank):\n",
        "        if iv_rank < 20:\n",
        "            iv_score = 20; iv_interpretation = \"Excellent (Low IV)\"\n",
        "        elif iv_rank < 40:\n",
        "            iv_score = 15; iv_interpretation = \"Good\"\n",
        "        elif iv_rank < 60:\n",
        "            iv_score = 10; iv_interpretation = \"Average\"\n",
        "        elif iv_rank < 80:\n",
        "            iv_score = 5;  iv_interpretation = \"Poor (High IV)\"\n",
        "        else:\n",
        "            iv_score = 0;  iv_interpretation = \"Very Poor (Very High IV)\"\n",
        "    else:\n",
        "        iv_score = 10; iv_interpretation = \"Unknown\"\n",
        "    score += iv_score\n",
        "    score_breakdown['iv_rank_score'] = {'score': iv_score, 'max': 20, 'interpretation': iv_interpretation}\n",
        "\n",
        "    # 2) Put/Call Ratio (0-15)\n",
        "    pc_ratio = flow_metrics.get('put_call_ratio', flow_metrics.get('Ratio_Put_Call_Vol', 1.0))\n",
        "    if pd.notna(pc_ratio):\n",
        "        if 0.7 <= pc_ratio <= 1.3:\n",
        "            pc_score = 15; pc_interpretation = \"Balanced sentiment\"\n",
        "        elif pc_ratio < 0.7:\n",
        "            pc_score = 12; pc_interpretation = \"Bullish (Call heavy)\"\n",
        "        elif pc_ratio > 1.3:\n",
        "            pc_score = 8;  pc_interpretation = \"Bearish (Put heavy)\"\n",
        "        else:\n",
        "            pc_score = 10; pc_interpretation = \"Normal\"\n",
        "    else:\n",
        "        pc_score = 10; pc_interpretation = \"Unknown\"\n",
        "    score += pc_score\n",
        "    score_breakdown['put_call_ratio_score'] = {'score': pc_score, 'max': 15, 'interpretation': pc_interpretation}\n",
        "\n",
        "    # 3) Unusual Options Activity (0-15)\n",
        "    if unusual_activity and 'summary' in unusual_activity:\n",
        "        uoa_df = unusual_activity['summary']\n",
        "        if not uoa_df.empty:\n",
        "            total_unusual = uoa_df['total_unusual'].iloc[0]\n",
        "            bullish_count = uoa_df['bullish_count'].iloc[0]\n",
        "            bearish_count = uoa_df['bearish_count'].iloc[0]\n",
        "\n",
        "            if total_unusual > 10:\n",
        "                uoa_score = 15; uoa_interpretation = f\"High activity ({total_unusual} signals)\"\n",
        "            elif total_unusual > 5:\n",
        "                uoa_score = 10; uoa_interpretation = f\"Moderate activity ({total_unusual} signals)\"\n",
        "            elif total_unusual > 0:\n",
        "                uoa_score = 5;  uoa_interpretation = f\"Low activity ({total_unusual} signals)\"\n",
        "            else:\n",
        "                uoa_score = 0;  uoa_interpretation = \"No unusual activity\"\n",
        "\n",
        "            if bullish_count > bearish_count * 1.5:\n",
        "                uoa_interpretation += \" - Bullish bias\"\n",
        "            elif bearish_count > bullish_count * 1.5:\n",
        "                uoa_interpretation += \" - Bearish bias\"\n",
        "        else:\n",
        "            uoa_score = 5; uoa_interpretation = \"No data\"\n",
        "    else:\n",
        "        uoa_score = 5; uoa_interpretation = \"No data\"\n",
        "    score += uoa_score\n",
        "    score_breakdown['unusual_activity_score'] = {'score': uoa_score, 'max': 15, 'interpretation': uoa_interpretation}\n",
        "\n",
        "    # 4) GEX Positioning (0-15)\n",
        "    if gex_metrics and 'summary' in gex_metrics:\n",
        "        gex_df = gex_metrics['summary']\n",
        "        if not gex_df.empty:\n",
        "            total_gex  = gex_df['total_gex_shares'].iloc[0]\n",
        "            gamma_flip = gex_df['gamma_flip_strike'].iloc[0]\n",
        "\n",
        "            if total_gex > 0:\n",
        "                gex_score = 12; gex_interpretation = \"Supportive (Dealers long gamma)\"\n",
        "            else:\n",
        "                gex_score = 8;  gex_interpretation = \"Volatile (Dealers short gamma)\"\n",
        "\n",
        "            if isinstance(gamma_flip, (int, float)):\n",
        "                distance_from_flip = abs(float(spot_price) - float(gamma_flip)) / float(spot_price)\n",
        "                if distance_from_flip < 0.02:\n",
        "                    gex_score += 3\n",
        "                    gex_interpretation += \" - Near flip level\"\n",
        "        else:\n",
        "            gex_score = 7; gex_interpretation = \"No data\"\n",
        "    else:\n",
        "        gex_score = 7; gex_interpretation = \"No data\"\n",
        "    score += gex_score\n",
        "    score_breakdown['gex_positioning_score'] = {'score': gex_score, 'max': 15, 'interpretation': gex_interpretation}\n",
        "\n",
        "    # 5) IV Skew Signal (0-10)\n",
        "    if skew_metrics and 'summary' in skew_metrics:\n",
        "        skew_df = skew_metrics['summary']\n",
        "        if not skew_df.empty:\n",
        "            risk_reversal = skew_df['current_risk_reversal'].iloc[0]\n",
        "            if pd.notna(risk_reversal):\n",
        "                if -0.03 <= risk_reversal <= 0.03:\n",
        "                    skew_score = 10; skew_interpretation = \"Balanced skew\"\n",
        "                elif risk_reversal > 0.05:\n",
        "                    skew_score = 6;  skew_interpretation = \"Put skew (Bearish sentiment)\"\n",
        "                elif risk_reversal < -0.05:\n",
        "                    skew_score = 6;  skew_interpretation = \"Call skew (Bullish sentiment)\"\n",
        "                else:\n",
        "                    skew_score = 8;  skew_interpretation = \"Normal skew\"\n",
        "            else:\n",
        "                skew_score = 5; skew_interpretation = \"No data\"\n",
        "        else:\n",
        "            skew_score = 5; skew_interpretation = \"No data\"\n",
        "    else:\n",
        "        skew_score = 5; skew_interpretation = \"No data\"\n",
        "    score += skew_score\n",
        "    score_breakdown['iv_skew_score'] = {'score': skew_score, 'max': 10, 'interpretation': skew_interpretation}\n",
        "\n",
        "    # 6) Earnings Setup Quality (0-15)\n",
        "    if earnings_metrics and 'expected_gap_summary' in earnings_metrics:\n",
        "        earn_df = earnings_metrics['expected_gap_summary']\n",
        "        if not earn_df.empty:\n",
        "            expected_gap = earn_df['winsorized_median_%'].iloc[0]\n",
        "            if pd.notna(expected_gap):\n",
        "                if expected_gap > 8:\n",
        "                    earn_score = 15; earn_interpretation = f\"High volatility expected ({expected_gap:.1f}%)\"\n",
        "                elif expected_gap > 5:\n",
        "                    earn_score = 12; earn_interpretation = f\"Moderate volatility ({expected_gap:.1f}%)\"\n",
        "                elif expected_gap > 3:\n",
        "                    earn_score = 9;  earn_interpretation = f\"Normal volatility ({expected_gap:.1f}%)\"\n",
        "                else:\n",
        "                    earn_score = 6;  earn_interpretation = f\"Low volatility ({expected_gap:.1f}%)\"\n",
        "            else:\n",
        "                earn_score = 7; earn_interpretation = \"No data\"\n",
        "        else:\n",
        "            earn_score = 7; earn_interpretation = \"No data\"\n",
        "    else:\n",
        "        earn_score = 7; earn_interpretation = \"No data\"\n",
        "    score += earn_score\n",
        "    score_breakdown['earnings_setup_score'] = {'score': earn_score, 'max': 15, 'interpretation': earn_interpretation}\n",
        "\n",
        "    # 7) Risk/Reward Ratio (0-10) — simplified\n",
        "    risk_reward_score = 8\n",
        "    risk_reward_interpretation = \"Average risk/reward\"\n",
        "    score += risk_reward_score\n",
        "    score_breakdown['risk_reward_score'] = {'score': risk_reward_score, 'max': 10, 'interpretation': risk_reward_interpretation}\n",
        "\n",
        "    # Final score\n",
        "    final_score = (score / max_score) * 100.0\n",
        "\n",
        "    # Recommendation\n",
        "    if final_score >= 80:\n",
        "        recommendation = \"STRONG BUY\"; confidence = \"High\"\n",
        "    elif final_score >= 65:\n",
        "        recommendation = \"BUY\"; confidence = \"Moderate-High\"\n",
        "    elif final_score >= 50:\n",
        "        recommendation = \"HOLD\"; confidence = \"Moderate\"\n",
        "    elif final_score >= 35:\n",
        "        recommendation = \"SELL\"; confidence = \"Moderate-High\"\n",
        "    else:\n",
        "        recommendation = \"STRONG SELL\"; confidence = \"High\"\n",
        "\n",
        "    scorecard = {\n",
        "        'ticker': ticker,\n",
        "        'spot_price': spot_price,\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'total_score': round(final_score, 1),\n",
        "        'max_score': max_score,\n",
        "        'recommendation': recommendation,\n",
        "        'confidence': confidence,\n",
        "        'iv_rank_score': score_breakdown['iv_rank_score']['score'],\n",
        "        'iv_rank_interpretation': score_breakdown['iv_rank_score']['interpretation'],\n",
        "        'put_call_score': score_breakdown['put_call_ratio_score']['score'],\n",
        "        'put_call_interpretation': score_breakdown['put_call_ratio_score']['interpretation'],\n",
        "        'unusual_activity_score': score_breakdown['unusual_activity_score']['score'],\n",
        "        'unusual_activity_interpretation': score_breakdown['unusual_activity_score']['interpretation'],\n",
        "        'gex_score': score_breakdown['gex_positioning_score']['score'],\n",
        "        'gex_interpretation': score_breakdown['gex_positioning_score']['interpretation'],\n",
        "        'skew_score': score_breakdown['iv_skew_score']['score'],\n",
        "        'skew_interpretation': score_breakdown['iv_skew_score']['interpretation'],\n",
        "        'earnings_score': score_breakdown['earnings_setup_score']['score'],\n",
        "        'earnings_interpretation': score_breakdown['earnings_setup_score']['interpretation'],\n",
        "        'risk_reward_score': score_breakdown['risk_reward_score']['score'],\n",
        "        'risk_reward_interpretation': score_breakdown['risk_reward_score']['interpretation']\n",
        "    }\n",
        "\n",
        "    print(f\"✅ Scorecard: {final_score:.1f}/100 - {recommendation} ({confidence} confidence)\")\n",
        "    return pd.DataFrame([scorecard])\n",
        "\n",
        "\n",
        "# =========================\n",
        "# HELPER: Generate All Enhanced Sheets\n",
        "# =========================\n",
        "def generate_all_enhanced_sheets(\n",
        "    ticker: str,\n",
        "    spot_price: float,\n",
        "    df_options: pd.DataFrame,\n",
        "    df_flow: pd.DataFrame,\n",
        "    df_earnings_hist: pd.DataFrame,\n",
        "    df_price_hist: pd.DataFrame,\n",
        "    FMP_KEY: str,\n",
        "    POLY_KEY: str\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Master function to generate all 10 enhanced analysis sheets\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"🚀 HYPERION V9 - GENERATING ALL ENHANCED SHEETS\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    sheets: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    # Get next earnings date (if available)\n",
        "    earnings_date = None\n",
        "    if not df_earnings_hist.empty:\n",
        "        df_earn = df_earnings_hist.copy()\n",
        "        date_col = next((c for c in ['date', 'publishedDate'] if c in df_earn.columns), None)\n",
        "        if date_col:\n",
        "            df_earn['date'] = pd.to_datetime(df_earn[date_col], errors='coerce')\n",
        "            future_earnings = df_earn[df_earn['date'] >= datetime.now()].sort_values('date')\n",
        "            if not future_earnings.empty:\n",
        "                earnings_date = future_earnings.iloc[0]['date'].strftime('%Y-%m-%d')\n",
        "\n",
        "    # 1. Expected Move Calculator\n",
        "    try:\n",
        "        df_expected_move = calculate_expected_move(df_options, spot_price, earnings_date)\n",
        "        if not df_expected_move.empty:\n",
        "            sheets['Expected_Move'] = df_expected_move\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Expected Move failed: {e}\")\n",
        "\n",
        "    # 2. Comprehensive Greeks\n",
        "    try:\n",
        "        df_greeks = calculate_comprehensive_greeks(df_options, spot_price)\n",
        "        if not df_greeks.empty:\n",
        "            greek_cols = [\n",
        "                'options_ticker', 'expiration_date', 'strike_price', 'contract_type',\n",
        "                'delta_calc', 'gamma_calc', 'vega_calc', 'theta_calc', 'rho_calc',\n",
        "                'vanna_calc', 'vomma_calc', 'charm_calc', 'veta_calc',\n",
        "                'speed_calc', 'color_calc', 'ultima_calc'\n",
        "            ]\n",
        "            greek_cols = [c for c in greek_cols if c in df_greeks.columns]\n",
        "            sheets['Advanced_Greeks'] = df_greeks[greek_cols]\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Advanced Greeks failed: {e}\")\n",
        "        df_greeks = df_options.copy()  # fallback para las siguientes etapas\n",
        "\n",
        "    # 3. GEX Analysis\n",
        "    try:\n",
        "        df_for_gex = df_greeks if ('gamma_calc' in df_greeks.columns) else df_options\n",
        "        gex_results = calculate_gex_comprehensive(df_for_gex, spot_price)\n",
        "        if gex_results:\n",
        "            sheets['GEX_By_Strike']    = gex_results.get('gex_by_strike', pd.DataFrame())\n",
        "            sheets['GEX_By_Expiration'] = gex_results.get('gex_by_expiration', pd.DataFrame())\n",
        "            sheets['GEX_Walls']        = gex_results.get('gex_walls', pd.DataFrame())\n",
        "            sheets['GEX_Summary']      = gex_results.get('summary', pd.DataFrame())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GEX Analysis failed: {e}\")\n",
        "        gex_results = {}\n",
        "\n",
        "    # 4. IV Term Structure\n",
        "    try:\n",
        "        iv_term_results = calculate_iv_term_structure_enhanced(df_options)\n",
        "        if iv_term_results:\n",
        "            sheets['IV_Term_Structure'] = iv_term_results.get('term_structure', pd.DataFrame())\n",
        "            sheets['IV_Term_Summary']   = iv_term_results.get('summary', pd.DataFrame())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ IV Term Structure failed: {e}\")\n",
        "        iv_term_results = {}\n",
        "\n",
        "    # 5. IV Skew 25-Delta\n",
        "    try:\n",
        "        skew_results = calculate_iv_skew_25delta_enhanced(df_options)\n",
        "        if skew_results:\n",
        "            sheets['IV_Skew_25Delta'] = skew_results.get('skew_25delta', pd.DataFrame())\n",
        "            sheets['IV_Skew_Summary'] = skew_results.get('summary', pd.DataFrame())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ IV Skew failed: {e}\")\n",
        "        skew_results = {}\n",
        "\n",
        "    # 6. Smile Regression\n",
        "    try:\n",
        "        smile_results = calculate_smile_regression_svi(df_options, spot_price)\n",
        "        if smile_results:\n",
        "            sheets['Smile_Regression'] = smile_results.get('smile_regression', pd.DataFrame())\n",
        "            sheets['Smile_Summary']    = smile_results.get('summary', pd.DataFrame())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Smile Regression failed: {e}\")\n",
        "\n",
        "    # 7. Unusual Options Activity\n",
        "    try:\n",
        "        unusual_results = detect_unusual_options_activity(df_options, df_flow)\n",
        "        if unusual_results:\n",
        "            sheets['Unusual_Activity'] = unusual_results.get('unusual_activity', pd.DataFrame())\n",
        "            sheets['Unusual_Summary']  = unusual_results.get('summary', pd.DataFrame())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unusual Activity detection failed: {e}\")\n",
        "        unusual_results = {}\n",
        "\n",
        "    # 8. Expected Earnings Gap\n",
        "    try:\n",
        "        earnings_gap_results = calculate_expected_earnings_gap(ticker, df_earnings_hist, df_price_hist)\n",
        "        if earnings_gap_results:\n",
        "            sheets['Earnings_Gap_History'] = earnings_gap_results.get('historical_moves', pd.DataFrame())\n",
        "            sheets['Earnings_Gap_Summary'] = earnings_gap_results.get('expected_gap_summary', pd.DataFrame())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Earnings Gap failed: {e}\")\n",
        "        earnings_gap_results = {}\n",
        "\n",
        "    # 9. IV Crush History\n",
        "    try:\n",
        "        iv_crush_results = calculate_iv_crush_history(ticker, df_earnings_hist, FMP_KEY, POLY_KEY)\n",
        "        if iv_crush_results:\n",
        "            sheets['IV_Crush_History'] = iv_crush_results.get('iv_crush_history', pd.DataFrame())\n",
        "            sheets['IV_Crush_Summary'] = iv_crush_results.get('summary', pd.DataFrame())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ IV Crush History failed: {e}\")\n",
        "\n",
        "    # 10. Quantitative Scorecard\n",
        "    try:\n",
        "        iv_metrics   = (iv_term_results.get('summary', pd.DataFrame()).to_dict('records')[0]\n",
        "                        if iv_term_results and 'summary' in iv_term_results else {})\n",
        "        flow_metrics = {}  # (extraer de df_flow si corresponde)\n",
        "        scorecard = calculate_quantitative_scorecard(\n",
        "            ticker, spot_price, iv_metrics, flow_metrics, gex_results,\n",
        "            earnings_gap_results, unusual_results, skew_results\n",
        "        )\n",
        "        if not scorecard.empty:\n",
        "            sheets['Scorecard'] = scorecard\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Scorecard calculation failed: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"✅ HYPERION V9 - Generated {len(sheets)} enhanced sheets\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    return sheets\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"✅ HYPERION V9 Enhancement Module Loaded\")\n",
        "    print(\"   All 10 advanced features implemented:\")\n",
        "    print(\"   1. Expected Move Calculator\")\n",
        "    print(\"   2. Comprehensive Greeks (1st, 2nd, 3rd order)\")\n",
        "    print(\"   3. GEX Analysis\")\n",
        "    print(\"   4. IV Term Structure\")\n",
        "    print(\"   5. IV Skew 25-Delta\")\n",
        "    print(\"   6. Smile Regression (SVI)\")\n",
        "    print(\"   7. Unusual Options Activity Detection\")\n",
        "    print(\"   8. Expected Earnings Gap\")\n",
        "    print(\"   9. IV Crush History\")\n",
        "    print(\"   10. Quantitative Scorecard\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 47,
          "status": "ok",
          "timestamp": 1763076829466,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "VqXD8tqrGjy7",
        "outputId": "88d2db20-0be9-4477-b3d7-094096ba9b05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ V9 Integration layer loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V9 - Integration of Enhanced Features\n",
        "# =========================\n",
        "import pandas as pd  # requerido para pd.DataFrame en get()\n",
        "\n",
        "def integrate_v9_enhancements(\n",
        "    ticker,\n",
        "    spot_price,\n",
        "    df_options_chain,\n",
        "    df_flow_analysis,\n",
        "    df_earnings_hist,\n",
        "    data_sources,\n",
        "    persistent_dir\n",
        "):\n",
        "    \"\"\"\n",
        "    Integrate all V9 enhanced features into the main pipeline\n",
        "    \"\"\"\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"🚀 HYPERION V9 - ENHANCED ANALYSIS STARTING\")\n",
        "    logger.info(\"=\" * 80)\n",
        "\n",
        "    enhanced_sheets = {}\n",
        "\n",
        "    # Get price history for earnings gap analysis\n",
        "    df_price_hist = data_sources.get('3_Daily_Bars_5Y', pd.DataFrame())\n",
        "\n",
        "    try:\n",
        "        # Call the master function from enhancements module\n",
        "        enhanced_sheets = generate_all_enhanced_sheets(\n",
        "            ticker=ticker,\n",
        "            spot_price=spot_price,\n",
        "            df_options=df_options_chain,\n",
        "            df_flow=df_flow_analysis,\n",
        "            df_earnings_hist=df_earnings_hist,\n",
        "            df_price_hist=df_price_hist,\n",
        "            FMP_KEY=FMP_KEY,\n",
        "            POLY_KEY=POLY_KEY\n",
        "        )\n",
        "        logger.success(f\"✅ Generated {len(enhanced_sheets)} enhanced analysis sheets\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error generating enhanced sheets: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return enhanced_sheets\n",
        "\n",
        "\n",
        "logger.info(\"✅ V9 Integration layer loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 28,
          "status": "ok",
          "timestamp": 1763076829508,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "Grzdvp1VGjy8",
        "outputId": "07e0d725-54ac-49ce-f2f2-667044ad9496"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Hyperion V9 pipeline ready\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V9 - Pipeline Integration\n",
        "# =========================\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log, sqrt, exp\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import brentq\n",
        "\n",
        "\n",
        "# Modify the main pipeline to include V9 features\n",
        "def run_hyperion_v9(tickers):\n",
        "    \"\"\"\n",
        "    Main pipeline for Hyperion V9 with all enhancements\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    # === Helper: V9 Edge Pack (FPR + VCP) ===\n",
        "    def build_edge_pack(options_df, greeks_df, gex_by_strike, expected_move_df, spot):\n",
        "        \"\"\"\n",
        "        options_df     : DataFrame base (cadena) — puede o no tener gex_shares/vanna/charm\n",
        "        greeks_df      : 'V9_Greeks_By_Contract' o 'greeks_second_order' si existe\n",
        "        gex_by_strike  : hoja de GEX por strike si existe\n",
        "        expected_move_df: hoja Expected_Move\n",
        "        spot           : precio spot (float)\n",
        "        \"\"\"\n",
        "        if expected_move_df is None or expected_move_df.empty or spot is None:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # 1) EM (front-month) con fallback a straddle\n",
        "        em_front = expected_move_df.sort_values('days_to_expiration').head(1)\n",
        "        em_iv = em_front.get('expected_move_%_iv')\n",
        "        em_str = em_front.get('expected_move_%_straddle')\n",
        "        try:\n",
        "            em_iv = float(em_iv.iloc[0]) if em_iv is not None else np.nan\n",
        "        except Exception:\n",
        "            em_iv = np.nan\n",
        "        try:\n",
        "            em_str = float(em_str.iloc[0]) if em_str is not None else np.nan\n",
        "        except Exception:\n",
        "            em_str = np.nan\n",
        "        em_pct = em_iv if pd.notna(em_iv) else (em_str if pd.notna(em_str) else np.nan)\n",
        "        if not pd.notna(em_pct) or em_pct <= 0:\n",
        "            return pd.DataFrame()  # sin EM no hay FPR ni corredor\n",
        "\n",
        "        em_frac = em_pct / 100.0\n",
        "\n",
        "        # 2) Gamma flip (prefiere gex_by_strike; si no, intenta options_df)\n",
        "        gamma_flip = np.nan\n",
        "        try:\n",
        "            if gex_by_strike is not None and not gex_by_strike.empty and 'gex_shares' in gex_by_strike.columns:\n",
        "                s = gex_by_strike.sort_values('strike_price')\n",
        "                cum = s['gex_shares'].cumsum()\n",
        "                flips = s.loc[(cum * cum.shift(fill_value=0)) < 0, 'strike_price']\n",
        "                if len(flips) > 0:\n",
        "                    gamma_flip = float(flips.iloc[0])\n",
        "            elif options_df is not None and not options_df.empty and 'gex_shares' in options_df.columns:\n",
        "                s = (options_df.groupby('strike_price')['gex_shares'].sum().sort_index())\n",
        "                cum = s.cumsum()\n",
        "                flips = s.index[(cum * cum.shift(fill_value=0)) < 0]\n",
        "                if len(flips) > 0:\n",
        "                    gamma_flip = float(flips.min())\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # 3) FPR (Flip-Proximity Ratio)\n",
        "        if pd.notna(gamma_flip):\n",
        "            try:\n",
        "                fpr = abs(float(spot) - gamma_flip) / (float(spot) * em_frac)\n",
        "            except Exception:\n",
        "                fpr = np.nan\n",
        "        else:\n",
        "            fpr = np.nan\n",
        "\n",
        "        # 4) Vanna/Charm en corredor ±EM% (front-month)\n",
        "        low = float(spot) * (1 - em_frac)\n",
        "        high = float(spot) * (1 + em_frac)\n",
        "\n",
        "        # Fuente para vanna/charm: prioriza greeks_df; si no, options_df *_advanced\n",
        "        vanna_col, charm_col, oi_col = None, None, None\n",
        "        src = None\n",
        "        if greeks_df is not None and not greeks_df.empty:\n",
        "            for c in ['vanna', 'vanna_calc', 'vanna_advanced']:\n",
        "                if c in greeks_df.columns:\n",
        "                    vanna_col = c\n",
        "                    break\n",
        "            for c in ['charm', 'charm_calc', 'charm_advanced']:\n",
        "                if c in greeks_df.columns:\n",
        "                    charm_col = c\n",
        "                    break\n",
        "            for c in ['open_interest', 'oi']:\n",
        "                if c in greeks_df.columns:\n",
        "                    oi_col = c\n",
        "                    break\n",
        "            src = greeks_df\n",
        "        if src is None or vanna_col is None or charm_col is None:\n",
        "            if options_df is not None and not options_df.empty:\n",
        "                for c in ['vanna', 'vanna_calc', 'vanna_advanced']:\n",
        "                    if c in options_df.columns:\n",
        "                        vanna_col = c\n",
        "                        break\n",
        "                for c in ['charm', 'charm_calc', 'charm_advanced']:\n",
        "                    if c in options_df.columns:\n",
        "                        charm_col = c\n",
        "                        break\n",
        "                for c in ['open_interest', 'oi']:\n",
        "                    if c in options_df.columns:\n",
        "                        oi_col = c\n",
        "                        break\n",
        "                src = options_df if (vanna_col and charm_col) else None\n",
        "\n",
        "        vannaX = np.nan\n",
        "        charmX = np.nan\n",
        "        if src is not None and vanna_col and charm_col:\n",
        "            tmp = src.copy()\n",
        "            strike_name = 'strike_price' if 'strike_price' in tmp.columns else ('strike' if 'strike' in tmp.columns else None)\n",
        "            if strike_name:\n",
        "                tmp[strike_name] = pd.to_numeric(tmp[strike_name], errors='coerce')\n",
        "                tmp = tmp[(tmp[strike_name] >= low) & (tmp[strike_name] <= high)]\n",
        "                oi_series = pd.to_numeric(tmp.get(oi_col, 0), errors='coerce').fillna(0.0)\n",
        "                vannaX = (pd.to_numeric(tmp[vanna_col], errors='coerce') * float(spot) * 100.0 * oi_series).sum(skipna=True)\n",
        "                charmX = (pd.to_numeric(tmp[charm_col], errors='coerce') * float(spot) * 100.0 * oi_series).sum(skipna=True)\n",
        "\n",
        "        out = {\n",
        "            'EM_%': em_pct,\n",
        "            'Gamma_Flip': gamma_flip,\n",
        "            'FPR': fpr,\n",
        "            'VannaCorridorX': vannaX,\n",
        "            'CharmCorridorX': charmX\n",
        "        }\n",
        "        return pd.DataFrame([out])\n",
        "\n",
        "    config = {\n",
        "        \"start_date\": (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        \"end_date\": datetime.now().strftime('%Y-%m-%d')\n",
        "    }\n",
        "\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR)\n",
        "    persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    raw_base_dir = persistent_dir / 'raw'\n",
        "    raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Lit exchanges\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"\\n{'='*80}\")\n",
        "        logger.info(f\"🚀 HYPERION V9 - Processing {ticker}\")\n",
        "        logger.info(f\"{'='*80}\\n\")\n",
        "\n",
        "        raw_dir = raw_base_dir / ticker\n",
        "        raw_dir.mkdir(exist_ok=True)\n",
        "        rd = raw_dir  # alias a la ruta (mkdir no devuelve Path)\n",
        "\n",
        "        run_ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_excel_path = (\n",
        "            persistent_dir / f\"{ticker}_hyperion_v9_report_{run_ts}.xlsx\"\n",
        "        )\n",
        "\n",
        "        # 1) Extract base data\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, rd)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) CIK-based data\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty:\n",
        "            profile = data_sources['1_Profile'].iloc[0]\n",
        "            if profile.get('cik'):\n",
        "                cik = profile.get('cik')\n",
        "                cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "                cik_params = {'cik': cik, 'limit': 100}\n",
        "                df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "                if not df_cik.empty:\n",
        "                    data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "\n",
        "        # 3) Earnings transcripts\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 4) Options chain\n",
        "        logger.info(\"📋 Fetching options chain...\")\n",
        "        options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "        df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "        df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "            logger.success(f\"✅ Options chain: {len(df_options_chain)} contracts\")\n",
        "\n",
        "        # Helpers (ámbito local) para reconstrucción de IV si los greeks vienen vacíos\n",
        "        def bs_price(S, K, r, q, sigma, T, cp):  # cp = +1 call, -1 put\n",
        "            from math import log, sqrt, exp\n",
        "            if sigma <= 0 or T <= 0:\n",
        "                return np.nan\n",
        "            d1 = (log(S / K) + (r - q + 0.5 * sigma * sigma) * T) / (sigma * sqrt(T))\n",
        "            d2 = d1 - sigma * sqrt(T)\n",
        "            if cp > 0:\n",
        "                return S * exp(-q * T) * norm.cdf(d1) - K * exp(-r * T) * norm.cdf(d2)\n",
        "            else:\n",
        "                return K * exp(-r * T) * norm.cdf(-d2) - S * exp(-q * T) * norm.cdf(-d1)\n",
        "\n",
        "        def iv_from_mid(S, K, r, q, T, mid, cp, lo=1e-4, hi=5.0):\n",
        "            f = lambda s: bs_price(S, K, r, q, s, T, cp) - mid\n",
        "            try:\n",
        "                return float(brentq(f, lo, hi, maxiter=100))\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        # 5) Flow analysis\n",
        "        df_flow_analysis = pd.DataFrame()\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(\n",
        "                df_options_chain['volume'], errors='coerce'\n",
        "            ).fillna(0)\n",
        "\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (\n",
        "                df_options_chain\n",
        "                .groupby('expiration_date')\n",
        "                .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "\n",
        "            if len(top_contracts) > 50:\n",
        "                top_contracts = top_contracts.nlargest(50, 'volume')\n",
        "\n",
        "            logger.info(f\"🔄 Analyzing flow for {len(top_contracts)} contracts...\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                logger.success(f\"✅ Flow analysis: {len(df_flow_analysis)} contracts\")\n",
        "\n",
        "        # Fallback: si no hubo trades/flow, construye proxy desde snapshot\n",
        "        if df_flow_analysis.empty and not df_options_chain.empty:\n",
        "            _df = df_options_chain.copy()\n",
        "\n",
        "            # Normaliza numéricos\n",
        "            _df['volume'] = pd.to_numeric(_df.get('volume', 0), errors='coerce').fillna(0)\n",
        "            _df['open_interest'] = pd.to_numeric(_df.get('open_interest', 0), errors='coerce').fillna(0)\n",
        "\n",
        "            # Contract symbol desde snapshot\n",
        "            _df['contract'] = _df.get('options_ticker', _df.get('symbol'))\n",
        "\n",
        "            # Normaliza expiration\n",
        "            _df['expiration_norm'] = (\n",
        "                _df['expiration'] if 'expiration' in _df.columns else _df.get('expiration_date')\n",
        "            )\n",
        "\n",
        "            # Selección mínima para análisis de flujo\n",
        "            df_flow_analysis = _df[\n",
        "                ['contract', 'contract_type', 'strike_price', 'expiration_norm', 'volume', 'open_interest']\n",
        "            ].rename(columns={'expiration_norm': 'expiration'})\n",
        "\n",
        "            # Proxy simple: ratio volumen/OI\n",
        "            df_flow_analysis['vol_oi_ratio'] = df_flow_analysis.apply(\n",
        "                lambda x: x['volume'] / x['open_interest'] if x['open_interest'] > 0 else 0, axis=1\n",
        "            )\n",
        "\n",
        "            data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "            logger.warning(\"⚠️ Sin trades para flow; usando snapshot fallback (vol/OI proxy).\")\n",
        "\n",
        "        # Get spot price\n",
        "        spot_price = 0\n",
        "        if '2_Quote' in data_sources and not data_sources['2_Quote'].empty:\n",
        "            spot_price = float(data_sources['2_Quote'].iloc[0].get('price', 0))\n",
        "            logger.info(f\"💰 Spot price: ${spot_price:.2f}\")\n",
        "\n",
        "        # Calculate basic metrics\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, 10)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, 20)\n",
        "\n",
        "        # Advanced options metrics\n",
        "        if not df_options_chain.empty and spot_price:\n",
        "            adv_metrics = calculate_advanced_options_metrics(df_options_chain, spot_price)\n",
        "            if adv_metrics:\n",
        "                data_sources['Options_Metrics_Advanced'] = pd.DataFrame([adv_metrics])\n",
        "\n",
        "        # ====================================================================\n",
        "        # ✅ V3 ENHANCED INTEGRATION (reubicado después de flow/spot)\n",
        "        #    *No* se escribe Excel aquí para evitar doble escritura.\n",
        "        # ====================================================================\n",
        "        enhanced_sheets_v3 = {}\n",
        "        try:\n",
        "            if not df_options_chain.empty:\n",
        "                enhanced_sheets_v3 = generate_enhanced_sheets(\n",
        "                    df_options_chain=df_options_chain,\n",
        "                    df_flow_analysis=df_flow_analysis if not df_flow_analysis.empty else pd.DataFrame(),\n",
        "                    df_earnings_hist=data_sources.get('12_Earnings_Cal', pd.DataFrame()),\n",
        "                    spot_price=spot_price,\n",
        "                    ticker=ticker,\n",
        "                    status_logger=status_logger\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ V3 Enhanced block failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # ====================================================================\n",
        "        # 🚀 HYPERION V9 ENHANCED FEATURES\n",
        "        # ====================================================================\n",
        "\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"🚀 HYPERION V9 - ENHANCED ANALYSIS\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"\")\n",
        "\n",
        "        enhanced_sheets_v9 = {}\n",
        "\n",
        "        try:\n",
        "            # Get earnings history\n",
        "            df_earnings_hist = data_sources.get('12_Earnings_Cal', pd.DataFrame())\n",
        "\n",
        "            # Call V9 integration function\n",
        "            enhanced_sheets_v9 = integrate_v9_enhancements(\n",
        "                ticker=ticker,\n",
        "                spot_price=spot_price,\n",
        "                df_options_chain=df_options_chain,\n",
        "                df_flow_analysis=df_flow_analysis,\n",
        "                df_earnings_hist=df_earnings_hist,\n",
        "                data_sources=data_sources,\n",
        "                persistent_dir=persistent_dir\n",
        "            )\n",
        "\n",
        "            logger.success(\n",
        "                f\"✅ V9 Enhanced Analysis Complete: {len(enhanced_sheets_v9)} sheets generated\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ V9 Enhanced Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # ====================================================================\n",
        "        # 📊 GENERATING EXCEL REPORT\n",
        "        # ====================================================================\n",
        "\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"📊 GENERATING EXCEL REPORT\")\n",
        "        logger.info(\"=\" * 80)\n",
        "\n",
        "        all_sheets = {}\n",
        "\n",
        "        # Add base data sheets\n",
        "        for name, df in data_sources.items():\n",
        "            if not df.empty and name not in ['Options_Chain']:\n",
        "                sheet_name = name.replace('_', ' ')[:31]\n",
        "                all_sheets[sheet_name] = df\n",
        "\n",
        "        # Add V3 + V9 enhanced sheets\n",
        "        all_sheets.update(enhanced_sheets_v3)\n",
        "        all_sheets.update(enhanced_sheets_v9)\n",
        "\n",
        "        # Create dashboard\n",
        "        dashboard_data = {\n",
        "            'Ticker': ticker,\n",
        "            'Spot_Price': spot_price,\n",
        "            'Report_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Total_Sheets': len(all_sheets),\n",
        "            'V9_Enhanced_Sheets': len(enhanced_sheets_v9),\n",
        "            'Options_Contracts_Analyzed': len(df_options_chain) if not df_options_chain.empty else 0,\n",
        "        }\n",
        "\n",
        "        # Add V9 summary metrics to dashboard\n",
        "        if 'Scorecard' in enhanced_sheets_v9:\n",
        "            scorecard = enhanced_sheets_v9['Scorecard']\n",
        "            if not scorecard.empty:\n",
        "                dashboard_data['V9_Score'] = scorecard.iloc[0]['total_score']\n",
        "                dashboard_data['V9_Recommendation'] = scorecard.iloc[0]['recommendation']\n",
        "                dashboard_data['V9_Confidence'] = scorecard.iloc[0]['confidence']\n",
        "\n",
        "        if 'GEX_Summary' in enhanced_sheets_v9:\n",
        "            gex_summary = enhanced_sheets_v9['GEX_Summary']\n",
        "            if not gex_summary.empty:\n",
        "                dashboard_data['GEX_Total_Shares'] = gex_summary.iloc[0]['total_gex_shares']\n",
        "                dashboard_data['GEX_Positioning'] = gex_summary.iloc[0]['positioning']\n",
        "\n",
        "        if 'Expected_Move' in enhanced_sheets_v9:\n",
        "            exp_move = enhanced_sheets_v9['Expected_Move']\n",
        "            if not exp_move.empty and len(exp_move) > 0:\n",
        "                front_move = exp_move.sort_values('days_to_expiration').iloc[0]\n",
        "                em_iv = front_move.get('expected_move_%_iv')\n",
        "                em_str = front_move.get('expected_move_%_straddle')\n",
        "                if em_iv is not None and pd.notna(em_iv):\n",
        "                    dashboard_data['Expected_Move_%'] = float(em_iv)\n",
        "                elif em_str is not None and pd.notna(em_str):\n",
        "                    dashboard_data['Expected_Move_%'] = float(em_str)\n",
        "\n",
        "        # === V9 EDGE PACK (FPR + VCP) ===\n",
        "        edge_df = build_edge_pack(\n",
        "            options_df=df_options_chain,\n",
        "            greeks_df=enhanced_sheets_v9.get(\n",
        "                'V9_Greeks_By_Contract',\n",
        "                enhanced_sheets_v9.get('greeks_second_order', pd.DataFrame())\n",
        "            ),\n",
        "            gex_by_strike=enhanced_sheets_v9.get('GEX_By_Strike', pd.DataFrame()),\n",
        "            expected_move_df=enhanced_sheets_v9.get('Expected_Move', pd.DataFrame()),\n",
        "            spot=spot_price\n",
        "        )\n",
        "        if edge_df is not None and not edge_df.empty:\n",
        "            enhanced_sheets_v9['V9_Edge_Pack'] = edge_df\n",
        "            all_sheets['V9_Edge_Pack'] = edge_df\n",
        "\n",
        "        # Dashboard sheet\n",
        "        df_dashboard = pd.DataFrame([dashboard_data])\n",
        "        all_sheets = {'Dashboard': df_dashboard, **all_sheets}\n",
        "\n",
        "        # Write to Excel (una sola vez)\n",
        "        try:\n",
        "            with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
        "                for sheet_name, df in all_sheets.items():\n",
        "                    if not df.empty:\n",
        "                        safe_sheet_name = str(sheet_name)[:31]\n",
        "                        df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "            logger.success(f\"✅ Excel report saved: {output_excel_path}\")\n",
        "            logger.success(f\"   Total sheets: {len(all_sheets)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Excel export error: {e}\")\n",
        "\n",
        "        # ====================================================================\n",
        "        # 📈 SUMMARY\n",
        "        # ====================================================================\n",
        "\n",
        "        elapsed = time.time() - t_start\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"📈 HYPERION V9 - ANALYSIS COMPLETE\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(f\"   Ticker: {ticker}\")\n",
        "        logger.info(f\"   Time Elapsed: {elapsed:.1f}s\")\n",
        "        logger.info(f\"   Report: {output_excel_path.name}\")\n",
        "        logger.info(f\"   Total Sheets: {len(all_sheets)}\")\n",
        "        logger.info(f\"   V9 Enhanced Sheets: {len(enhanced_sheets_v9)}\")\n",
        "\n",
        "        if 'Scorecard' in enhanced_sheets_v9:\n",
        "            scorecard = enhanced_sheets_v9['Scorecard']\n",
        "            if not scorecard.empty:\n",
        "                score = scorecard.iloc[0]['total_score']\n",
        "                rec = scorecard.iloc[0]['recommendation']\n",
        "                logger.success(f\"   V9 Score: {score:.1f}/100\")\n",
        "                logger.success(f\"   Recommendation: {rec}\")\n",
        "\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"\")\n",
        "\n",
        "logger.info(\"✅ Hyperion V9 pipeline ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjY_RisTv5hr"
      },
      "source": [
        "# =========================\n",
        "# HYPERION V3 ENHANCED\n",
        "# =========================\n",
        "# This section contains ADD-ONLY enhancements:\n",
        "# - Extended data ingestion with pagination\n",
        "# - 2nd/3rd order Greeks (Vanna, Vomma, Charm, Speed, Color, Ultima)\n",
        "# - Aggregated exposures (GEX, VannaExp, CharmExp, VommaExp)\n",
        "# - Gamma flip analysis\n",
        "# - IV term structure, skew, smile regression\n",
        "# - IV Rank/Percentile, LQS Grade\n",
        "# - Order flow proxies (OAI, block trade ratio)\n",
        "# - Earnings analytics (expected GAP, IV-crush)\n",
        "# - Scorecard preview with probabilities\n",
        "# - Robust error handling with status:degraded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 980,
          "status": "ok",
          "timestamp": 1763076830493,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "PpGqpWzkv5hs",
        "outputId": "2436c105-335e-40aa-9598-33f604cced51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Enhanced imports loaded successfully\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 ENHANCED - Additional Imports\n",
        "# =========================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Added for advanced Greeks and analytics\n",
        "try:\n",
        "    import jax\n",
        "    import jax.numpy as jnp\n",
        "    from jax import grad, vmap\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"jax\", \"jaxlib\", \"-q\"])\n",
        "    import jax\n",
        "    import jax.numpy as jnp\n",
        "    from jax import grad, vmap\n",
        "\n",
        "try:\n",
        "    from scipy import stats\n",
        "    from scipy.optimize import minimize, brentq\n",
        "    from scipy.interpolate import interp1d, CubicSpline\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scipy\", \"-q\"])\n",
        "    from scipy import stats\n",
        "    from scipy.optimize import minimize, brentq\n",
        "    from scipy.interpolate import interp1d, CubicSpline\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    sns.set_style(\"darkgrid\")\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib\", \"seaborn\", \"-q\"])\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    sns.set_style(\"darkgrid\")\n",
        "\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logger.info(\"✅ Enhanced imports loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1064,
          "status": "ok",
          "timestamp": 1763076831565,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "rO-3Pm3nv5hs",
        "outputId": "a4ce4fc0-939e-4dda-92a1-32b825a135c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Advanced Greeks calculator loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 - Advanced Greeks Calculator (Black-Scholes-Merton)\n",
        "# =========================\n",
        "\n",
        "@dataclass\n",
        "class BSMParams:\n",
        "    \"\"\"Black-Scholes-Merton parameters\"\"\"\n",
        "    S: float          # Spot price\n",
        "    K: float          # Strike price\n",
        "    T: float          # Time to expiration (years)\n",
        "    r: float          # Risk-free rate\n",
        "    q: float          # Dividend yield\n",
        "    sigma: float      # Implied volatility\n",
        "    option_type: str  # 'call' or 'put'\n",
        "\n",
        "\n",
        "def bsm_price_jax(S, K, T, r, q, sigma):\n",
        "    \"\"\"Black-Scholes-Merton price using JAX for autodiff\"\"\"\n",
        "    d1 = (jnp.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * jnp.sqrt(T))\n",
        "    d2 = d1 - sigma * jnp.sqrt(T)\n",
        "\n",
        "    call_price = (\n",
        "        S * jnp.exp(-q * T) * jax.scipy.stats.norm.cdf(d1)\n",
        "        - K * jnp.exp(-r * T) * jax.scipy.stats.norm.cdf(d2)\n",
        "    )\n",
        "    return call_price\n",
        "\n",
        "\n",
        "def calculate_advanced_greeks_jax(params: BSMParams) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate 1st, 2nd, and 3rd order Greeks using JAX autodifferentiation\n",
        "\n",
        "    Greeks calculated:\n",
        "    1st order: Delta, Vega, Theta, Rho\n",
        "    2nd order: Gamma, Vanna, Vomma (Volga), Charm\n",
        "    3rd order: Speed, Color, Ultima\n",
        "    \"\"\"\n",
        "    S, K, T, r, q, sigma = params.S, params.K, params.T, params.r, params.q, params.sigma\n",
        "\n",
        "    # Avoid edge cases\n",
        "    if T <= 0 or sigma <= 0 or S <= 0 or K <= 0:\n",
        "        return {\n",
        "            'delta': np.nan, 'gamma': np.nan, 'vega': np.nan, 'theta': np.nan,\n",
        "            'rho': np.nan, 'vanna': np.nan, 'vomma': np.nan, 'charm': np.nan,\n",
        "            'speed': np.nan, 'color': np.nan, 'ultima': np.nan\n",
        "        }\n",
        "\n",
        "    # Create JAX arrays\n",
        "    S_jax = jnp.array(S)\n",
        "    sigma_jax = jnp.array(sigma)\n",
        "    T_jax = jnp.array(T)\n",
        "\n",
        "    # Calculate call price\n",
        "    call_price = bsm_price_jax(S_jax, K, T_jax, r, q, sigma_jax)\n",
        "\n",
        "    # 1st order Greeks using autodiff\n",
        "    delta_call = float(grad(lambda s: bsm_price_jax(s, K, T_jax, r, q, sigma_jax))(S_jax))\n",
        "    vega_call = float(grad(lambda sig: bsm_price_jax(S_jax, K, T_jax, r, q, sig))(sigma_jax))\n",
        "\n",
        "    # 2nd order Greeks\n",
        "    gamma = float(grad(grad(lambda s: bsm_price_jax(s, K, T_jax, r, q, sigma_jax)))(S_jax))\n",
        "    vanna = float(\n",
        "        grad(\n",
        "            grad(lambda s: bsm_price_jax(s, K, T_jax, r, q, sigma_jax), argnums=0),\n",
        "            argnums=0\n",
        "        )(S_jax)\n",
        "    )  # ∂²V/∂S∂σ (según lógica original)\n",
        "    vomma = float(grad(grad(lambda sig: bsm_price_jax(S_jax, K, T_jax, r, q, sig)))(sigma_jax))\n",
        "\n",
        "    # Charm (∂Δ/∂t) - numerical approximation\n",
        "    dt = 1 / 365  # 1 day\n",
        "    T_minus_dt = max(T - dt, 0.001)\n",
        "    delta_t_minus = float(grad(lambda s: bsm_price_jax(s, K, T_minus_dt, r, q, sigma_jax))(S_jax))\n",
        "    charm = (delta_t_minus - delta_call) / dt\n",
        "\n",
        "    # 3rd order Greeks\n",
        "    speed = float(grad(grad(grad(lambda s: bsm_price_jax(s, K, T_jax, r, q, sigma_jax))))(S_jax))\n",
        "\n",
        "    # Color (∂Γ/∂t) - numerical approximation\n",
        "    gamma_t_minus = float(grad(grad(lambda s: bsm_price_jax(s, K, T_minus_dt, r, q, sigma_jax)))(S_jax))\n",
        "    color = (gamma_t_minus - gamma) / dt\n",
        "\n",
        "    ultima = float(grad(grad(grad(lambda sig: bsm_price_jax(S_jax, K, T_jax, r, q, sig))))(sigma_jax))\n",
        "\n",
        "    # Theta - analytical\n",
        "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "\n",
        "    if params.option_type.lower() in ['call', 'c']:\n",
        "        theta = (\n",
        "            -S * stats.norm.pdf(d1) * sigma * np.exp(-q * T) / (2 * np.sqrt(T))\n",
        "            - r * K * np.exp(-r * T) * stats.norm.cdf(d2)\n",
        "            + q * S * np.exp(-q * T) * stats.norm.cdf(d1)\n",
        "        )\n",
        "        delta = delta_call\n",
        "        vega = vega_call\n",
        "    else:  # put\n",
        "        theta = (\n",
        "            -S * stats.norm.pdf(d1) * sigma * np.exp(-q * T) / (2 * np.sqrt(T))\n",
        "            + r * K * np.exp(-r * T) * stats.norm.cdf(-d2)\n",
        "            - q * S * np.exp(-q * T) * stats.norm.cdf(-d1)\n",
        "        )\n",
        "        delta = delta_call - np.exp(-q * T)\n",
        "        vega = vega_call\n",
        "\n",
        "    # Rho\n",
        "    if params.option_type.lower() in ['call', 'c']:\n",
        "        rho = K * T * np.exp(-r * T) * stats.norm.cdf(d2)\n",
        "    else:\n",
        "        rho = -K * T * np.exp(-r * T) * stats.norm.cdf(-d2)\n",
        "\n",
        "    return {\n",
        "        # 1st order\n",
        "        'delta': delta,\n",
        "        'gamma': gamma,\n",
        "        'vega': vega / 100,         # Per 1% change in vol\n",
        "        'theta': theta / 365,       # Per day\n",
        "        'rho': rho / 100,           # Per 1% change in rate\n",
        "        # 2nd order\n",
        "        'vanna': vanna / 100,       # ∂²V/∂S∂σ\n",
        "        'vomma': vomma / (100**2),  # ∂²V/∂σ²\n",
        "        'charm': charm / 365,       # ∂Δ/∂t per day\n",
        "        # 3rd order\n",
        "        'speed': speed,             # ∂Γ/∂S\n",
        "        'color': color / 365,       # ∂Γ/∂t per day\n",
        "        'ultima': ultima / (100**3) # ∂³V/∂σ³\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_greeks_for_chain(\n",
        "    df_options: pd.DataFrame,\n",
        "    spot: float,\n",
        "    rfr: float,\n",
        "    div_yield: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate advanced Greeks for entire options chain\n",
        "    \"\"\"\n",
        "    if df_options.empty or not spot:\n",
        "        logger.warning(\"Empty options chain or no spot price\")\n",
        "        return df_options\n",
        "\n",
        "    logger.info(f\"Calculating advanced Greeks for {len(df_options)} contracts...\")\n",
        "\n",
        "    results = []\n",
        "    for idx, row in df_options.iterrows():\n",
        "        try:\n",
        "            # Parse expiration date\n",
        "            exp_date = pd.to_datetime(row.get('expiration_date'), errors='coerce')\n",
        "            if pd.isna(exp_date):\n",
        "                continue\n",
        "\n",
        "            # Time to expiration in years\n",
        "            T = (exp_date - pd.Timestamp.now()).total_seconds() / (365.25 * 24 * 3600)\n",
        "            if T <= 0:\n",
        "                continue\n",
        "\n",
        "            strike = float(row.get('strike_price', 0))\n",
        "\n",
        "            iv_raw = row.get(\"iv\")\n",
        "            # Si no hay IV o es <= 0, no tiene sentido calcular griegas\n",
        "            if iv_raw is None:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                iv = float(iv_raw)\n",
        "            except (TypeError, ValueError):\n",
        "                continue\n",
        "\n",
        "            if iv <= 0:\n",
        "                continue\n",
        "\n",
        "            # Si viene en porcentaje (ej. 25 = 25%), normalizamos a 0.25\n",
        "            if iv > 5:\n",
        "                iv = iv / 100.0\n",
        "\n",
        "            option_type = str(row.get('contract_type', 'call')).lower()\n",
        "\n",
        "            # Create BSM params\n",
        "            params = BSMParams(\n",
        "                S=spot,\n",
        "                K=strike,\n",
        "                T=T,\n",
        "                r=rfr,\n",
        "                q=div_yield,\n",
        "                sigma=iv,\n",
        "                option_type=option_type\n",
        "            )\n",
        "\n",
        "            # Calculate Greeks\n",
        "            greeks = calculate_advanced_greeks_jax(params)\n",
        "\n",
        "            # Add to row\n",
        "            row_dict = row.to_dict()\n",
        "            for greek_name, greek_value in greeks.items():\n",
        "                row_dict[greek_name + '_advanced'] = greek_value\n",
        "\n",
        "            results.append(row_dict)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to calculate Greeks for contract {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_enhanced = pd.DataFrame(results)\n",
        "    logger.success(f\"✅ Advanced Greeks calculated for {len(df_enhanced)} contracts\")\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "\n",
        "logger.info(\"✅ Advanced Greeks calculator loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 20,
          "status": "ok",
          "timestamp": 1763076831591,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "cQ-eOG5bv5ht",
        "outputId": "e674bd14-a0c7-4bce-96c4-75c48cdfafcb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Aggregated exposures calculator loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 - Aggregated Exposures & Gamma Flip\n",
        "# =========================\n",
        "from typing import Dict, Any  # imports mínimos para tipos (no cambia la lógica)\n",
        "\n",
        "def calculate_aggregated_exposures(df_options: pd.DataFrame, spot: float) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Calculate aggregated exposures: GEX, VannaExp, CharmExp, VommaExp by expiration\n",
        "    \"\"\"\n",
        "    if df_options.empty or not spot:\n",
        "        return {}\n",
        "\n",
        "    logger.info(\"Calculating aggregated exposures...\")\n",
        "\n",
        "    # Ensure numeric columns\n",
        "    for col in [\n",
        "        'gamma', 'vanna_advanced', 'charm_advanced', 'vomma_advanced',\n",
        "        'open_interest', 'strike_price', 'delta'\n",
        "    ]:\n",
        "        if col in df_options.columns:\n",
        "            df_options[col] = pd.to_numeric(df_options[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Add sign based on contract type\n",
        "    df_options['sign'] = df_options['contract_type'].map({\n",
        "        'call': 1, 'C': 1, 'put': -1, 'P': -1\n",
        "    }).fillna(0)\n",
        "\n",
        "    # Calculate exposures (per contract, in shares)\n",
        "    df_options['gex_shares']   = df_options['gamma'] * df_options['open_interest'] * 100 * df_options['sign']\n",
        "    df_options['gex_notional'] = df_options['gex_shares'] * spot * spot / 1e9  # in $B\n",
        "\n",
        "    if 'vanna_advanced' in df_options.columns:\n",
        "        df_options['vanna_exp'] = df_options['vanna_advanced'] * df_options['open_interest'] * 100\n",
        "    else:\n",
        "        df_options['vanna_exp'] = 0\n",
        "\n",
        "    if 'charm_advanced' in df_options.columns:\n",
        "        df_options['charm_exp'] = df_options['charm_advanced'] * df_options['open_interest'] * 100\n",
        "    else:\n",
        "        df_options['charm_exp'] = 0\n",
        "\n",
        "    if 'vomma_advanced' in df_options.columns:\n",
        "        df_options['vomma_exp'] = df_options['vomma_advanced'] * df_options['open_interest'] * 100\n",
        "    else:\n",
        "        df_options['vomma_exp'] = 0\n",
        "\n",
        "    # GEX by expiration\n",
        "    gex_by_exp = (\n",
        "        df_options.groupby('expiration_date')\n",
        "        .agg({\n",
        "            'gex_shares': 'sum',\n",
        "            'gex_notional': 'sum',\n",
        "            'open_interest': 'sum',\n",
        "            'strike_price': ['min', 'max', 'count']\n",
        "        })\n",
        "        .reset_index()\n",
        "    )\n",
        "    gex_by_exp.columns = [\n",
        "        'expiration_date', 'total_gex_shares', 'total_gex_notional_$B',\n",
        "        'total_oi', 'min_strike', 'max_strike', 'n_contracts'\n",
        "    ]\n",
        "\n",
        "    # Total GEX\n",
        "    total_gex_shares   = df_options['gex_shares'].sum()\n",
        "    total_gex_notional = df_options['gex_notional'].sum()\n",
        "\n",
        "    # Vanna, Charm, Vomma exposures by expiration\n",
        "    vanna_charm_vomma_exp = (\n",
        "        df_options.groupby('expiration_date')\n",
        "        .agg({\n",
        "            'vanna_exp': 'sum',\n",
        "            'charm_exp': 'sum',\n",
        "            'vomma_exp': 'sum',\n",
        "            'open_interest': 'sum'\n",
        "        })\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Normalize by total OI\n",
        "    total_oi = df_options['open_interest'].sum()\n",
        "    if total_oi > 0:\n",
        "        vanna_charm_vomma_exp['vanna_exp_normalized'] = vanna_charm_vomma_exp['vanna_exp'] / total_oi\n",
        "        vanna_charm_vomma_exp['charm_exp_normalized'] = vanna_charm_vomma_exp['charm_exp'] / total_oi\n",
        "        vanna_charm_vomma_exp['vomma_exp_normalized'] = vanna_charm_vomma_exp['vomma_exp'] / total_oi\n",
        "\n",
        "    logger.success(f\"✅ Total GEX: {total_gex_shares:,.0f} shares, ${total_gex_notional:.2f}B notional\")\n",
        "\n",
        "    return {\n",
        "        'gex_by_exp': gex_by_exp,\n",
        "        'vanna_charm_vomma_exp': vanna_charm_vomma_exp,\n",
        "        'total_gex_shares': total_gex_shares,\n",
        "        'total_gex_notional': total_gex_notional\n",
        "    }\n",
        "\n",
        "\n",
        "def find_gamma_flip_level(df_options: pd.DataFrame, spot: float) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Find the gamma flip level (spot where GEX changes sign)\n",
        "    \"\"\"\n",
        "    if df_options.empty or 'gex_shares' not in df_options.columns:\n",
        "        return {'gamma_flip_strike': 'N/A', 'current_spot': spot}\n",
        "\n",
        "    logger.info(\"Finding gamma flip level...\")\n",
        "\n",
        "    # GEX by strike\n",
        "    gex_by_strike = (\n",
        "        df_options.groupby('strike_price')['gex_shares']\n",
        "        .sum()\n",
        "        .sort_index()\n",
        "    )\n",
        "\n",
        "    # Cumulative GEX\n",
        "    cum_gex = gex_by_strike.cumsum()\n",
        "\n",
        "    # Find sign changes\n",
        "    sign_changes   = (cum_gex * cum_gex.shift(fill_value=0)) < 0\n",
        "    flip_candidates = cum_gex[sign_changes]\n",
        "\n",
        "    if flip_candidates.empty:\n",
        "        gamma_flip = \"No flip found\"\n",
        "        logger.warning(\"⚠️ No gamma flip level found\")\n",
        "    else:\n",
        "        gamma_flip = float(flip_candidates.index.min())\n",
        "        logger.success(f\"✅ Gamma flip level: ${gamma_flip:.2f}\")\n",
        "\n",
        "    # Additional analysis\n",
        "    strikes    = gex_by_strike.index.values\n",
        "    gex_values = cum_gex.values\n",
        "\n",
        "    return {\n",
        "        'gamma_flip_strike': gamma_flip,\n",
        "        'current_spot': spot,\n",
        "        'spot_vs_flip': f\"{((spot / gamma_flip - 1) * 100):.2f}%\" if isinstance(gamma_flip, float) else 'N/A',\n",
        "        'total_call_gex': df_options[df_options['contract_type'].isin(['call', 'C'])]['gex_shares'].sum(),\n",
        "        'total_put_gex':  df_options[df_options['contract_type'].isin(['put',  'P'])]['gex_shares'].sum(),\n",
        "        'strikes_analyzed': len(strikes),\n",
        "        'gex_by_strike_table': pd.DataFrame({\n",
        "            'strike': strikes,\n",
        "            'gex_shares': gex_by_strike.values,\n",
        "            'cumulative_gex': gex_values\n",
        "        })\n",
        "    }\n",
        "\n",
        "logger.info(\"✅ Aggregated exposures calculator loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 240,
          "status": "ok",
          "timestamp": 1763076831836,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "4H2MuPWEv5ht",
        "outputId": "f4b92878-0d3e-4d91-8e83-0241f781238c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Volatility & microstructure metrics loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 - Volatility & Microstructure Metrics\n",
        "# =========================\n",
        "\n",
        "def calculate_iv_term_structure(df_options: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate IV term structure (ATM IV by expiration)\n",
        "    \"\"\"\n",
        "    if df_options.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logger.info(\"Calculating IV term structure...\")\n",
        "\n",
        "    # Find ATM contracts (closest to delta = 0.5 for calls, -0.5 for puts)\n",
        "    df_calls = df_options[df_options['contract_type'].isin(['call', 'C'])].copy()\n",
        "    df_puts = df_options[df_options['contract_type'].isin(['put', 'P'])].copy()\n",
        "\n",
        "    results = []\n",
        "    for exp_date in df_options['expiration_date'].unique():\n",
        "        exp_calls = df_calls[df_calls['expiration_date'] == exp_date]\n",
        "        exp_puts = df_puts[df_puts['expiration_date'] == exp_date]\n",
        "\n",
        "        # Get ATM options (delta closest to 0.5 / -0.5)\n",
        "        if not exp_calls.empty and 'delta' in exp_calls.columns:\n",
        "            atm_call = exp_calls.iloc[(exp_calls['delta'] - 0.5).abs().argsort()[:1]]\n",
        "            if not atm_call.empty:\n",
        "                results.append({\n",
        "                    'expiration_date': exp_date,\n",
        "                    'days_to_exp': (pd.to_datetime(exp_date) - pd.Timestamp.now()).days,\n",
        "                    'atm_iv_call': float(atm_call['iv'].iloc[0]),\n",
        "                    'atm_strike_call': float(atm_call['strike_price'].iloc[0]),\n",
        "                    'option_type': 'call'\n",
        "                })\n",
        "\n",
        "        if not exp_puts.empty and 'delta' in exp_puts.columns:\n",
        "            atm_put = exp_puts.iloc[(exp_puts['delta'] + 0.5).abs().argsort()[:1]]\n",
        "            if not atm_put.empty:\n",
        "                results.append({\n",
        "                    'expiration_date': exp_date,\n",
        "                    'days_to_exp': (pd.to_datetime(exp_date) - pd.Timestamp.now()).days,\n",
        "                    'atm_iv_put': float(atm_put['iv'].iloc[0]),\n",
        "                    'atm_strike_put': float(atm_put['strike_price'].iloc[0]),\n",
        "                    'option_type': 'put'\n",
        "                })\n",
        "\n",
        "    df_term = pd.DataFrame(results).sort_values('days_to_exp') if results else pd.DataFrame()\n",
        "\n",
        "    if not df_term.empty:\n",
        "        logger.success(f\"✅ IV term structure calculated for {len(df_term)} expirations\")\n",
        "\n",
        "    return df_term\n",
        "\n",
        "\n",
        "def calculate_iv_skew_25delta(df_options: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate 25-delta IV skew (put25Δ - call25Δ) by expiration\n",
        "    \"\"\"\n",
        "    if df_options.empty or 'delta' not in df_options.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logger.info(\"Calculating 25-delta IV skew...\")\n",
        "\n",
        "    df_calls = df_options[df_options['contract_type'].isin(['call', 'C'])].copy()\n",
        "    df_puts = df_options[df_options['contract_type'].isin(['put', 'P'])].copy()\n",
        "\n",
        "    results = []\n",
        "    for exp_date in df_options['expiration_date'].unique():\n",
        "        exp_calls = df_calls[df_calls['expiration_date'] == exp_date]\n",
        "        exp_puts = df_puts[df_puts['expiration_date'] == exp_date]\n",
        "\n",
        "        # Find 25-delta options\n",
        "        if not exp_calls.empty:\n",
        "            call_25d = exp_calls.iloc[(exp_calls['delta'] - 0.25).abs().argsort()[:1]]\n",
        "        else:\n",
        "            call_25d = pd.DataFrame()\n",
        "\n",
        "        if not exp_puts.empty:\n",
        "            put_25d = exp_puts.iloc[(exp_puts['delta'] + 0.25).abs().argsort()[:1]]\n",
        "        else:\n",
        "            put_25d = pd.DataFrame()\n",
        "\n",
        "        if not call_25d.empty and not put_25d.empty:\n",
        "            iv_call_25d = float(call_25d['iv'].iloc[0])\n",
        "            iv_put_25d = float(put_25d['iv'].iloc[0])\n",
        "            skew = iv_put_25d - iv_call_25d\n",
        "\n",
        "            results.append({\n",
        "                'expiration_date': exp_date,\n",
        "                'days_to_exp': (pd.to_datetime(exp_date) - pd.Timestamp.now()).days,\n",
        "                'iv_call_25d': iv_call_25d,\n",
        "                'iv_put_25d': iv_put_25d,\n",
        "                'skew_25d': skew\n",
        "            })\n",
        "\n",
        "    df_skew = pd.DataFrame(results).sort_values('days_to_exp') if results else pd.DataFrame()\n",
        "\n",
        "    if not df_skew.empty:\n",
        "        logger.success(f\"✅ 25-delta IV skew calculated for {len(df_skew)} expirations\")\n",
        "\n",
        "    return df_skew\n",
        "\n",
        "\n",
        "def calculate_iv_smile_regression(df_options: pd.DataFrame, spot: float) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate IV smile regression (slope and curvature) by expiration\n",
        "    \"\"\"\n",
        "    if df_options.empty or not spot:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logger.info(\"Calculating IV smile regression...\")\n",
        "\n",
        "    results = []\n",
        "    for exp_date in df_options['expiration_date'].unique():\n",
        "        exp_opts = df_options[df_options['expiration_date'] == exp_date].copy()\n",
        "\n",
        "        if len(exp_opts) < 5:  # Need enough points\n",
        "            continue\n",
        "\n",
        "        # Calculate moneyness\n",
        "        exp_opts['moneyness'] = exp_opts['strike_price'] / spot\n",
        "        exp_opts['log_moneyness'] = np.log(exp_opts['moneyness'])\n",
        "\n",
        "        # Filter valid IV\n",
        "        exp_opts_valid = exp_opts[exp_opts['iv'].notna()]\n",
        "\n",
        "        if len(exp_opts_valid) < 5:\n",
        "            continue\n",
        "\n",
        "        # Fit quadratic regression: IV = a + b*k + c*k^2\n",
        "        try:\n",
        "            coeffs = np.polyfit(exp_opts_valid['log_moneyness'], exp_opts_valid['iv'], 2)\n",
        "            a, b, c = coeffs[2], coeffs[1], coeffs[0]\n",
        "\n",
        "            # Calculate R-squared\n",
        "            y_pred = np.polyval(coeffs, exp_opts_valid['log_moneyness'])\n",
        "            ss_res = np.sum((exp_opts_valid['iv'] - y_pred) ** 2)\n",
        "            ss_tot = np.sum((exp_opts_valid['iv'] - np.mean(exp_opts_valid['iv'])) ** 2)\n",
        "            r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
        "\n",
        "            results.append({\n",
        "                'expiration_date': exp_date,\n",
        "                'days_to_exp': (pd.to_datetime(exp_date) - pd.Timestamp.now()).days,\n",
        "                'smile_intercept': a,\n",
        "                'smile_slope': b,\n",
        "                'smile_curvature': c,\n",
        "                'r_squared': r_squared\n",
        "            })\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed smile regression for {exp_date}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_smile = pd.DataFrame(results).sort_values('days_to_exp') if results else pd.DataFrame()\n",
        "\n",
        "    if not df_smile.empty:\n",
        "        logger.success(f\"✅ Smile regression calculated for {len(df_smile)} expirations\")\n",
        "\n",
        "    return df_smile\n",
        "\n",
        "\n",
        "def calculate_iv_rank_percentile(df_historical_iv: pd.DataFrame, current_iv: float, lookback_days: int = 252) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate IV Rank and IV Percentile\n",
        "    \"\"\"\n",
        "    if df_historical_iv.empty or not current_iv:\n",
        "        return {}\n",
        "\n",
        "    # Get recent IV values\n",
        "    recent_iv = df_historical_iv.tail(lookback_days)['iv'].dropna()\n",
        "\n",
        "    if len(recent_iv) < 2:\n",
        "        return {}\n",
        "\n",
        "    iv_max = recent_iv.max()\n",
        "    iv_min = recent_iv.min()\n",
        "\n",
        "    # IV Rank: % of days with IV below current\n",
        "    iv_rank = (current_iv - iv_min) / (iv_max - iv_min) * 100 if iv_max > iv_min else np.nan\n",
        "\n",
        "    # IV Percentile: % of days with IV below current\n",
        "    iv_percentile = (recent_iv < current_iv).sum() / len(recent_iv) * 100\n",
        "\n",
        "    return {\n",
        "        'iv_rank': iv_rank,\n",
        "        'iv_percentile': iv_percentile,\n",
        "        'current_iv': current_iv,\n",
        "        'iv_min': iv_min,\n",
        "        'iv_max': iv_max,\n",
        "        'lookback_days': lookback_days\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_lqs_grade(df_flow: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate Liquidity Quality Score (LQS) Grade\n",
        "\n",
        "    Metrics:\n",
        "    - Average bid-ask spread %\n",
        "    - Average bid-ask size\n",
        "    - % time lit vs off-exchange\n",
        "    - Average trade size\n",
        "    \"\"\"\n",
        "    if df_flow.empty:\n",
        "        return {}\n",
        "\n",
        "    logger.info(\"Calculating LQS Grade...\")\n",
        "\n",
        "    # Calculate average spread % (proxy from volume data)\n",
        "    avg_spread_pct = df_flow['spread_pct'].mean() if 'spread_pct' in df_flow.columns else np.nan\n",
        "    avg_bid_ask_size = df_flow['bid_ask_size'].mean() if 'bid_ask_size' in df_flow.columns else np.nan\n",
        "    pct_lit_exchange_pct = df_flow['lit_exchange_pct'].mean() if 'lit_exchange_pct' in df_flow.columns else np.nan\n",
        "\n",
        "    # Proxy for spread from Lit vs Off-exchange\n",
        "    if 'Volume_Lit_X' in df_flow.columns:\n",
        "        avg_lit_exchange_pct = df_flow['Volume_Lit_X'].mean() if 'Volume_Lit_X' in df_flow.columns else np.nan\n",
        "    else:\n",
        "        avg_lit_exchange_pct = np.nan\n",
        "\n",
        "    # Rough estimate for LQS Grade (A+ to D)\n",
        "    # Based on spread (lower is better) and lit volume (higher is better)\n",
        "    if not np.isnan(avg_spread_pct):\n",
        "        if avg_spread_pct < 2:\n",
        "            lqs_grade = 'A+'\n",
        "        elif avg_spread_pct < 5:\n",
        "            lqs_grade = 'A'\n",
        "        elif avg_spread_pct < 10:\n",
        "            lqs_grade = 'B'\n",
        "        elif avg_spread_pct < 20:\n",
        "            lqs_grade = 'C'\n",
        "        else:\n",
        "            lqs_grade = 'D'\n",
        "    else:\n",
        "        lqs_grade = 'N/A'\n",
        "\n",
        "    return {\n",
        "        'lqs_grade': lqs_grade,\n",
        "        'avg_spread_pct': avg_spread_pct,\n",
        "        'avg_bid_ask_size': avg_bid_ask_size,\n",
        "        'pct_lit_exchange': pct_lit_exchange_pct\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_slippage_fill_probability(df_flow: pd.DataFrame, lqs_metrics: Dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate expected slippage and fill probability by contract\n",
        "    \"\"\"\n",
        "    if df_flow.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logger.info(\"Calculating slippage/fill probability...\")\n",
        "\n",
        "    # Calculate expected slippage (proxy from spread)\n",
        "    df_result = df_flow.copy()\n",
        "\n",
        "    # Expected slippage (based on volume)\n",
        "    if 'expected_slippage_bps' not in df_result.columns:\n",
        "        df_result['expected_slippage_bps'] = np.nan\n",
        "\n",
        "    # Fill probability (based on volume)\n",
        "    if 'total_volume_trades' in df_result.columns:\n",
        "        max_vol = df_result['total_volume_trades'].max()\n",
        "        df_result['fill_probability_%'] = np.clip(\n",
        "            df_result['total_volume_trades'] / max_vol * 100 if max_vol > 0 else 0,\n",
        "            0, 100\n",
        "        )\n",
        "    else:\n",
        "        df_result['fill_probability_%'] = np.nan\n",
        "\n",
        "    if not df_result.empty:\n",
        "        logger.success(f\"✅ Slippage/fill calculated for {len(df_result)} contracts\")\n",
        "\n",
        "    return df_result[['contract', 'expected_slippage_bps', 'fill_probability_%', 'total_volume_trades']] if not df_result.empty else pd.DataFrame()\n",
        "\n",
        "\n",
        "logger.info(\"✅ Volatility & microstructure metrics loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 139,
          "status": "ok",
          "timestamp": 1763076831980,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "wEdMzezCv5hu",
        "outputId": "cc0c88f8-19a2-4613-dc37-9a5198481282"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Order flow proxies loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 - Order Flow Proxies (OAI, Block Trades)\n",
        "# =========================\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "def calculate_oai_proxy(df_flow: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate Order Anticipation Index (OAI) proxy\n",
        "\n",
        "    OAI = (Aggressor Buy Volume - Aggressor Sell Volume) / Total Volume\n",
        "    Using Lee-Ready classification from flow analysis\n",
        "    \"\"\"\n",
        "    if df_flow.empty or 'buy_volume' not in df_flow.columns:\n",
        "        return {'oai': np.nan, 'buy_pressure': np.nan, 'sell_pressure': np.nan}\n",
        "\n",
        "    logger.info(\"Calculating OAI proxy...\")\n",
        "\n",
        "    total_buy = df_flow['buy_volume'].sum()\n",
        "    total_sell = df_flow['sell_volume'].sum()\n",
        "    total_vol = total_buy + total_sell\n",
        "\n",
        "    if total_vol == 0:\n",
        "        return {'oai': np.nan, 'buy_pressure': 0, 'sell_pressure': 0}\n",
        "\n",
        "    oai = (total_buy - total_sell) / total_vol\n",
        "    buy_pressure = total_buy / total_vol\n",
        "    sell_pressure = total_sell / total_vol\n",
        "\n",
        "    logger.success(\n",
        "        f\"✅ OAI: {oai:.3f}, Buy pressure: {buy_pressure:.2%}, Sell pressure: {sell_pressure:.2%}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'oai': oai,\n",
        "        'buy_pressure': buy_pressure,\n",
        "        'sell_pressure': sell_pressure,\n",
        "        'total_buy_volume': total_buy,\n",
        "        'total_sell_volume': total_sell,\n",
        "        'total_volume': total_vol,\n",
        "        'interpretation': 'Bullish' if oai > 0.2 else 'Bearish' if oai < -0.2 else 'Neutral'\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_block_trade_ratio(df_flow: pd.DataFrame, block_threshold: int = 100) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate block trade ratio (trades >= threshold as % of total)\n",
        "    \"\"\"\n",
        "    if df_flow.empty or 'total_volume_trades' not in df_flow.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logger.info(f\"Calculating block trade ratio (threshold: {block_threshold} contracts)...\")\n",
        "\n",
        "    df_result = df_flow.copy()\n",
        "\n",
        "    # Identify block trades\n",
        "    df_result['is_block_trade'] = df_result['total_volume_trades'] >= block_threshold\n",
        "    df_result['block_trade_volume'] = df_result['total_volume_trades'].where(\n",
        "        df_result['is_block_trade'], 0\n",
        "    )\n",
        "\n",
        "    # Aggregate\n",
        "    total_volume = df_result['total_volume_trades'].sum()\n",
        "    block_volume = df_result['block_trade_volume'].sum()\n",
        "    n_blocks = df_result['is_block_trade'].sum()\n",
        "\n",
        "    block_ratio = block_volume / total_volume if total_volume > 0 else 0\n",
        "\n",
        "    logger.success(f\"✅ Block trades: {n_blocks}, Block ratio: {block_ratio:.2%}\")\n",
        "\n",
        "    # Summary\n",
        "    summary = pd.DataFrame([{\n",
        "        'total_contracts': len(df_result),\n",
        "        'block_trades_count': n_blocks,\n",
        "        'block_trade_volume': block_volume,\n",
        "        'total_volume': total_volume,\n",
        "        'block_ratio_%': block_ratio * 100,\n",
        "        'threshold_contracts': block_threshold\n",
        "    }])\n",
        "\n",
        "    # Detailed by contract\n",
        "    df_blocks = df_result[df_result['is_block_trade']].copy()\n",
        "    df_blocks = df_blocks.sort_values('block_trade_volume', ascending=False)\n",
        "\n",
        "    return {\n",
        "        'summary': summary,\n",
        "        'block_trades_detail': df_blocks[\n",
        "            ['contract', 'type', 'strike', 'expiration',\n",
        "             'total_volume_trades', 'imbalance', 'buy_volume', 'sell_volume']\n",
        "        ]\n",
        "    }\n",
        "\n",
        "logger.info(\"✅ Order flow proxies loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 106,
          "status": "ok",
          "timestamp": 1763076832090,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "71CDfRpqv5hu",
        "outputId": "70fcd5fa-b54e-4bfa-a059-8f753dd6a7ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Earnings analytics loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 - Earnings Analytics\n",
        "# =========================\n",
        "from typing import Dict, Any  # imports mínimos para tipos (no cambia la lógica)\n",
        "\n",
        "def calculate_expected_earnings_gap(df_earnings_hist: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate expected earnings GAP based on historical moves\n",
        "\n",
        "    Method: Median |move| from last 8-12 earnings, winsorized at 5%\n",
        "    \"\"\"\n",
        "    if df_earnings_hist.empty:\n",
        "        return {'expected_gap_pct': np.nan, 'method': 'N/A'}\n",
        "\n",
        "    logger.info(\"Calculating expected earnings GAP...\")\n",
        "\n",
        "    # Need actual vs estimated EPS\n",
        "    if 'eps' not in df_earnings_hist.columns or 'epsEstimated' not in df_earnings_hist.columns:\n",
        "        logger.warning(\"Missing EPS columns for gap calculation\")\n",
        "        return {'expected_gap_pct': np.nan, 'method': 'N/A'}\n",
        "\n",
        "    # Calculate surprise %\n",
        "    df_hist = df_earnings_hist.copy()\n",
        "    df_hist['eps'] = pd.to_numeric(df_hist['eps'], errors='coerce')\n",
        "    df_hist['epsEstimated'] = pd.to_numeric(df_hist['epsEstimated'], errors='coerce')\n",
        "    df_hist = df_hist.dropna(subset=['eps', 'epsEstimated'])\n",
        "\n",
        "    if len(df_hist) < 4:\n",
        "        logger.warning(\"Insufficient earnings history\")\n",
        "        return {'expected_gap_pct': np.nan, 'method': 'N/A'}\n",
        "\n",
        "    df_hist['surprise_pct'] = abs((df_hist['eps'] - df_hist['epsEstimated']) / df_hist['epsEstimated']) * 100\n",
        "    df_hist = df_hist[df_hist['surprise_pct'].notna()]\n",
        "\n",
        "    # Use last 8-12 earnings\n",
        "    recent_hist = df_hist.tail(12)\n",
        "    if len(recent_hist) < 4:\n",
        "        return {'expected_gap_pct': np.nan, 'method': 'N/A'}\n",
        "\n",
        "    # Winsorize at 5% (remove extreme outliers)\n",
        "    lower = recent_hist['surprise_pct'].quantile(0.05)\n",
        "    upper = recent_hist['surprise_pct'].quantile(0.95)\n",
        "    winsorized = recent_hist['surprise_pct'].clip(lower, upper)\n",
        "\n",
        "    # Median expected gap\n",
        "    expected_gap = winsorized.median()\n",
        "\n",
        "    logger.success(f\"✅ Expected earnings GAP: {expected_gap:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'expected_gap_pct': expected_gap,\n",
        "        'method': 'median_winsorized_5pct',\n",
        "        'n_earnings': len(recent_hist),\n",
        "        'historical_gaps': recent_hist['surprise_pct'].tolist()[-8:],  # Last 8\n",
        "        'mean_gap': recent_hist['surprise_pct'].mean(),\n",
        "        'std_gap': recent_hist['surprise_pct'].std(),\n",
        "        'min_gap': recent_hist['surprise_pct'].min(),\n",
        "        'max_gap': recent_hist['surprise_pct'].max()\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_iv_crush_history(df_options_pre: pd.DataFrame, df_options_post: pd.DataFrame = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate expected IV-crush (Δ ATM IV pre vs post earnings)\n",
        "\n",
        "    If post-earnings data not available, use historical average\n",
        "    \"\"\"\n",
        "    if df_options_pre.empty:\n",
        "        return {'iv_crush_pct': np.nan, 'method': 'N/A'}\n",
        "\n",
        "    logger.info(\"Calculating IV crush expectation...\")\n",
        "\n",
        "    # Get ATM IV from current data (pre-earnings)\n",
        "    df_calls = df_options_pre[df_options_pre['contract_type'].isin(['call', 'C'])]\n",
        "\n",
        "    if df_calls.empty or 'delta' not in df_calls.columns:\n",
        "        return {'iv_crush_pct': np.nan, 'method': 'N/A'}\n",
        "\n",
        "    # Find ATM call (delta ~ 0.5)\n",
        "    atm_call = df_calls.iloc[(df_calls['delta'] - 0.5).abs().argsort()[:1]]\n",
        "\n",
        "    if atm_call.empty:\n",
        "        return {'iv_crush_pct': np.nan, 'method': 'N/A'}\n",
        "\n",
        "    current_atm_iv = float(atm_call['iv'].iloc[0])\n",
        "    if current_atm_iv > 5:\n",
        "        current_atm_iv /= 100\n",
        "\n",
        "    # If post-earnings data available, calculate actual crush\n",
        "    if df_options_post is not None and not df_options_post.empty:\n",
        "        df_calls_post = df_options_post[df_options_post['contract_type'].isin(['call', 'C'])]\n",
        "        if not df_calls_post.empty and 'delta' in df_calls_post.columns:\n",
        "            atm_call_post = df_calls_post.iloc[(df_calls_post['delta'] - 0.5).abs().argsort()[:1]]\n",
        "            if not atm_call_post.empty:\n",
        "                post_atm_iv = float(atm_call_post['iv'].iloc[0])\n",
        "                if post_atm_iv > 5:\n",
        "                    post_atm_iv /= 100\n",
        "\n",
        "                iv_crush = (current_atm_iv - post_atm_iv) / current_atm_iv * 100\n",
        "\n",
        "                logger.success(f\"✅ Actual IV crush: {iv_crush:.2f}%\")\n",
        "\n",
        "                return {\n",
        "                    'iv_crush_pct': iv_crush,\n",
        "                    'pre_earnings_atm_iv': current_atm_iv,\n",
        "                    'post_earnings_atm_iv': post_atm_iv,\n",
        "                    'method': 'actual_post_earnings'\n",
        "                }\n",
        "\n",
        "    # Otherwise, use historical average (30-50% typical crush)\n",
        "    # This would ideally come from historical earnings data\n",
        "    historical_avg_crush = 40.0  # Conservative estimate\n",
        "\n",
        "    expected_post_iv = current_atm_iv * (1 - historical_avg_crush / 100)\n",
        "\n",
        "    logger.info(f\"Using historical average IV crush: {historical_avg_crush}%\")\n",
        "\n",
        "    return {\n",
        "        'iv_crush_pct': historical_avg_crush,\n",
        "        'pre_earnings_atm_iv': current_atm_iv,\n",
        "        'expected_post_atm_iv': expected_post_iv,\n",
        "        'method': 'historical_average',\n",
        "        'event_premium_pct': (current_atm_iv / 0.3 - 1) * 100 if current_atm_iv > 0 else np.nan  # vs baseline 30% IV\n",
        "    }\n",
        "\n",
        "logger.info(\"✅ Earnings analytics loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 381,
          "status": "ok",
          "timestamp": 1763076832475,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "oIsUNz1Bv5hv",
        "outputId": "027fa743-ae99-43a5-f469-c370221fc176"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Scorecard preview generator loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 - Scorecard Preview (Probability Model)\n",
        "# =========================\n",
        "\n",
        "def calculate_scorecard_preview(\n",
        "    ticker: str,\n",
        "    spot_price: float,\n",
        "    expected_gap: Dict,\n",
        "    iv_metrics: Dict,\n",
        "    flow_metrics: Dict,\n",
        "    gex_metrics: Dict,\n",
        "    earnings_date: str = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate scorecard preview with probability metrics\n",
        "\n",
        "    Returns:\n",
        "    - prob_up_pct: Probability of upward move\n",
        "    - prob_down_pct: Probability of downward move\n",
        "    - gap_esperado_pct: Expected gap magnitude\n",
        "    - confianza_global_pct: Global confidence score\n",
        "\n",
        "    Note: This is a PREVIEW based on signal aggregation, not a trained model\n",
        "    \"\"\"\n",
        "    logger.info(\"Generating scorecard preview...\")\n",
        "\n",
        "    # Initialize scores\n",
        "    bullish_score = 0\n",
        "    bearish_score = 0\n",
        "    total_signals = 0\n",
        "\n",
        "    # 1. OAI Signal\n",
        "    if 'oai' in flow_metrics and not pd.isna(flow_metrics['oai']):\n",
        "        oai = flow_metrics['oai']\n",
        "        if oai > 0.2:\n",
        "            bullish_score += 2\n",
        "        elif oai < -0.2:\n",
        "            bearish_score += 2\n",
        "        else:\n",
        "            bullish_score += 1\n",
        "            bearish_score += 1\n",
        "        total_signals += 2\n",
        "\n",
        "    # 2. GEX Signal (gamma flip vs spot)\n",
        "    if 'gamma_flip_strike' in gex_metrics:\n",
        "        flip = gex_metrics['gamma_flip_strike']\n",
        "        if isinstance(flip, (int, float)):\n",
        "            if spot_price > flip:\n",
        "                bullish_score += 1\n",
        "            elif spot_price < flip:\n",
        "                bearish_score += 1\n",
        "            total_signals += 1\n",
        "\n",
        "    # 3. IV Skew Signal\n",
        "    if 'iv_skew_25d' in iv_metrics and not pd.isna(iv_metrics.get('iv_skew_25d')):\n",
        "        skew = iv_metrics['iv_skew_25d']\n",
        "        if skew > 0.05:  # Puts more expensive -> fear\n",
        "            bearish_score += 1\n",
        "        elif skew < -0.05:  # Calls more expensive -> greed\n",
        "            bullish_score += 1\n",
        "        total_signals += 1\n",
        "\n",
        "    # 4. Block Trade Signal\n",
        "    if 'buy_pressure' in flow_metrics:\n",
        "        buy_pressure = flow_metrics['buy_pressure']\n",
        "        if buy_pressure > 0.6:\n",
        "            bullish_score += 1\n",
        "        elif buy_pressure < 0.4:\n",
        "            bearish_score += 1\n",
        "        total_signals += 1\n",
        "\n",
        "    # 5. IV Rank Signal\n",
        "    if 'iv_rank' in iv_metrics and not pd.isna(iv_metrics['iv_rank']):\n",
        "        iv_rank = iv_metrics['iv_rank']\n",
        "        if iv_rank > 70:\n",
        "            # High IV -> potential reversion down\n",
        "            bearish_score += 0.5\n",
        "        elif iv_rank < 30:\n",
        "            # Low IV -> potential move up\n",
        "            bullish_score += 0.5\n",
        "        total_signals += 0.5\n",
        "\n",
        "    # Calculate probabilities (monotonic with signals)\n",
        "    if total_signals > 0:\n",
        "        prob_up = bullish_score / total_signals\n",
        "        prob_down = bearish_score / total_signals\n",
        "\n",
        "        # Normalize so prob_up + prob_down ≈ 1\n",
        "        total_prob = prob_up + prob_down\n",
        "        if total_prob > 0:\n",
        "            prob_up = prob_up / total_prob\n",
        "            prob_down = prob_down / total_prob\n",
        "        else:\n",
        "            prob_up = prob_down = 0.5\n",
        "    else:\n",
        "        prob_up = prob_down = 0.5\n",
        "\n",
        "    # Expected gap\n",
        "    gap_esperado = expected_gap.get('expected_gap_pct', 5.0)  # Default 5%\n",
        "    if pd.isna(gap_esperado):\n",
        "        gap_esperado = 5.0\n",
        "\n",
        "    # Global confidence (based on data quality)\n",
        "    confidence_factors = []\n",
        "\n",
        "    # Factor 1: Number of signals\n",
        "    if total_signals >= 4:\n",
        "        confidence_factors.append(0.25)\n",
        "    elif total_signals >= 2:\n",
        "        confidence_factors.append(0.15)\n",
        "    else:\n",
        "        confidence_factors.append(0.05)\n",
        "\n",
        "    # Factor 2: OAI confidence\n",
        "    if 'oai' in flow_metrics and abs(flow_metrics['oai']) > 0.3:\n",
        "        confidence_factors.append(0.20)\n",
        "    else:\n",
        "        confidence_factors.append(0.10)\n",
        "\n",
        "    # Factor 3: Data completeness\n",
        "    if len(iv_metrics) >= 3 and len(gex_metrics) >= 3:\n",
        "        confidence_factors.append(0.25)\n",
        "    else:\n",
        "        confidence_factors.append(0.10)\n",
        "\n",
        "    # Factor 4: Volume/liquidity\n",
        "    if 'total_volume' in flow_metrics and flow_metrics['total_volume'] > 1000:\n",
        "        confidence_factors.append(0.20)\n",
        "    else:\n",
        "        confidence_factors.append(0.08)\n",
        "\n",
        "    # Base confidence\n",
        "    confidence_factors.append(0.10)\n",
        "\n",
        "    confianza_global = sum(confidence_factors) * 100  # Convert to percentage\n",
        "    confianza_global = min(confianza_global, 95)  # Cap at 95%\n",
        "\n",
        "    logger.success(\n",
        "        f\"✅ Scorecard: ↑{prob_up*100:.1f}% ↓{prob_down*100:.1f}%, \"\n",
        "        f\"Gap: {gap_esperado:.1f}%, Conf: {confianza_global:.1f}%\"\n",
        "    )\n",
        "\n",
        "    scorecard = pd.DataFrame([{\n",
        "        'ticker': ticker,\n",
        "        'spot_price': spot_price,\n",
        "        'earnings_date': earnings_date or 'TBD',\n",
        "        'prob_up_pct': round(prob_up * 100, 2),\n",
        "        'prob_down_pct': round(prob_down * 100, 2),\n",
        "        'gap_esperado_pct': round(gap_esperado, 2),\n",
        "        'confianza_global_pct': round(confianza_global, 2),\n",
        "        'total_signals': total_signals,\n",
        "        'bullish_score': bullish_score,\n",
        "        'bearish_score': bearish_score,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model_version': 'v3.0_preview',\n",
        "        'disclaimer': 'PREVIEW ONLY - Not a trained model'\n",
        "    }])\n",
        "\n",
        "    return scorecard\n",
        "\n",
        "\n",
        "logger.info(\"✅ Scorecard preview generator loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 45,
          "status": "ok",
          "timestamp": 1763076832527,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "Ry2oei-3v5hv",
        "outputId": "47ae1cf4-1db2-4376-e0e8-a6aa960fa85f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Status logging system loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# HYPERION V3 - Status Logging\n",
        "# =========================\n",
        "from typing import Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "class StatusLogger:\n",
        "    \"\"\"Track API calls and failures for status:degraded handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.log = []\n",
        "        self.errors = []\n",
        "        self.warnings = []\n",
        "\n",
        "    def log_success(self, source: str, message: str):\n",
        "        self.log.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'source': source,\n",
        "            'status': 'success',\n",
        "            'message': message\n",
        "        })\n",
        "\n",
        "    def log_error(self, source: str, message: str, exception: Exception = None):\n",
        "        self.errors.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'source': source,\n",
        "            'status': 'error',\n",
        "            'message': message,\n",
        "            'exception': str(exception) if exception else None\n",
        "        })\n",
        "\n",
        "    def log_warning(self, source: str, message: str):\n",
        "        self.warnings.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'source': source,\n",
        "            'status': 'warning',\n",
        "            'message': message\n",
        "        })\n",
        "\n",
        "    def get_status_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get overall status summary\"\"\"\n",
        "        total_calls = len(self.log)\n",
        "        total_errors = len(self.errors)\n",
        "        total_warnings = len(self.warnings)\n",
        "\n",
        "        if total_errors > 0:\n",
        "            overall_status = 'degraded'\n",
        "        elif total_warnings > 3:\n",
        "            overall_status = 'warning'\n",
        "        else:\n",
        "            overall_status = 'healthy'\n",
        "\n",
        "        return {\n",
        "            'overall_status': overall_status,\n",
        "            'total_calls': total_calls,\n",
        "            'total_errors': total_errors,\n",
        "            'total_warnings': total_warnings,\n",
        "            'success_rate_%': ((total_calls - total_errors) / total_calls * 100) if total_calls > 0 else 0\n",
        "        }\n",
        "\n",
        "    def to_dataframe(self) -> pd.DataFrame:\n",
        "        \"\"\"Convert log to DataFrame\"\"\"\n",
        "        all_logs = self.log + self.errors + self.warnings\n",
        "        return (\n",
        "            pd.DataFrame(all_logs).sort_values('timestamp', ascending=False)\n",
        "            if all_logs else pd.DataFrame()\n",
        "        )\n",
        "\n",
        "# Global status logger\n",
        "status_logger = StatusLogger()\n",
        "logger.info(\"✅ Status logging system loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 556,
          "status": "ok",
          "timestamp": 1763076833086,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "WLiMHvztv5hw",
        "outputId": "107a57ba-3267-494d-e062-e1deb52f308f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Integration layer loaded\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ StatusLogger class loaded\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# STATUS LOGGER CLASS\n",
        "# =========================\n",
        "\n",
        "from typing import Dict, List\n",
        "from datetime import datetime\n",
        "\n",
        "class StatusLogger:\n",
        "    \"\"\"\n",
        "    Tracks extraction status for all data sources\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.logs = []\n",
        "\n",
        "    def log_success(self, source: str, message: str):\n",
        "        \"\"\"Log successful extraction\"\"\"\n",
        "        self.logs.append({\n",
        "            'timestamp': datetime.now(),\n",
        "            'source': source,\n",
        "            'status': 'SUCCESS',\n",
        "            'message': message,\n",
        "            'error': None\n",
        "        })\n",
        "\n",
        "    def log_warning(self, source: str, message: str):\n",
        "        \"\"\"Log warning\"\"\"\n",
        "        self.logs.append({\n",
        "            'timestamp': datetime.now(),\n",
        "            'source': source,\n",
        "            'status': 'WARNING',\n",
        "            'message': message,\n",
        "            'error': None\n",
        "        })\n",
        "\n",
        "    def log_error(self, source: str, message: str, error=None):\n",
        "        \"\"\"Log error\"\"\"\n",
        "        self.logs.append({\n",
        "            'timestamp': datetime.now(),\n",
        "            'source': source,\n",
        "            'status': 'ERROR',\n",
        "            'message': message,\n",
        "            'error': str(error) if error else None\n",
        "        })\n",
        "\n",
        "    def to_dataframe(self):\n",
        "        \"\"\"Convert logs to DataFrame\"\"\"\n",
        "        if not self.logs:\n",
        "            return pd.DataFrame()\n",
        "        return pd.DataFrame(self.logs)\n",
        "\n",
        "    def get_status_summary(self):\n",
        "        \"\"\"Get summary statistics\"\"\"\n",
        "        if not self.logs:\n",
        "            return {\n",
        "                'total_sources': 0,\n",
        "                'successful': 0,\n",
        "                'warnings': 0,\n",
        "                'errors': 0,\n",
        "                'overall_status': 'no data'\n",
        "            }\n",
        "\n",
        "        df = self.to_dataframe()\n",
        "        return {\n",
        "            'total_sources': len(df),\n",
        "            'successful': len(df[df['status'] == 'SUCCESS']),\n",
        "            'warnings': len(df[df['status'] == 'WARNING']),\n",
        "            'errors': len(df[df['status'] == 'ERROR']),\n",
        "            'overall_status': 'success' if len(df[df['status'] == 'ERROR']) == 0 else 'partial'\n",
        "        }\n",
        "\n",
        "print(\"✅ StatusLogger class loaded\")\n",
        "\n",
        "# =========================\n",
        "# HYPERION V3 - Integration Layer\n",
        "# =========================\n",
        "\n",
        "def generate_enhanced_sheets(\n",
        "    df_options_chain: pd.DataFrame,\n",
        "    df_flow_analysis: pd.DataFrame,\n",
        "    df_earnings_hist: pd.DataFrame,\n",
        "    spot_price: float,\n",
        "    ticker: str,\n",
        "    status_logger: StatusLogger\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generate all new enhanced sheets\n",
        "\n",
        "    Returns dict of {sheet_name: dataframe}\n",
        "    \"\"\"\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"🚀 GENERATING ENHANCED SHEETS\")\n",
        "    logger.info(\"=\" * 60)\n",
        "\n",
        "    enhanced_sheets = {}\n",
        "\n",
        "    # Constants\n",
        "    rfr = 0.05  # 5% risk-free rate (update as needed)\n",
        "    div_yield = 0.0  # Dividend yield (update from profile data)\n",
        "\n",
        "    try:\n",
        "        # 1. Calculate advanced Greeks for all contracts\n",
        "        logger.info(\"1️⃣  Calculating 2nd/3rd order Greeks...\")\n",
        "        df_greeks = calculate_greeks_for_chain(df_options_chain, spot_price, rfr, div_yield)\n",
        "\n",
        "        if not df_greeks.empty:\n",
        "            # Select advanced Greek columns\n",
        "            greek_cols = ['options_ticker', 'expiration_date', 'strike_price', 'contract_type',\n",
        "                         'open_interest', 'volume', 'iv', 'delta', 'gamma', 'vega', 'theta',\n",
        "                         'vanna_advanced', 'vomma_advanced', 'charm_advanced',\n",
        "                         'speed_advanced', 'color_advanced', 'ultima_advanced']\n",
        "            greek_cols = [c for c in greek_cols if c in df_greeks.columns]\n",
        "            enhanced_sheets['greeks_second_order'] = df_greeks[greek_cols]\n",
        "            status_logger.log_success('greeks_second_order', f'{len(df_greeks)} contracts processed')\n",
        "        else:\n",
        "            status_logger.log_warning('greeks_second_order', 'No Greeks calculated')\n",
        "\n",
        "        # 2. Aggregated exposures\n",
        "        logger.info(\"2️⃣  Calculating aggregated exposures...\")\n",
        "        agg_exp = calculate_aggregated_exposures(df_greeks if not df_greeks.empty else df_options_chain, spot_price)\n",
        "\n",
        "        if agg_exp:\n",
        "            if 'gex_by_exp' in agg_exp:\n",
        "                enhanced_sheets['gex_by_exp'] = agg_exp['gex_by_exp']\n",
        "                status_logger.log_success('gex_by_exp', 'GEX by expiration calculated')\n",
        "\n",
        "            if 'vanna_charm_vomma_exp' in agg_exp:\n",
        "                enhanced_sheets['vanna_charm_vomma_exp'] = agg_exp['vanna_charm_vomma_exp']\n",
        "                status_logger.log_success('vanna_charm_vomma_exp', 'Higher-order exposures calculated')\n",
        "\n",
        "        # 3. Gamma flip\n",
        "        logger.info(\"3️⃣  Finding gamma flip level...\")\n",
        "        gamma_flip_data = find_gamma_flip_level(df_greeks if not df_greeks.empty else df_options_chain, spot_price)\n",
        "\n",
        "        # Create summary DataFrame\n",
        "        gamma_flip_summary = pd.DataFrame([{\n",
        "            'ticker': ticker,\n",
        "            'spot_price': gamma_flip_data['current_spot'],\n",
        "            'gamma_flip_strike': gamma_flip_data['gamma_flip_strike'],\n",
        "            'spot_vs_flip': gamma_flip_data.get('spot_vs_flip', 'N/A'),\n",
        "            'total_call_gex': gamma_flip_data.get('total_call_gex', 0),\n",
        "            'total_put_gex': gamma_flip_data.get('total_put_gex', 0),\n",
        "            'strikes_analyzed': gamma_flip_data.get('strikes_analyzed', 0)\n",
        "        }])\n",
        "        enhanced_sheets['gamma_flip'] = gamma_flip_summary\n",
        "\n",
        "        # Also include detailed table if available\n",
        "        if 'gex_by_strike_table' in gamma_flip_data:\n",
        "            # This could be a separate sheet or combined\n",
        "            pass\n",
        "\n",
        "        status_logger.log_success('gamma_flip', f\"Flip level: {gamma_flip_data['gamma_flip_strike']}\")\n",
        "\n",
        "        # 4. IV term structure\n",
        "        logger.info(\"4️⃣  Calculating IV term structure...\")\n",
        "        df_iv_term = calculate_iv_term_structure(df_options_chain)\n",
        "        if not df_iv_term.empty:\n",
        "            enhanced_sheets['iv_term'] = df_iv_term\n",
        "            status_logger.log_success('iv_term', f'{len(df_iv_term)} expirations')\n",
        "        else:\n",
        "            status_logger.log_warning('iv_term', 'No IV term data')\n",
        "\n",
        "        # 5. IV skew (25-delta)\n",
        "        logger.info(\"5️⃣  Calculating 25-delta IV skew...\")\n",
        "        df_iv_skew = calculate_iv_skew_25delta(df_options_chain)\n",
        "        if not df_iv_skew.empty:\n",
        "            enhanced_sheets['iv_skew'] = df_iv_skew\n",
        "            status_logger.log_success('iv_skew', f'{len(df_iv_skew)} expirations')\n",
        "        else:\n",
        "            status_logger.log_warning('iv_skew', 'No IV skew data')\n",
        "\n",
        "        # 6. Smile regression\n",
        "        logger.info(\"6️⃣  Calculating IV smile regression...\")\n",
        "        df_smile = calculate_iv_smile_regression(df_options_chain, spot_price)\n",
        "        if not df_smile.empty:\n",
        "            enhanced_sheets['smile_regression'] = df_smile\n",
        "            status_logger.log_success('smile_regression', f'{len(df_smile)} expirations')\n",
        "        else:\n",
        "            status_logger.log_warning('smile_regression', 'No smile data')\n",
        "\n",
        "        # 7. IV Rank/Percentile (requires historical data - use available or skip)\n",
        "        logger.info(\"7️⃣  IV Rank/Percentile (requires historical data - skipping for now)\")\n",
        "        # This would need historical IV data from a database or previous runs\n",
        "        # For now, create placeholder\n",
        "        status_logger.log_warning('iv_rank_percentile', 'Requires historical IV database')\n",
        "\n",
        "        # 8. LQS metrics\n",
        "        logger.info(\"8️⃣  Calculating LQS (Liquidity Quality Score)...\")\n",
        "        lqs_metrics = calculate_lqs_grade(df_flow_analysis)\n",
        "        if lqs_metrics:\n",
        "            df_lqs = pd.DataFrame([lqs_metrics])\n",
        "            enhanced_sheets['lqs_metrics'] = df_lqs\n",
        "            status_logger.log_success('lqs_metrics', f\"Grade: {lqs_metrics.get('lqs_grade', 'N/A')}\")\n",
        "        else:\n",
        "            status_logger.log_warning('lqs_metrics', 'No LQS data')\n",
        "\n",
        "        # 9. Slippage & fill probability\n",
        "        logger.info(\"9️⃣  Calculating slippage and fill probability...\")\n",
        "        df_slippage = calculate_slippage_fill_probability(df_flow_analysis, lqs_metrics)\n",
        "        if not df_slippage.empty:\n",
        "            enhanced_sheets['slippage_fill'] = df_slippage\n",
        "            status_logger.log_success('slippage_fill', f'{len(df_slippage)} contracts')\n",
        "        else:\n",
        "            status_logger.log_warning('slippage_fill', 'No slippage data')\n",
        "\n",
        "        # 10. OAI (Order Anticipation Index)\n",
        "        logger.info(\"🔟 Calculating OAI proxy...\")\n",
        "        oai_metrics = calculate_oai_proxy(df_flow_analysis)\n",
        "        if oai_metrics:\n",
        "            df_oai = pd.DataFrame([oai_metrics])\n",
        "            enhanced_sheets['oai_proxy'] = df_oai\n",
        "            status_logger.log_success('oai_proxy', f\"OAI: {oai_metrics.get('oai', 'N/A'):.3f}\")\n",
        "        else:\n",
        "            status_logger.log_warning('oai_proxy', 'No OAI data')\n",
        "\n",
        "        # 11. Block trade ratio\n",
        "        logger.info(\"1️⃣1️⃣  Calculating block trade ratio...\")\n",
        "        block_data = calculate_block_trade_ratio(df_flow_analysis, block_threshold=100)\n",
        "        if block_data and 'summary' in block_data:\n",
        "            enhanced_sheets['block_trade_ratio'] = block_data['summary']\n",
        "            if not block_data['block_trades_detail'].empty:\n",
        "                enhanced_sheets['block_trades_detail'] = block_data['block_trades_detail']\n",
        "            status_logger.log_success('block_trade_ratio', f\"{block_data['summary']['block_trades_count'].iloc[0]} blocks\")\n",
        "        else:\n",
        "            status_logger.log_warning('block_trade_ratio', 'No block trade data')\n",
        "\n",
        "        # 12. Earnings analytics\n",
        "        logger.info(\"1️⃣2️⃣  Calculating expected earnings gap...\")\n",
        "        gap_metrics = calculate_expected_earnings_gap(df_earnings_hist)\n",
        "        df_gap = pd.DataFrame([gap_metrics])\n",
        "        enhanced_sheets['earnings_expected_gap'] = df_gap\n",
        "        if gap_metrics.get('expected_gap_pct'):\n",
        "            status_logger.log_success('earnings_expected_gap', f\"Gap: {gap_metrics['expected_gap_pct']:.2f}%\")\n",
        "        else:\n",
        "            status_logger.log_warning('earnings_expected_gap', 'Insufficient earnings history')\n",
        "\n",
        "        # 13. IV crush history\n",
        "        logger.info(\"1️⃣3️⃣  Calculating IV crush expectation...\")\n",
        "        iv_crush_metrics = calculate_iv_crush_history(df_options_chain)\n",
        "        df_iv_crush = pd.DataFrame([iv_crush_metrics])\n",
        "        enhanced_sheets['iv_crush_history'] = df_iv_crush\n",
        "        status_logger.log_success('iv_crush_history', f\"Expected crush: {iv_crush_metrics.get('iv_crush_pct', 'N/A')}%\")\n",
        "\n",
        "        # 14. Scorecard preview\n",
        "        logger.info(\"1️⃣4️⃣  Generating scorecard preview...\")\n",
        "\n",
        "        # Aggregate metrics for scorecard\n",
        "        iv_metrics_agg = {}\n",
        "        if not df_iv_skew.empty:\n",
        "            iv_metrics_agg['iv_skew_25d'] = df_iv_skew['iv_skew_25d'].iloc[0] if len(df_iv_skew) > 0 else np.nan\n",
        "\n",
        "        flow_metrics_agg = oai_metrics if oai_metrics else {}\n",
        "        gex_metrics_agg = gamma_flip_data\n",
        "\n",
        "        df_scorecard = calculate_scorecard_preview(\n",
        "            ticker=ticker,\n",
        "            spot_price=spot_price,\n",
        "            expected_gap=gap_metrics,\n",
        "            iv_metrics=iv_metrics_agg,\n",
        "            flow_metrics=flow_metrics_agg,\n",
        "            gex_metrics=gex_metrics_agg,\n",
        "            earnings_date='TBD'  # Would come from earnings calendar\n",
        "        )\n",
        "        enhanced_sheets['scorecard_preview'] = df_scorecard\n",
        "        status_logger.log_success('scorecard_preview', 'Scorecard generated')\n",
        "\n",
        "        # 15. Status log\n",
        "        logger.info(\"1️⃣5️⃣  Compiling status log...\")\n",
        "        df_status = status_logger.to_dataframe()\n",
        "        enhanced_sheets['status_log'] = df_status\n",
        "\n",
        "        # Add summary\n",
        "        status_summary = status_logger.get_status_summary()\n",
        "        df_status_summary = pd.DataFrame([status_summary])\n",
        "        enhanced_sheets['status_summary'] = df_status_summary\n",
        "\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.success(f\"✅ Generated {len(enhanced_sheets)} enhanced sheets\")\n",
        "        logger.info(f\"📊 Overall status: {status_summary['overall_status'].upper()}\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating enhanced sheets: {e}\")\n",
        "        status_logger.log_error('enhanced_sheets', f'Generation failed: {e}', e)\n",
        "\n",
        "        # Create minimal status log\n",
        "        df_status = status_logger.to_dataframe()\n",
        "        enhanced_sheets['status_log'] = df_status\n",
        "\n",
        "    return enhanced_sheets\n",
        "\n",
        "logger.info(\"✅ Integration layer loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 21,
          "status": "ok",
          "timestamp": 1763076833112,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "QqkkNnUKv5hw",
        "outputId": "49feadcb-7512-4ce3-bb32-b2a560f4fefc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Enhanced Excel writer loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================\n",
        "# HYPERION V3 - Enhanced Excel Writer (ADD-ONLY)\n",
        "# =========================\n",
        "\n",
        "def write_enhanced_excel(output_path: str, existing_sheets: Dict[str, pd.DataFrame],\n",
        "                        enhanced_sheets: Dict[str, pd.DataFrame]):\n",
        "    \"\"\"\n",
        "    Write Excel with both existing and new enhanced sheets\n",
        "\n",
        "    ADD-ONLY approach: Existing sheets written first, then enhanced sheets added\n",
        "    \"\"\"\n",
        "    logger.info(f\"📝 Writing enhanced Excel to {output_path}\")\n",
        "    logger.info(f\"   Existing sheets: {len(existing_sheets)}\")\n",
        "    logger.info(f\"   Enhanced sheets: {len(enhanced_sheets)}\")\n",
        "\n",
        "    with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
        "        # 1. Write ALL existing sheets first (unchanged)\n",
        "        for sheet_name, df in existing_sheets.items():\n",
        "            try:\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "                logger.debug(f\"   ✓ {sheet_name}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"   ✗ Failed to write {sheet_name}: {e}\")\n",
        "\n",
        "        # 2. Write NEW enhanced sheets\n",
        "        for sheet_name, df in enhanced_sheets.items():\n",
        "            try:\n",
        "                # Ensure sheet name is valid\n",
        "                safe_name = sheet_name[:31]  # Excel limit\n",
        "                df.to_excel(writer, sheet_name=safe_name, index=False)\n",
        "                logger.debug(f\"   ✓ {safe_name} (NEW)\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"   ✗ Failed to write {safe_name}: {e}\")\n",
        "\n",
        "    logger.success(f\"✅ Excel written: {output_path}\")\n",
        "\n",
        "logger.info(\"✅ Enhanced Excel writer loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 30,
          "status": "ok",
          "timestamp": 1763076833174,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "kfV_h8L_v5hx",
        "outputId": "7fc96f96-ce47-49c6-c550-b59c229b99c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ Options chain flattener loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================\n",
        "# HYPERION V3 - Helper: Options Chain Flattener\n",
        "# =========================\n",
        "# This helper is needed if not already defined in the original code\n",
        "\n",
        "def flatten_options_details(df_snapshot_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Flatten nested options details from Polygon snapshot\n",
        "\n",
        "    If this function already exists in your code, you can skip this cell.\n",
        "    \"\"\"\n",
        "    if df_snapshot_raw.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logger.info(\"Flattening options snapshot data...\")\n",
        "\n",
        "    flattened = []\n",
        "\n",
        "    for _, row in df_snapshot_raw.iterrows():\n",
        "        try:\n",
        "            details = row.get('details', {})\n",
        "            greeks = row.get('greeks', {})\n",
        "\n",
        "            if not isinstance(details, dict):\n",
        "                details = {}\n",
        "            if not isinstance(greeks, dict):\n",
        "                greeks = {}\n",
        "\n",
        "            flat_row = {\n",
        "                'options_ticker': row.get('ticker', ''),\n",
        "                'expiration_date': details.get('expiration_date', ''),\n",
        "                'strike_price': details.get('strike_price', 0),\n",
        "                'contract_type': details.get('contract_type', ''),\n",
        "                'open_interest': row.get('open_interest', 0),\n",
        "                'volume': row.get('day', {}).get('volume', 0) if isinstance(row.get('day'), dict) else 0,\n",
        "                'close': row.get('day', {}).get('close', 0) if isinstance(row.get('day'), dict) else 0,\n",
        "                'iv': greeks.get('implied_volatility', 0),\n",
        "                'delta': greeks.get('delta', 0),\n",
        "                'gamma': greeks.get('gamma', 0),\n",
        "                'theta': greeks.get('theta', 0),\n",
        "                'vega': greeks.get('vega', 0)\n",
        "            }\n",
        "\n",
        "            flattened.append(flat_row)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error flattening row: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_flat = pd.DataFrame(flattened)\n",
        "    logger.success(f\"✅ Flattened {len(df_flat)} option contracts\")\n",
        "\n",
        "    return df_flat\n",
        "\n",
        "logger.info(\"✅ Options chain flattener loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 50,
          "status": "ok",
          "timestamp": 1763076833228,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          },
          "user_tz": -660
        },
        "id": "3Rf0S8W4v5hx",
        "outputId": "a87da7a0-8f9e-4c47-e1ca-8a271eb8e899"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ V3 Complete integration example loaded\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================\n",
        "# HYPERION V3 - Complete Integration Example\n",
        "# =========================\n",
        "# This cell shows how to integrate V3 enhancements into the existing pipeline\n",
        "# Add this code in the main run_analysis_pipeline() function\n",
        "\n",
        "def run_analysis_pipeline_v3_enhanced(tickers, config):\n",
        "    \"\"\"\n",
        "    Enhanced version of run_analysis_pipeline with V3 features\n",
        "\n",
        "    This function demonstrates how to integrate all V3 enhancements\n",
        "    while preserving all existing functionality (ADD-ONLY approach)\n",
        "    \"\"\"\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR)\n",
        "    persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'\n",
        "    raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Initialize V3 status logger\n",
        "    status_logger_v3 = StatusLogger()\n",
        "\n",
        "    # Lit exchanges\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"\\n{'='*80}\")\n",
        "        logger.info(f\"🚀 HYPERION V9 - Processing {ticker}\")\n",
        "        logger.info(f\"{'='*80}\\n\")\n",
        "\n",
        "        raw_dir = raw_base_dir / ticker\n",
        "        raw_dir.mkdir(exist_ok=True)   # crea la carpeta si no existe\n",
        "\n",
        "        # nombre de archivo con timestamp (YYYYMMDD_HHMMSS) para evitar colisiones\n",
        "        run_ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_v9_report_{run_ts}.xlsx\"\n",
        "\n",
        "        # 1) Extract base data\n",
        "        tasks = _create_api_tasks(\n",
        "            ticker,\n",
        "            config[\"start_date\"],\n",
        "            config[\"end_date\"]\n",
        "        )\n",
        "        data_sources = {}\n",
        "\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "                status_logger_v3.log_success(name, f\"{len(df)} rows fetched\")\n",
        "            else:\n",
        "                status_logger_v3.log_warning(name, \"No data returned\")\n",
        "\n",
        "        # 2) Dependent data (EXISTING CODE - UNCHANGED)\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty:\n",
        "            profile = data_sources['1_Profile'].iloc[0]\n",
        "            if profile.get('cik'):\n",
        "                cik = profile.get('cik')\n",
        "                logger.info(f\"CIK found: {cik}\")\n",
        "                cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "                cik_params = {'cik': cik, 'limit': 100}\n",
        "                df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "                if not df_cik.empty:\n",
        "                    data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Options snapshot (EXISTING CODE - UNCHANGED)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "\n",
        "        logger.info(\"Fetching options chain...\")\n",
        "        options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "        df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "        df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "            status_logger_v3.log_success('Options_Chain', f\"{len(df_options_chain)} contracts\")\n",
        "\n",
        "        # 4) Flow analysis (EXISTING CODE - UNCHANGED)\n",
        "        metrics_dashboard = {}\n",
        "        df_flow_analysis = pd.DataFrame()\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "\n",
        "            # Select top contracts for flow analysis\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (\n",
        "                df_options_chain.groupby('expiration_date')\n",
        "                .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "\n",
        "            if len(top_contracts) > 50:\n",
        "                top_contracts = top_contracts.nlargest(50, 'volume')\n",
        "\n",
        "            logger.info(f\"Analyzing flow for {len(top_contracts)} contracts...\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                status_logger_v3.log_success('Options_Flow_Analysis', f\"{len(df_flow_analysis)} contracts analyzed\")\n",
        "\n",
        "        # Get spot price\n",
        "        spot_price = 0\n",
        "        if '2_Quote' in data_sources and not data_sources['2_Quote'].empty:\n",
        "            spot_price = float(data_sources['2_Quote'].iloc[0].get('price', 0))\n",
        "\n",
        "        # 5) Calculate existing metrics (EXISTING CODE - UNCHANGED)\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, 10)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, 20)\n",
        "\n",
        "        # Advanced options metrics (EXISTING CODE - UNCHANGED)\n",
        "        if not df_options_chain.empty and spot_price:\n",
        "            adv_metrics = calculate_advanced_options_metrics(df_options_chain, spot_price)\n",
        "            if adv_metrics:\n",
        "                data_sources['Options_Metrics_Advanced'] = pd.DataFrame([adv_metrics])\n",
        "\n",
        "        # =====================================================================\n",
        "        # 🚀 V3 ENHANCED FEATURES START HERE\n",
        "        # =====================================================================\n",
        "\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"=\" * 70)\n",
        "        logger.info(\"🚀 V3 ENHANCED FEATURES\")\n",
        "        logger.info(\"=\" * 70)\n",
        "\n",
        "        enhanced_sheets_v3 = {}\n",
        "\n",
        "        try:\n",
        "            if not df_options_chain.empty and spot_price:\n",
        "                # Generate all enhanced sheets\n",
        "                enhanced_sheets_v3 = generate_enhanced_sheets(\n",
        "                    df_options_chain=df_options_chain,\n",
        "                    df_flow_analysis=df_flow_analysis if not df_flow_analysis.empty else pd.DataFrame(),\n",
        "                    df_earnings_hist=data_sources.get('12_Earnings_Cal', pd.DataFrame()),\n",
        "                    spot_price=spot_price,\n",
        "                    ticker=ticker,\n",
        "                    status_logger=status_logger_v3\n",
        "                )\n",
        "\n",
        "                logger.success(f\"✅ Generated {len(enhanced_sheets_v3)} enhanced sheets\")\n",
        "            else:\n",
        "                logger.warning(\"⚠️ Skipping V3 enhancements (no options chain or spot price)\")\n",
        "                status_logger_v3.log_warning('enhanced_features', 'Skipped due to missing data')\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ V3 enhancement error: {e}\")\n",
        "            status_logger_v3.log_error('enhanced_features', f'Error: {e}', e)\n",
        "\n",
        "            # Create minimal status log even on failure\n",
        "            enhanced_sheets_v3['status_log'] = status_logger_v3.to_dataframe()\n",
        "            enhanced_sheets_v3['status_summary'] = pd.DataFrame([status_logger_v3.get_status_summary()])\n",
        "\n",
        "        logger.info(\"=\" * 70)\n",
        "\n",
        "        # =====================================================================\n",
        "        # Excel Output (ENHANCED)\n",
        "        # =====================================================================\n",
        "\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"📝 Writing Excel output...\")\n",
        "\n",
        "        # # Write enhanced Excel with both existing and new sheets\n",
        "        # try:\n",
        "        #     write_enhanced_excel(output_excel_path, data_sources, enhanced_sheets_v3)\n",
        "        #     logger.success(f\"✅ Excel saved: {output_excel_path}\")\n",
        "        # except Exception as e:\n",
        "        #     logger.error(f\"❌ Excel write error: {e}\")\n",
        "        #\n",
        "        #     # Fallback: write without enhanced sheets\n",
        "        #     with pd.ExcelWriter(output_excel_path, engine='xlsxwriter') as writer:\n",
        "        #         for sheet_name, df in data_sources.items():\n",
        "        #             try:\n",
        "        #                 df.to_excel(writer, sheet_name=sheet_name[:31], index=False)\n",
        "        #             except:\n",
        "        #                 pass\n",
        "        #\n",
        "        #     logger.warning(f\"⚠️ Wrote Excel without enhanced sheets due to error\")\n",
        "\n",
        "        # JSON summary (EXISTING CODE - with V3 additions)\n",
        "        json_summary = {\n",
        "            'ticker': ticker,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'spot_price': spot_price,\n",
        "            'version': 'v3.0_enhanced',\n",
        "            'metrics_dashboard': metrics_dashboard,\n",
        "            'v3_status': status_logger_v3.get_status_summary(),\n",
        "            'sheets_generated': {\n",
        "                'existing': list(data_sources.keys()),\n",
        "                'enhanced': list(enhanced_sheets_v3.keys())\n",
        "            }\n",
        "        }\n",
        "\n",
        "        json_path = persistent_dir / f\"{ticker}_{datetime.now():%Y%m%d}_summary.json\"\n",
        "        json_path.write_text(json.dumps(json_summary, indent=2, default=str))\n",
        "        logger.success(f\"✅ JSON summary: {json_path}\")\n",
        "\n",
        "        # Execution time\n",
        "        elapsed = time.time() - t_start\n",
        "        logger.info(f\"\")\n",
        "        logger.info(f\"⏱️  Total execution time: {elapsed:.1f}s\")\n",
        "        logger.info(f\"=\" * 70)\n",
        "\n",
        "# Usage:\n",
        "# Replace the call to run_analysis_pipeline() with run_analysis_pipeline_v3_enhanced()\n",
        "# OR integrate the V3 section into your existing run_analysis_pipeline()\n",
        "\n",
        "logger.info(\"✅ V3 Complete integration example loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VyYzWFcrpa"
      },
      "source": [
        "=========================\n",
        "🎯 HYPERION V3 - Usage Instructions\n",
        "=========================\n",
        "\n",
        "✅ Quick Start\n",
        "\n",
        "1) Set your ticker (only change needed):\n",
        "   TICKERS_A_PROCESAR = ['VRT']  Change to your ticker\n",
        "\n",
        "2) Run all cells from top to bottom\n",
        "\n",
        "3) Option 1: Use the complete V3 integration\n",
        "   Replace the last line with:\n",
        "   run_analysis_pipeline_v3_enhanced(TICKERS_A_PROCESAR, config)\n",
        "\n",
        "4) Option 2: Keep existing pipeline and add V3 features\n",
        "   - Copy the V3 ENHANCED section from run_analysis_pipeline_v3_enhanced()\n",
        "   - Insert it into your existing run_analysis_pipeline() function\n",
        "   - Before the Excel write section\n",
        "\n",
        "📊 New Sheets Generated\n",
        "\n",
        "The enhanced version adds these 15 new sheets to your Excel output:\n",
        "\n",
        "1. greeks_second_order - Advanced Greeks (Vanna, Vomma, Charm, Speed, Color, Ultima)\n",
        "2. gex_by_exp - Gamma exposure by expiration\n",
        "3. vanna_charm_vomma_exp - Higher-order exposure aggregates\n",
        "4. gamma_flip - Gamma flip level analysis\n",
        "5. iv_term - IV term structure (ATM IV by expiration)\n",
        "6. iv_skew - 25-delta IV skew by expiration\n",
        "7. smile_regression - IV smile curvature and slope\n",
        "8. lqs_metrics - Liquidity Quality Score\n",
        "9. slippage_fill - Expected slippage and fill probability\n",
        "10. oai_proxy - Order Anticipation Index\n",
        "11. block_trade_ratio - Block trade analysis\n",
        "12. block_trades_detail - Detailed block trade list\n",
        "13. earnings_expected_gap - Expected earnings move\n",
        "14. iv_crush_history - Expected IV crush post-earnings\n",
        "15. scorecard_preview - Probability forecast (⚠️ PREVIEW - not a trained model)\n",
        "16. status_log - Detailed execution log\n",
        "17. status_summary - Overall status summary\n",
        "\n",
        "⚠️ Important Notes\n",
        "\n",
        "- ALL existing sheets are preserved (ADD-ONLY approach)\n",
        "- The scorecard is a PREVIEW based on signal aggregation, not a trained ML model\n",
        "- If API errors occur, the status will be set to degraded but processing continues\n",
        "- Historical IV data for IV Rank/Percentile requires a database (placeholder for now)\n",
        "\n",
        "🔧 Configuration\n",
        "\n",
        "You can adjust these parameters in the code:\n",
        "- block_threshold: Minimum contracts for block trades (default: 100)\n",
        "- rfr: Risk-free rate (default: 0.05 or 5%)\n",
        "- div_yield: Dividend yield (extracted from profile data)\n",
        "\n",
        "📝 Error Handling\n",
        "\n",
        "The V3 enhancement includes robust error handling:\n",
        "- If a sheet fails to generate, it's logged but doesn't stop the process\n",
        "- Check the status_log sheet for detailed error information\n",
        "- The status_summary sheet shows overall health\n",
        "\n",
        "🎯 Output Files\n",
        "\n",
        "- Excel: /content/drive/MyDrive/hyperion_data/{TICKER}_hyperion_report_{TIMESTAMP}.xlsx\n",
        "- JSON:  /content/drive/MyDrive/hyperion_data/{TICKER}_{DATE}_summary.json\n",
        "\n",
        "---\n",
        "Version: Hyperion V3.0 Enhanced\n",
        "Last Updated: October 2025\n",
        "Compatibility: All existing Hyperion V2.x features preserved\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZtk1p7XGjzD",
        "outputId": "41ee2a6e-3fc3-48ee-8502-9730bbee0dc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🚀 HYPERION V9 COMPLETE - Enhanced Options Analytics\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFeatures included:\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  1. Expected Move Calculator\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  2. Comprehensive Greeks (1st, 2nd, 3rd order)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  3. GEX Analysis\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  4. IV Term Structure\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  5. IV Skew 25-Delta\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  6. Smile Regression (SVI)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  7. Unusual Options Activity Detection\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  8. Expected Earnings Gap\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  9. IV Crush History\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m  10. Quantitative Scorecard\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\n",
            "================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🚀 HYPERION V9 - Processing CVS\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\n",
            "\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-13 23:33:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m📋 Fetching options chain...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlattening options snapshot data...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Flattened 1130 option contracts\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Options chain: 1130 contracts\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🔄 Analyzing flow for 50 contracts...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo →  (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/trades/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/trades/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mHTTPError https://api.polygon.io/v3/quotes/: 404 Client Error: Not Found for url: https://api.polygon.io/v3/quotes/?timestamp.gte=1762957800000000000&timestamp.lte=1762981200000000000&limit=50000\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para quotes_\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Flow analysis: 50 contracts\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m💰 Spot price: $79.24\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🚀 GENERATING ENHANCED SHEETS\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m1️⃣  Calculating 2nd/3rd order Greeks...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating advanced Greeks for 1130 contracts...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Advanced Greeks calculated for 1130 contracts\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m2️⃣  Calculating aggregated exposures...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating aggregated exposures...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Total GEX: 606,605 shares, $3.81B notional\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m3️⃣  Finding gamma flip level...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFinding gamma flip level...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Gamma flip level: $70.00\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m4️⃣  Calculating IV term structure...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating IV term structure...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ IV term structure calculated for 32 expirations\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m5️⃣  Calculating 25-delta IV skew...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating 25-delta IV skew...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ 25-delta IV skew calculated for 16 expirations\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m6️⃣  Calculating IV smile regression...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating IV smile regression...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Smile regression calculated for 16 expirations\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m7️⃣  IV Rank/Percentile (requires historical data - skipping for now)\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m8️⃣  Calculating LQS (Liquidity Quality Score)...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating LQS Grade...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m9️⃣  Calculating slippage and fill probability...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating slippage/fill probability...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Slippage/fill calculated for 50 contracts\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🔟 Calculating OAI proxy...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating OAI proxy...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m1️⃣1️⃣  Calculating block trade ratio...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating block trade ratio (threshold: 100 contracts)...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Block trades: 0, Block ratio: 0.00%\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m1️⃣2️⃣  Calculating expected earnings gap...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating expected earnings GAP...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Expected earnings GAP: 19.86%\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m1️⃣3️⃣  Calculating IV crush expectation...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating IV crush expectation...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mUsing historical average IV crush: 40.0%\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m1️⃣4️⃣  Generating scorecard preview...\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError generating enhanced sheets: 'iv_skew_25d'\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🚀 HYPERION V9 - ENHANCED ANALYSIS\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🚀 HYPERION V9 - ENHANCED ANALYSIS STARTING\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "🚀 HYPERION V9 - GENERATING ALL ENHANCED SHEETS\n",
            "============================================================\n",
            "\n",
            "📊 Calculating Expected Move...\n",
            "✅ Expected Move calculated for 15 expirations\n",
            "🔢 Calculating Comprehensive Greeks (1st, 2nd, 3rd order)...\n",
            "✅ Greeks calculated for 1130 contracts\n",
            "💥 Calculating Comprehensive GEX Analysis...\n",
            "✅ GEX Analysis Complete:\n",
            "   Total GEX: -606,605 shares (-3.81B)\n",
            "   Gamma Flip: 70.0\n",
            "   Positioning: Dealers SHORT gamma (market volatile)\n",
            "📈 Calculating Enhanced IV Term Structure...\n",
            "✅ IV Term Structure: Flat, 0 anomalies\n",
            "📐 Calculating Enhanced 25-Delta IV Skew...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Generated 15 enhanced analysis sheets\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ V9 Enhanced Analysis Complete: 15 sheets generated\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m📊 GENERATING EXCEL REPORT\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ IV Skew: Risk Reversal = 0.0000, Signal = Neutral\n",
            "😊 Calculating Smile Regression (SVI Model)...\n",
            "✅ Smile Regression: 15 expirations fitted\n",
            "🔍 Detecting Unusual Options Activity...\n",
            "✅ Unusual Activity: 1 detected (1 bullish, 0 bearish)\n",
            "❌ Earnings Gap failed: calculate_expected_earnings_gap() takes 1 positional argument but 3 were given\n",
            "❌ IV Crush History failed: calculate_iv_crush_history() takes from 1 to 2 positional arguments but 4 were given\n",
            "🎯 Calculating Quantitative Scorecard...\n",
            "✅ Scorecard: 63.0/100 - HOLD (Moderate confidence)\n",
            "\n",
            "============================================================\n",
            "✅ HYPERION V9 - Generated 15 enhanced sheets\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ Excel report saved: /content/drive/MyDrive/hyperion_data/CVS_hyperion_v9_report_20251113_233346.xlsx\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m   Total sheets: 68\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m📈 HYPERION V9 - ANALYSIS COMPLETE\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m   Ticker: CVS\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m   Time Elapsed: 66.8s\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m   Report: CVS_hyperion_v9_report_20251113_233346.xlsx\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m   Total Sheets: 68\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m   V9 Enhanced Sheets: 16\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m   V9 Score: 63.0/100\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m   Recommendation: HOLD\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m================================================================================\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m✅ HYPERION V9 COMPLETE - All analyses finished\u001b[0m\n",
            "\u001b[32m2025-11-13 23:34:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# EXECUTE HYPERION V9\n",
        "# =========================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    logger.info(\"\")\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"🚀 HYPERION V9 COMPLETE - Enhanced Options Analytics\")\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"\")\n",
        "    logger.info(\"Features included:\")\n",
        "    logger.info(\"  1. Expected Move Calculator\")\n",
        "    logger.info(\"  2. Comprehensive Greeks (1st, 2nd, 3rd order)\")\n",
        "    logger.info(\"  3. GEX Analysis\")\n",
        "    logger.info(\"  4. IV Term Structure\")\n",
        "    logger.info(\"  5. IV Skew 25-Delta\")\n",
        "    logger.info(\"  6. Smile Regression (SVI)\")\n",
        "    logger.info(\"  7. Unusual Options Activity Detection\")\n",
        "    logger.info(\"  8. Expected Earnings Gap\")\n",
        "    logger.info(\"  9. IV Crush History\")\n",
        "    logger.info(\"  10. Quantitative Scorecard\")\n",
        "    logger.info(\"\")\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"\")\n",
        "\n",
        "    # Run the analysis\n",
        "    run_hyperion_v9(TICKERS_A_PROCESAR)\n",
        "\n",
        "    logger.success(\"\")\n",
        "    logger.success(\"✅ HYPERION V9 COMPLETE - All analyses finished\")\n",
        "    logger.success(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c482fdbc"
      },
      "source": [
        "# Task\n",
        "To replace the options data fetching with `polygon-api-client`, I will modify the first code cell to import `RESTClient` and initialize `polygon_client`. Then, I will update the `run_hyperion_v9` function to use `polygon_client.vx.options.get_options_snapshot()` for fetching the options data.\n",
        "\n",
        "Here's the plan:\n",
        "1.  **Modify Cell `JRvsEFHImK7Y`**:\n",
        "    *   Add `from polygon import RESTClient` to the imports.\n",
        "    *   Initialize `polygon_client = RESTClient(api_key=POLY_KEY)` after `POLY_KEY` is loaded.\n",
        "2.  **Modify Cell `Grzdvp1VGjy8` (inside `run_hyperion_v9` function)**:\n",
        "    *   Locate the options snapshot fetching block.\n",
        "    *   Replace the existing `options_snapshot_url` and `fetch_paginated_data` calls with `options_snapshot_response = polygon_client.vx.options.get_options_snapshot(ticker)` and `df_snapshot_raw = pd.DataFrame(options_snapshot_response.results)`.\n",
        "\n",
        "This will ensure the options snapshot data is fetched using the dedicated Polygon RESTClient.\n",
        "\n",
        "```python\n",
        "# Add RESTClient import and client initialization to the first cell.\n",
        "# Then, replace the options data fetching logic in `run_hyperion_v9` with the new client call.\n",
        "edit_cell(\n",
        "    cell_id=\"JRvsEFHImK7Y\",\n",
        "    content=\"\"\"# -*- coding: utf-8 -*-\n",
        "\\\"\\\"\\\"HYPERION INGEST\\\"\\\"\\\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# Import Polygon RESTClient\n",
        "from polygon import RESTClient\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Initialize Polygon RESTClient\n",
        "polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            # For direct polygon.io calls that are not handled by polygon_client,\n",
        "            # ensure API key is appended as before or handled by the client.\n",
        "            # However, the plan is to move direct polygon.io calls for options snapshot to polygon_client.\n",
        "            # This block might remain for other direct polygon.io calls if any.\n",
        "            if 'apiKey' not in query_params:\n",
        "                query_params[\"apiKey\"] = POLY_KEY\n",
        "            r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "            r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        else:\n",
        "            r = requests.get(url, params=query_params, headers=headers, timeout=60) # Fallback for other APIs\n",
        "\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        trades_url = f\"https://api.polygon.io/v3/trades/{options_ticker}\"\n",
        "        quotes_url = f\"https://api.polygon.io/v3/quotes/{options_ticker}\"\n",
        "        common_params = {'timestamp.gte': start_ns, 'timestamp.lte': end_ns, 'limit': 50000}\n",
        "\n",
        "        df_trades = fetch_data_block(f\"trades_{options_ticker}\", trades_url, common_params, base_ticker, raw_dir)\n",
        "        df_quotes = fetch_data_block(f\"quotes_{options_ticker}\", quotes_url, common_params, base_ticker, raw_dir)\n",
        "\n",
        "        if df_trades.empty:\n",
        "            flow_results.append({\n",
        "                'contract': options_ticker,\n",
        "                'type': contract.get('contract_type'),\n",
        "                'strike': contract.get('strike_price'),\n",
        "                'expiration': contract.get('expiration_date'),\n",
        "                'imbalance': np.nan, 'buy_volume': 0, 'sell_volume': 0,\n",
        "                'total_volume_trades': 0,\n",
        "                'Volumen_Lit_%': 0.0, 'Volumen_Off_%': 0.0, 'Volumen_Unknown_%': 0.0,\n",
        "                'quotes_present': False, 'session_source': session_source\n",
        "            })\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra, spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain, spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n",
        "\"\"\"\n",
        ")\n",
        "edit_cell(\n",
        "    cell_id=\"Grzdvp1VGjy8\",\n",
        "    content=\"\"\"# =========================\n",
        "# HYPERION V9 - Pipeline Integration\n",
        "# =========================\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log, sqrt, exp\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import brentq\n",
        "\n",
        "\n",
        "# Modify the main pipeline to include V9 features\n",
        "def run_hyperion_v9(tickers):\n",
        "    \\\"\\\"\\\"\n",
        "    Main pipeline for Hyperion V9 with all enhancements\n",
        "    \\\"\\\"\\\"\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    # === Helper: V9 Edge Pack (FPR + VCP) ===\n",
        "    def build_edge_pack(options_df, greeks_df, gex_by_strike, expected_move_df, spot):\n",
        "        \\\"\\\"\\\"\n",
        "        options_df     : DataFrame base (cadena) — puede o no tener gex_shares/vanna/charm\n",
        "        greeks_df      : 'V9_Greeks_By_Contract' o 'greeks_second_order' si existe\n",
        "        gex_by_strike  : hoja de GEX por strike si existe\n",
        "        expected_move_df: hoja Expected_Move\n",
        "        spot           : precio spot (float)\n",
        "        \\\"\\\"\\\"\n",
        "        if expected_move_df is None or expected_move_df.empty or spot is None:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # 1) EM (front-month) con fallback a straddle\n",
        "        em_front = expected_move_df.sort_values('days_to_expiration').head(1)\n",
        "        em_iv = em_front.get('expected_move_%_iv')\n",
        "        em_str = em_front.get('expected_move_%_straddle')\n",
        "        try:\n",
        "            em_iv = float(em_iv.iloc[0]) if em_iv is not None else np.nan\n",
        "        except Exception:\n",
        "            em_iv = np.nan\n",
        "        try:\n",
        "            em_str = float(em_str.iloc[0]) if em_str is not None else np.nan\n",
        "        except Exception:\n",
        "            em_str = np.nan\n",
        "        em_pct = em_iv if pd.notna(em_iv) else (em_str if pd.notna(em_str) else np.nan)\n",
        "        if not pd.notna(em_pct) or em_pct <= 0:\n",
        "            return pd.DataFrame()  # sin EM no hay FPR ni corredor\n",
        "\n",
        "        em_frac = em_pct / 100.0\n",
        "\n",
        "        # 2) Gamma flip (prefiere gex_by_strike; si no, intenta options_df)\n",
        "        gamma_flip = np.nan\n",
        "        try:\n",
        "            if gex_by_strike is not None and not gex_by_strike.empty and 'gex_shares' in gex_by_strike.columns:\n",
        "                s = gex_by_strike.sort_values('strike_price')\n",
        "                cum = s['gex_shares'].cumsum()\n",
        "                flips = s.loc[(cum * cum.shift(fill_value=0)) < 0, 'strike_price']\n",
        "                if len(flips) > 0:\n",
        "                    gamma_flip = float(flips.iloc[0])\n",
        "            elif options_df is not None and not options_df.empty and 'gex_shares' in options_df.columns:\n",
        "                s = (options_df.groupby('strike_price')['gex_shares'].sum().sort_index())\n",
        "                cum = s.cumsum()\n",
        "                flips = s.index[(cum * cum.shift(fill_value=0)) < 0]\n",
        "                if len(flips) > 0:\n",
        "                    gamma_flip = float(flips.min())\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # 3) FPR (Flip-Proximity Ratio)\n",
        "        if pd.notna(gamma_flip):\n",
        "            try:\n",
        "                fpr = abs(float(spot) - gamma_flip) / (float(spot) * em_frac)\n",
        "            except Exception:\n",
        "                fpr = np.nan\n",
        "        else:\n",
        "            fpr = np.nan\n",
        "\n",
        "        # 4) Vanna/Charm en corredor ±EM% (front-month)\n",
        "        low = float(spot) * (1 - em_frac)\n",
        "        high = float(spot) * (1 + em_frac)\n",
        "\n",
        "        # Fuente para vanna/charm: prioriza greeks_df; si no, options_df *_advanced\n",
        "        vanna_col, charm_col, oi_col = None, None, None\n",
        "        src = None\n",
        "        if greeks_df is not None and not greeks_df.empty:\n",
        "            for c in ['vanna', 'vanna_calc', 'vanna_advanced']:\n",
        "                if c in greeks_df.columns:\n",
        "                    vanna_col = c\n",
        "                    break\n",
        "            for c in ['charm', 'charm_calc', 'charm_advanced']:\n",
        "                if c in greeks_df.columns:\n",
        "                    charm_col = c\n",
        "                    break\n",
        "            for c in ['open_interest', 'oi']:\n",
        "                if c in greeks_df.columns:\n",
        "                    oi_col = c\n",
        "                    break\n",
        "            src = greeks_df\n",
        "        if src is None or vanna_col is None or charm_col is None:\n",
        "            if options_df is not None and not options_df.empty:\n",
        "                for c in ['vanna', 'vanna_calc', 'vanna_advanced']:\n",
        "                    if c in options_df.columns:\n",
        "                        vanna_col = c\n",
        "                        break\n",
        "                for c in ['charm', 'charm_calc', 'charm_advanced']:\n",
        "                    if c in options_df.columns:\n",
        "                        charm_col = c\n",
        "                        break\n",
        "                for c in ['open_interest', 'oi']:\n",
        "                    if c in options_df.columns:\n",
        "                        oi_col = c\n",
        "                        break\n",
        "                src = options_df if (vanna_col and charm_col) else None\n",
        "\n",
        "        vannaX = np.nan\n",
        "        charmX = np.nan\n",
        "        if src is not None and vanna_col and charm_col:\n",
        "            tmp = src.copy()\n",
        "            strike_name = 'strike_price' if 'strike_price' in tmp.columns else ('strike' if 'strike' in tmp.columns else None)\n",
        "            if strike_name:\n",
        "                tmp[strike_name] = pd.to_numeric(tmp[strike_name], errors='coerce')\n",
        "                tmp = tmp[(tmp[strike_name] >= low) & (tmp[strike_name] <= high)]\n",
        "                oi_series = pd.to_numeric(tmp.get(oi_col, 0), errors='coerce').fillna(0.0)\n",
        "                vannaX = (pd.to_numeric(tmp[vanna_col], errors='coerce') * float(spot) * 100.0 * oi_series).sum(skipna=True)\n",
        "                charmX = (pd.to_numeric(tmp[charm_col], errors='coerce') * float(spot) * 100.0 * oi_series).sum(skipna=True)\n",
        "\n",
        "        out = {\n",
        "            'EM_%': em_pct,\n",
        "            'Gamma_Flip': gamma_flip,\n",
        "            'FPR': fpr,\n",
        "            'VannaCorridorX': vannaX,\n",
        "            'CharmCorridorX': charmX\n",
        "        }\n",
        "        return pd.DataFrame([out])\n",
        "\n",
        "    config = {\n",
        "        \"start_date\": (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        \"end_date\": datetime.now().strftime('%Y-%m-%d')\n",
        "    }\n",
        "\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR)\n",
        "    persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    raw_base_dir = persistent_dir / 'raw'\n",
        "    raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Lit exchanges\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"\\n{'='*80}\")\n",
        "        logger.info(f\"🚀 HYPERION V9 - Processing {ticker}\")\n",
        "        logger.info(f\"{'='*80}\\n\")\n",
        "\n",
        "        raw_dir = raw_base_dir / ticker\n",
        "        raw_dir.mkdir(exist_ok=True)\n",
        "        rd = raw_dir  # alias a la ruta (mkdir no devuelve Path)\n",
        "\n",
        "        run_ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_excel_path = (\n",
        "            persistent_dir / f\"{ticker}_hyperion_v9_report_{run_ts}.xlsx\"\n",
        "        )\n",
        "\n",
        "        # 1) Extract base data\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, rd)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) CIK-based data\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty:\n",
        "            profile = data_sources['1_Profile'].iloc[0]\n",
        "            if profile.get('cik'):\n",
        "                cik = profile.get('cik')\n",
        "                cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "                cik_params = {'cik': cik, 'limit': 100}\n",
        "                df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "                if not df_cik.empty:\n",
        "                    data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "\n",
        "        # 3) Earnings transcripts\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 4) Options chain - Refactor to use polygon_client\n",
        "        logger.info(\"📋 Fetching options chain using Polygon RESTClient...\")\n",
        "        options_snapshot_response = polygon_client.vx.options.get_options_snapshot(ticker)\n",
        "        df_snapshot_raw = pd.DataFrame(options_snapshot_response.results)\n",
        "        df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "            logger.success(f\"✅ Options chain: {len(df_options_chain)} contracts\")\n",
        "\n",
        "        # Helpers (ámbito local) para reconstrucción de IV si los greeks vienen vacíos\n",
        "        def bs_price(S, K, r, q, sigma, T, cp):  # cp = +1 call, -1 put\n",
        "            from math import log, sqrt, exp\n",
        "            if sigma <= 0 or T <= 0:\n",
        "                return np.nan\n",
        "            d1 = (log(S / K) + (r - q + 0.5 * sigma * sigma) * T) / (sigma * sqrt(T))\n",
        "            d2 = d1 - sigma * sqrt(T)\n",
        "            if cp > 0:\n",
        "                return S * exp(-q * T) * norm.cdf(d1) - K * exp(-r * T) * norm.cdf(d2)\n",
        "            else:\n",
        "                return K * exp(-r * T) * norm.cdf(-d2) - S * exp(-r * T) * norm.cdf(-d1)\n",
        "\n",
        "        def iv_from_mid(S, K, r, q, T, mid, cp, lo=1e-4, hi=5.0):\n",
        "            f = lambda s: bs_price(S, K, r, q, s, T, cp) - mid\n",
        "            try:\n",
        "                return float(brentq(f, lo, hi, maxiter=100))\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        # 5) Flow analysis\n",
        "        df_flow_analysis = pd.DataFrame()\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(\n",
        "                df_options_chain['volume'], errors='coerce'\n",
        "            ).fillna(0)\n",
        "\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (\n",
        "                df_options_chain\n",
        "                .groupby('expiration_date')\n",
        "                .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "\n",
        "            if len(top_contracts) > 50:\n",
        "                top_contracts = top_contracts.nlargest(50, 'volume')\n",
        "\n",
        "            logger.info(f\"🔄 Analyzing flow for {len(top_contracts)} contracts...\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                logger.success(f\"✅ Flow analysis: {len(df_flow_analysis)} contracts\")\n",
        "\n",
        "        # Fallback: si no hubo trades/flow, construye proxy desde snapshot\n",
        "        if df_flow_analysis.empty and not df_options_chain.empty:\n",
        "            _df = df_options_chain.copy()\n",
        "\n",
        "            # Normaliza numéricos\n",
        "            _df['volume'] = pd.to_numeric(_df.get('volume', 0), errors='coerce').fillna(0)\n",
        "            _df['open_interest'] = pd.to_numeric(_df.get('open_interest', 0), errors='coerce').fillna(0)\n",
        "\n",
        "            # Contract symbol desde snapshot\n",
        "            _df['contract'] = _df.get('options_ticker', _df.get('symbol'))\n",
        "\n",
        "            # Normaliza expiration\n",
        "            _df['expiration_norm'] = (\n",
        "                _df['expiration'] if 'expiration' in _df.columns else _df.get('expiration_date')\n",
        "            )\n",
        "\n",
        "            # Selección mínima para análisis de flujo\n",
        "            df_flow_analysis = _df[\n",
        "                ['contract', 'contract_type', 'strike_price', 'expiration_norm', 'volume', 'open_interest']\n",
        "            ].rename(columns={'expiration_norm': 'expiration'})\n",
        "\n",
        "            # Proxy simple: ratio volumen/OI\n",
        "            df_flow_analysis['vol_oi_ratio'] = df_flow_analysis.apply(\n",
        "                lambda x: x['volume'] / x['open_interest'] if x['open_interest'] > 0 else 0, axis=1\n",
        "            )\n",
        "\n",
        "            data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "            logger.warning(\"⚠️ Sin trades para flow; usando snapshot fallback (vol/OI proxy).\")\n",
        "\n",
        "        # Get spot price\n",
        "        spot_price = 0\n",
        "        if '2_Quote' in data_sources and not data_sources['2_Quote'].empty:\n",
        "            spot_price = float(data_sources['2_Quote'].iloc[0].get('price', 0))\n",
        "            logger.info(f\"💰 Spot price: ${spot_price:.2f}\")\n",
        "\n",
        "        # Calculate basic metrics\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, 10)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, 20)\n",
        "\n",
        "        # Advanced options metrics\n",
        "        if not df_options_chain.empty and spot_price:\n",
        "            adv_metrics = calculate_advanced_options_metrics(df_options_chain, spot_price)\n",
        "            if adv_metrics:\n",
        "                data_sources['Options_Metrics_Advanced'] = pd.DataFrame([adv_metrics])\n",
        "\n",
        "        # ====================================================================\n",
        "        # ✅ V3 ENHANCED INTEGRATION (reubicado después de flow/spot)\n",
        "        #    *No* se escribe Excel aquí para evitar doble escritura.\n",
        "        # ====================================================================\n",
        "        enhanced_sheets_v3 = {}\n",
        "        try:\n",
        "            if not df_options_chain.empty:\n",
        "                enhanced_sheets_v3 = generate_enhanced_sheets(\n",
        "                    df_options_chain=df_options_chain,\n",
        "                    df_flow_analysis=df_flow_analysis if not df_flow_analysis.empty else pd.DataFrame(),\n",
        "                    df_earnings_hist=data_sources.get('12_Earnings_Cal', pd.DataFrame()),\n",
        "                    spot_price=spot_price,\n",
        "                    ticker=ticker,\n",
        "                    status_logger=status_logger\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ V3 Enhanced block failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # ====================================================================\n",
        "        # 🚀 HYPERION V9 ENHANCED FEATURES\n",
        "        # ====================================================================\n",
        "\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"🚀 HYPERION V9 - ENHANCED ANALYSIS\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"\")\n",
        "\n",
        "        enhanced_sheets_v9 = {}\n",
        "\n",
        "        try:\n",
        "            # Get earnings history\n",
        "            df_earnings_hist = data_sources.get('12_Earnings_Cal', pd.DataFrame())\n",
        "\n",
        "            # Call V9 integration function\n",
        "            enhanced_sheets_v9 = integrate_v9_enhancements(\n",
        "                ticker=ticker,\n",
        "                spot_price=spot_price,\n",
        "                df_options_chain=df_options_chain,\n",
        "                df_flow_analysis=df_flow_analysis,\n",
        "                df_earnings_hist=df_earnings_hist,\n",
        "                data_sources=data_sources,\n",
        "                persistent_dir=persistent_dir\n",
        "            )\n",
        "\n",
        "            logger.success(\n",
        "                f\"✅ V9 Enhanced Analysis Complete: {len(enhanced_sheets_v9)} sheets generated\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ V9 Enhanced Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # ====================================================================\n",
        "        # 📊 GENERATING EXCEL REPORT\n",
        "        # ====================================================================\n",
        "\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"📊 GENERATING EXCEL REPORT\")\n",
        "        logger.info(\"=\" * 80)\n",
        "\n",
        "        all_sheets = {}\n",
        "\n",
        "        # Add base data sheets\n",
        "        for name, df in data_sources.items():\n",
        "            if not df.empty and name not in ['Options_Chain']:\n",
        "                sheet_name = name.replace('_', ' ')[:31]\n",
        "                all_sheets[sheet_name] = df\n",
        "\n",
        "        # Add V3 + V9 enhanced sheets\n",
        "        all_sheets.update(enhanced_sheets_v3)\n",
        "        all_sheets.update(enhanced_sheets_v9)\n",
        "\n",
        "        # Create dashboard\n",
        "        dashboard_data = {\n",
        "            'Ticker': ticker,\n",
        "            'Spot_Price': spot_price,\n",
        "            'Report_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Total_Sheets': len(all_sheets),\n",
        "            'V9_Enhanced_Sheets': len(enhanced_sheets_v9),\n",
        "            'Options_Contracts_Analyzed': len(df_options_chain) if not df_options_chain.empty else 0,\n",
        "        }\n",
        "\n",
        "        # Add V9 summary metrics to dashboard\n",
        "        if 'Scorecard' in enhanced_sheets_v9:\n",
        "            scorecard = enhanced_sheets_v9['Scorecard']\n",
        "            if not scorecard.empty:\n",
        "                dashboard_data['V9_Score'] = scorecard.iloc[0]['total_score']\n",
        "                dashboard_data['V9_Recommendation'] = scorecard.iloc[0]['recommendation']\n",
        "                dashboard_data['V9_Confidence'] = scorecard.iloc[0]['confidence']\n",
        "\n",
        "        if 'GEX_Summary' in enhanced_sheets_v9:\n",
        "            gex_summary = enhanced_sheets_v9['GEX_Summary']\n",
        "            if not gex_summary.empty:\n",
        "                dashboard_data['GEX_Total_Shares'] = gex_summary.iloc[0]['total_gex_shares']\n",
        "                dashboard_data['GEX_Positioning'] = gex_summary.iloc[0]['positioning']\n",
        "\n",
        "        if 'Expected_Move' in enhanced_sheets_v9:\n",
        "            exp_move = enhanced_sheets_v9['Expected_Move']\n",
        "            if not exp_move.empty and len(exp_move) > 0:\n",
        "                front_move = exp_move.sort_values('days_to_expiration').iloc[0]\n",
        "                em_iv = front_move.get('expected_move_%_iv')\n",
        "                em_str = front_move.get('expected_move_%_straddle')\n",
        "                if em_iv is not None and pd.notna(em_iv):\n",
        "                    dashboard_data['Expected_Move_%'] = float(em_iv)\n",
        "                elif em_str is not None and pd.notna(em_str):\n",
        "                    dashboard_data['Expected_Move_%'] = float(em_str)\n",
        "\n",
        "        # === V9 EDGE PACK (FPR + VCP) ===\n",
        "        edge_df = build_edge_pack(\n",
        "            options_df=df_options_chain,\n",
        "            greeks_df=enhanced_sheets_v9.get(\n",
        "                'V9_Greeks_By_Contract',\n",
        "                enhanced_sheets_v9.get('greeks_second_order', pd.DataFrame())\n",
        "            ),\n",
        "            gex_by_strike=enhanced_sheets_v9.get('GEX_By_Strike', pd.DataFrame()),\n",
        "            expected_move_df=enhanced_sheets_v9.get('Expected_Move', pd.DataFrame()),\n",
        "            spot=spot_price\n",
        "        )\n",
        "        if edge_df is not None and not edge_df.empty:\n",
        "            enhanced_sheets_v9['V9_Edge_Pack'] = edge_df\n",
        "            all_sheets['V9_Edge_Pack'] = edge_df\n",
        "\n",
        "        # Dashboard sheet\n",
        "        df_dashboard = pd.DataFrame([dashboard_data])\n",
        "        all_sheets = {'Dashboard': df_dashboard, **all_sheets}\n",
        "\n",
        "        # Write to Excel (una sola vez)\n",
        "        try:\n",
        "            with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
        "                for sheet_name, df in all_sheets.items():\n",
        "                    if not df.empty:\n",
        "                        safe_sheet_name = str(sheet_name)[:31]\n",
        "                        df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "            logger.success(f\"✅ Excel report saved: {output_excel_path}\")\n",
        "            logger.success(f\"   Total sheets: {len(all_sheets)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Excel export error: {e}\")\n",
        "\n",
        "        # ====================================================================\n",
        "        # 📈 SUMMARY\n",
        "        # ====================================================================\n",
        "\n",
        "        elapsed = time.time() - t_start\n",
        "        logger.info(\"\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"📈 HYPERION V9 - ANALYSIS COMPLETE\")\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(f\"   Ticker: {ticker}\")\n",
        "        logger.info(f\"   Time Elapsed: {elapsed:.1f}s\")\n",
        "        logger.info(f\"   Report: {output_excel_path.name}\")\n",
        "        logger.info(f\"   Total Sheets: {len(all_sheets)}\")\n",
        "        logger.info(f\"   V9 Enhanced Sheets: {len(enhanced_sheets_v9)}\")\n",
        "\n",
        "        if 'Scorecard' in enhanced_sheets_v9:\n",
        "            scorecard = enhanced_sheets_v9['Scorecard']\n",
        "            if not scorecard.empty:\n",
        "                score = scorecard.iloc[0]['total_score']\n",
        "                rec = scorecard.iloc[0]['recommendation']\n",
        "                logger.success(f\"   V9 Score: {score:.1f}/100\")\n",
        "                logger.success(f\"   Recommendation: {rec}\")\n",
        "\n",
        "        logger.info(\"=\" * 80)\n",
        "        logger.info(\"\")\n",
        "\n",
        "logger.info(\"✅ Hyperion V9 pipeline ready\")\n",
        "\"\"\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e4e776a"
      },
      "source": [
        "## Add Polygon RESTClient Imports\n",
        "\n",
        "### Subtask:\n",
        "Modify the first code cell to import `RESTClient` from `polygon` and initialize `polygon_client`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "320461b5"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying an existing cell to import `RESTClient` from `polygon` and initialize it. I will provide the full updated code for cell `JRvsEFHImK7Y`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "b26a9c00",
        "executionInfo": {
          "status": "error",
          "timestamp": 1763872294555,
          "user_tz": -660,
          "elapsed": 197,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "d5c3f1ca-b179-4baa-e36b-fa4eb37cb6f8"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "    # Si Polygon no devuelve trades para este contrato en la ventana consultada,\n",
        "    # no añadimos una fila falsa llena de ceros.\n",
        "    contracts_no_quotes += 1\n",
        "    continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            flow_results.append({\n",
        "                'contract': options_ticker,\n",
        "                'type': contract.get('contract_type'),\n",
        "                'strike': contract.get('strike_price'),\n",
        "                'expiration': contract.get('expiration_date'),\n",
        "                'imbalance': np.nan, 'buy_volume': 0, 'sell_volume': 0,\n",
        "                'total_volume_trades': 0,\n",
        "                'Volumen_Lit_%': 0.0, 'Volumen_Off_%': 0.0, 'Volumen_Unknown_%': 0.0,\n",
        "                'quotes_present': False, 'session_source': session_source\n",
        "            })\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'if' statement on line 292 (ipython-input-476942649.py, line 295)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-476942649.py\"\u001b[0;36m, line \u001b[0;32m295\u001b[0m\n\u001b[0;31m    contracts_no_quotes += 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3384347"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to an IndentationError in the `analyze_contracts_flow` function, where a misplaced `if df_trades.empty:` block was incorrectly indented. I will remove this syntactically incorrect and logically misplaced block from the function. Additionally, I will modify the first code cell to import `RESTClient` from `polygon` and initialize `polygon_client` as requested in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "409ad628",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763872614641,
          "user_tz": -660,
          "elapsed": 156092,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "aa35e0f9-0c3c-4e80-837a-03c2552ca151"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 04:34:52\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:34:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 04:34:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 04:34:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:35:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:35:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 04:35:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3186988684.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 04:36:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:14\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:17\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:19\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:21\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:24\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:26\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:27\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:27\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:35\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:36\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:36:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:49\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:36:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 116.72 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6df982c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError caused by misplaced lines in the `analyze_contracts_flow` function. I will remove these incorrectly placed lines to resolve the indentation issue and ensure the code is syntactically correct. Additionally, I will incorporate the `RESTClient` import and `polygon_client` initialization as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817d8035",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763872888797,
          "user_tz": -660,
          "elapsed": 30475,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "c25017a8-a509-49e5-df5d-de30a65af609"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 04:39:38\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:39:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:39:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 04:39:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3186988684.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 04:40:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:52\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:53\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:56\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:58\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:40:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:40:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:00\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:09\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:10\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:13\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:15\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:16\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:17\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:18\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:19\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:41:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:23\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:23\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:41:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 105.25 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea8512c1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError in the `analyze_contracts_flow` function within cell `JRvsEFHImK7Y`. I will remove the incorrectly indented and misplaced lines from this function to resolve the syntax error. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dea8ce7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763873109061,
          "user_tz": -660,
          "elapsed": 107629,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "2c2fff8f-1355-408b-9684-a1b4cb3a7bf7"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 04:43:19\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:26\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:43:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 04:43:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3186988684.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 04:44:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:29\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:32\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:33\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:36\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:49\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:51\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:53\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:55\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:56\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:57\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:58\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:59\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:45:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:45:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:45:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:45:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:04\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:45:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 104.52 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e116013c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError caused by misplaced lines in the `analyze_contracts_flow` function. I will remove these incorrectly placed lines to resolve the indentation issue and ensure the code is syntactically correct. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42aa3da6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763873371833,
          "user_tz": -660,
          "elapsed": 109394,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "510bf460-4adb-4042-dcd6-afaf2ba0b55a"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%\"] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 04:47:40\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:47:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:47:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 04:47:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3712570164.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 04:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:51\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:54\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:56\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:58\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:48:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:48:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:01\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:03\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:12\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:13\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:16\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:17\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:19\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:20\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:21\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:22\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:49:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:26\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:26\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:49:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 106.21 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a6974d6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError in the `analyze_contracts_flow` function within cell `JRvsEFHImK7Y`. I will remove the incorrectly placed lines to resolve the syntax error. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78427e6f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763873636553,
          "user_tz": -660,
          "elapsed": 111130,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "01b0494f-d1e3-4420-88e9-61626f06540e"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%\"] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 04:52:06\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:52:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:13\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:52:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 04:52:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3712570164.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 04:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:16\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:19\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:21\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:23\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:26\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:29\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3712570164.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:53:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:51\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:51\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:53:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 105.31 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e87acd99"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError in the `analyze_contracts_flow` function within cell `JRvsEFHImK7Y` caused by incorrectly placed lines. I will remove these misplaced lines to resolve the syntax error. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cf11644",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763873935136,
          "user_tz": -660,
          "elapsed": 107717,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "d10de5c0-286f-4eab-ff4d-5f124312ea3d"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 04:57:05\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:57:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:12\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:57:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 04:57:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3186988684.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 04:58:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:15\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:18\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:20\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:22\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:24\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:26\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:27\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:28\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:35\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 04:58:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:50\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:50\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 04:58:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 104.55 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45f4cfb1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError caused by misplaced lines in the `analyze_contracts_flow` function. I will remove these incorrectly placed lines to resolve the indentation issue and ensure the code is syntactically correct. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae81cb5b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763874203932,
          "user_tz": -660,
          "elapsed": 108463,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "e484dcfc-6780-4683-b7c1-2f20d0e91575"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 05:01:33\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:01:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:01:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 05:01:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3777174840.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 05:02:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:48\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:51\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:53\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:55\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:56\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:57\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:02:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:02:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:04\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:06\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:10\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:11\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:12\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:13\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:14\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:03:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:18\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:18\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:03:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 105.21 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2342a7c2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError in the `analyze_contracts_flow` function. I will remove the incorrectly indented and misplaced lines from this function to resolve the syntax error. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d58dd9ad",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763874476244,
          "user_tz": -660,
          "elapsed": 111764,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "a1b524c2-2619-4435-ea5d-d45aa1279409"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 05:06:02\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:06:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:10\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:06:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 05:06:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-2939494950.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 05:07:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:16\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:19\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:20\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:23\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:25\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:27\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:28\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:29\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:36\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:45\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:46\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-2939494950.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:07:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:51\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:51\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:07:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 108.62 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6383a02c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError in the `analyze_contracts_flow` function within cell `JRvsEFHImK7Y`. I have removed the incorrectly placed lines to resolve the syntax error. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "112d9436",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763874767971,
          "user_tz": -660,
          "elapsed": 108535,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "4b750038-5a32-4824-9a00-b43a269581b1"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 05:10:57\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:10:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 05:10:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 05:10:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:10:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 05:10:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 05:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:05\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:11:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 05:11:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3186988684.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 05:12:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:07\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:10\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:12\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:14\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:17\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:19\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:20\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:20\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:28\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:29\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:32\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:34\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:35\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:36\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3186988684.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:12:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:42\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:12:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 105.41 s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825b2de4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError in the `analyze_contracts_flow` function within cell `JRvsEFHImK7Y`. I have removed the incorrectly placed lines that caused this syntax error. Additionally, I will modify cell `Grzdvp1VGjy8` to utilize `polygon_client.vx.options.get_options_snapshot` for fetching options data, replacing the previous `fetch_paginated_data` call as per the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f9ed76",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763875047372,
          "user_tz": -660,
          "elapsed": 111935,
          "user": {
            "displayName": "Miguel Antonio Guerrero Quijano",
            "userId": "04781437587956717230"
          }
        },
        "outputId": "49bcad63-a4a5-4a36-d935-b1242c08d785"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HYPERION INGEST\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Celda 1: Setup e imports\n",
        "# =========================\n",
        "import sys, os, json, time, subprocess, re\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from lxml import etree\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\", \"-q\"])\n",
        "    from lxml import etree\n",
        "\n",
        "try:\n",
        "    import pytz\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytz\", \"-q\"])\n",
        "    import pytz\n",
        "\n",
        "try:\n",
        "    from polygon import RESTClient\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polygon-api-client\", \"-q\"])\n",
        "    from polygon import RESTClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# =========================\n",
        "# Constantes\n",
        "# =========================\n",
        "TICKERS_A_PROCESAR = ['CVS']  # <-- cambia aquí tu ticker\n",
        "HISTORICAL_DAYS = 5 * 365\n",
        "ROLLING_WINDOW_SHORT = 10\n",
        "ROLLING_WINDOW_LONG = 20\n",
        "TOP_N_CONTRACTS_OFI = 50\n",
        "RUN_INTRADAY_TEST = False  # Fuerza el bucle intradía para pruebas\n",
        "\n",
        "# Directorio persistente (Drive si está disponible)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    PERSISTENT_DIR = '/content/drive/MyDrive/hyperion_data'\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: No se pudo montar Google Drive. El historial no será persistente. Causa: {e}\")\n",
        "    PERSISTENT_DIR = 'hyperion_dossiers'\n",
        "\n",
        "# Logger\n",
        "try:\n",
        "    from loguru import logger\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"loguru\", \"-q\"])\n",
        "    from loguru import logger\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\",\n",
        "           format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:<8}</level> | <level>{message}</level>\",\n",
        "           colorize=True)\n",
        "\n",
        "# API keys\n",
        "try:\n",
        "    FMP_KEY = userdata.get(\"FMP_API_KEY\")\n",
        "    POLY_KEY = userdata.get(\"POLYGON_API_KEY\")\n",
        "    if not FMP_KEY or not POLY_KEY:\n",
        "        raise ValueError(\"Claves FMP_API_KEY o POLYGON_API_KEY no encontradas en Colab Secrets.\")\n",
        "    polygon_client = RESTClient(api_key=POLY_KEY)\n",
        "    logger.success(\"API keys cargadas correctamente.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar API keys: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =========================\n",
        "# Celda 2: Capa de API\n",
        "# =========================\n",
        "def _create_api_tasks(ticker, start_date_str, end_date_str):\n",
        "    from_date_1y = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "    return {\n",
        "        '1_Profile': (f'https://financialmodelingprep.com/api/v3/profile/{ticker}', {}),\n",
        "        '2_Quote': (f'https://financialmodelingprep.com/api/v3/quote/{ticker}', {}),\n",
        "        '3_Daily_Bars_5Y': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date_str}/{end_date_str}',\n",
        "                            {'adjusted': 'true', 'sort': 'asc', 'limit': 50000}),\n",
        "        '4_Previous_Close': (f'https://api.polygon.io/v2/aggs/ticker/{ticker}/prev', {'adjusted': 'true'}),\n",
        "        '5_Income_Stmt_Annual': (f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '6_Balance_Sheet_Annual': (f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '7_Cash_Flow_Annual': (f'https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}', {'period': 'annual', 'limit': 10}),\n",
        "        '8_Key_Metrics_TTM': (f'https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}', {'limit': 40}),\n",
        "        '9_Financial_Growth': (f'https://financialmodelingprep.com/api/v3/financial-growth/{ticker}', {'period': 'annual', 'limit': 20}),\n",
        "        '10_Dividends_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_dividend/{ticker}', {}),\n",
        "        '11_Splits_Hist': (f'https://financialmodelingprep.com/api/v3/historical-price-full/stock_split/{ticker}', {}),\n",
        "        '12_Earnings_Cal': (f'https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}', {'limit': 12}),\n",
        "        '13_Institutional_Holders': (f'https://financialmodelingprep.com/api/v3/institutional-holder/{ticker}', {'limit': 100}),\n",
        "        '14_Institutional_List': ('https://financialmodelingprep.com/api/v4/institutional-ownership/list', {}),\n",
        "        '15_Senate_Disclosure': (f'https://financialmodelingprep.com/api/v4/senate-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '15b_Senate_Trading': (f'https://financialmodelingprep.com/api/v4/senate-trading', {'symbol': ticker, 'limit': 100}),\n",
        "        '16_House_Disclosure': (f'https://financialmodelingprep.com/api/v4/house-disclosure', {'symbol': ticker, 'limit': 100}),\n",
        "        '17_Analyst_Est': (f'https://financialmodelingprep.com/api/v3/analyst-estimates/{ticker}', {'period': 'annual', 'limit': 30}),\n",
        "        '18_Up_Down': (f'https://financialmodelingprep.com/api/v4/upgrades-downgrades', {'symbol': ticker}),\n",
        "        '19_Price_Target': (f'https://financialmodelingprep.com/api/v4/price-target', {'symbol': ticker}),\n",
        "        '19b_Price_Target_Consensus': (f'https://financialmodelingprep.com/api/v4/price-target-consensus', {'symbol': ticker}),\n",
        "        '21_ESG_Data': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data', {'symbol': ticker}),\n",
        "        '21b_ESG_Ratings': (f'https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data-ratings', {'symbol': ticker}),\n",
        "        '22_COT_Report': ('https://financialmodelingprep.com/stable/commitment-of-traders-report', {}),\n",
        "        '23_Peers': ('https://financialmodelingprep.com/api/v4/stock_peers', {'symbol': ticker}),\n",
        "        '24_Short_Interest': (f'https://api.polygon.io/stocks/v1/short-interest', {'ticker': ticker, 'limit': 100}),\n",
        "        'SPY_Daily_1Y': (f'https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/{from_date_1y}/{end_date_str}',\n",
        "                         {'adjusted': 'true', 'sort': 'desc', 'limit': 300}),\n",
        "        '50_News_Stock': (f'https://financialmodelingprep.com/api/v3/stock_news', {'tickers': ticker, 'limit': 100}),\n",
        "        '51_News_General': ('https://financialmodelingprep.com/api/v4/general_news', {'page': 0, 'limit': 50}),\n",
        "        '52_News_PR': (f'https://financialmodelingprep.com/api/v3/press-releases/{ticker}', {'limit': 100}),\n",
        "        '40_SEC_Search_8K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '8-K', 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41_SEC_Search_10K': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-K', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '41b_SEC_Search_10Q': ('https://financialmodelingprep.com/stable/sec-filings-search/form-type', {'formType': '10-Q', 'limit': 10, 'from': from_date_1y, 'to': end_date_str}),\n",
        "        '42_SEC_Search_By_Symbol': ('https://financialmodelingprep.com/stable/sec-filings-search/symbol', {'symbol': ticker, 'limit': 100, 'from': from_date_1y, 'to': end_date_str}),\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def fetch_api_data(url, params=None):\n",
        "    try:\n",
        "        headers = {}\n",
        "        query_params = params.copy() if params else {}\n",
        "        if \"polygon.io\" in url:\n",
        "            headers['Authorization'] = f'Bearer {POLY_KEY}'\n",
        "        elif \"financialmodelingprep\" in url:\n",
        "            query_params[\"apikey\"] = FMP_KEY\n",
        "        r = requests.get(url, params=query_params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict):\n",
        "            for key in (\"results\", \"historical\"):\n",
        "                if key in data and isinstance(data[key], list):\n",
        "                    return data[key]\n",
        "            return [data]\n",
        "        return data if isinstance(data, list) else []\n",
        "    except requests.HTTPError as e:\n",
        "        logger.error(f\"HTTPError {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error genérico {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_paginated_data(url, raw_dir, ticker, name_for_raw_file):\n",
        "    results = []\n",
        "    next_url = url\n",
        "    headers = {'Authorization': f'Bearer {POLY_KEY}'}\n",
        "    while next_url:\n",
        "        try:\n",
        "            if 'apiKey' not in next_url and '?' in next_url:\n",
        "                next_url_with_key = f\"{next_url}&apiKey={POLY_KEY}\"\n",
        "            elif 'apiKey' not in next_url:\n",
        "                next_url_with_key = f\"{next_url}?apiKey={POLY_KEY}\"\n",
        "            else:\n",
        "                next_url_with_key = next_url\n",
        "            response = requests.get(next_url_with_key, headers=headers, timeout=60).json()\n",
        "            reslist = response.get(\"results\", [])\n",
        "            if isinstance(reslist, list):\n",
        "                results.extend(reslist)\n",
        "            next_url = response.get(\"next_url\")\n",
        "            if next_url:\n",
        "                time.sleep(0.2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante paginación para {url}: {e}\")\n",
        "            break\n",
        "    save_raw_json(name_for_raw_file, ticker, results, raw_dir)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def fetch_data_block(name, url, params, ticker, raw_dir):\n",
        "    logger.info(f\"Extrayendo: {name}\")\n",
        "    data = fetch_api_data(url, params)\n",
        "    if not data:\n",
        "        logger.warning(f\"No se obtuvieron datos para {name}\")\n",
        "        return pd.DataFrame()\n",
        "    save_raw_json(name, ticker, data, raw_dir)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def fetch_historical_transcripts(ticker, raw_dir):\n",
        "    logger.info(\"Extrayendo transcripciones históricas (últimos 8 trimestres)...\")\n",
        "    all_transcripts = []\n",
        "    now = datetime.now()\n",
        "    current_year = now.year\n",
        "    current_quarter = (now.month - 1) // 3 + 1\n",
        "    for i in range(8):\n",
        "        year = current_year\n",
        "        quarter = current_quarter - i\n",
        "        while quarter <= 0:\n",
        "            quarter += 4\n",
        "            year -= 1\n",
        "        url = f\"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}\"\n",
        "        params = {'year': year, 'quarter': quarter}\n",
        "        data = fetch_api_data(url, params)\n",
        "        if data:\n",
        "            all_transcripts.extend(data)\n",
        "    save_raw_json(\"Earnings_Transcripts_Hist\", ticker, all_transcripts, raw_dir)\n",
        "    return pd.DataFrame(all_transcripts)\n",
        "\n",
        "# =========================\n",
        "# Celda 3: Lógica de negocio\n",
        "# =========================\n",
        "def save_raw_json(name, ticker, data, raw_dir):\n",
        "    if data is not None:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "        (raw_dir / f\"{ticker}_{name}_{ts}.json\").write_text(json.dumps(data, indent=2))\n",
        "\n",
        "# --- Ventana temporal inteligente (RTH hoy o RTH previo) ---\n",
        "def _is_rth(dt_et):\n",
        "    return dt_et.weekday() < 5 and ((dt_et.hour > 9 or (dt_et.hour == 9 and dt_et.minute >= 30)) and dt_et.hour < 16)\n",
        "\n",
        "def _prev_business_day(dt_et):\n",
        "    d = dt_et.date() - timedelta(days=1)\n",
        "    while d.weekday() >= 5:\n",
        "        d -= timedelta(days=1)\n",
        "    return d\n",
        "\n",
        "def get_session_window(now_market_time):\n",
        "    if RUN_INTRADAY_TEST or _is_rth(now_market_time):\n",
        "        session_day = now_market_time.date()\n",
        "        source = \"RTH_Today\" if not RUN_INTRADAY_TEST else \"RTH_Today(TEST)\"\n",
        "    else:\n",
        "        session_day = _prev_business_day(now_market_time)\n",
        "        source = \"RTH_PreviousDay\"\n",
        "    start = datetime.combine(session_day, datetime.min.time()).replace(hour=9, minute=30, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    end   = datetime.combine(session_day, datetime.min.time()).replace(hour=16, minute=0, second=0, microsecond=0, tzinfo=now_market_time.tzinfo)\n",
        "    return int(start.timestamp() * 1e9), int(end.timestamp() * 1e9), source\n",
        "\n",
        "# --- Exchanges: mapeo lit dinámico con fallback ---\n",
        "def fetch_lit_exchanges_polygon():\n",
        "    try:\n",
        "        url = \"https://api.polygon.io/v3/reference/exchanges\"\n",
        "        params = {\"asset_class\": \"options\", \"locale\": \"us\", \"apiKey\": POLY_KEY}\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if isinstance(results, dict):\n",
        "            results = [results]\n",
        "        lit_ids = []\n",
        "        for ex in results:\n",
        "            ex_id = ex.get(\"id\")\n",
        "            if ex_id is not None:\n",
        "                lit_ids.append(int(ex_id))\n",
        "        lit_ids = sorted(list(set(lit_ids)))\n",
        "        if lit_ids:\n",
        "            logger.info(f\"Lit exchanges (Polygon): {lit_ids}\")\n",
        "            return lit_ids\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudo obtener exchanges desde Polygon. Fallback a lista local. Causa: {e}\")\n",
        "    return [1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,21,22]\n",
        "\n",
        "# --- Clasificación Lee-Ready robusta ---\n",
        "def _normalize_quote_cols(df_quotes):\n",
        "    if 'bid_price' not in df_quotes.columns and 'bp' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'bp': 'bid_price'})\n",
        "    if 'ask_price' not in df_quotes.columns and 'ap' in df_quotes.columns:\n",
        "        df_quotes = df_quotes.rename(columns={'ap': 'ask_price'})\n",
        "    return df_quotes\n",
        "\n",
        "def classify_trade_side_lee_ready(df_merged):\n",
        "    if df_merged.empty:\n",
        "        return pd.DataFrame()\n",
        "    if 'bid_price' not in df_merged.columns or 'ask_price' not in df_merged.columns:\n",
        "        return pd.DataFrame()\n",
        "    conditions = [\n",
        "        (df_merged['price'] > df_merged['ask_price']),\n",
        "        (df_merged['price'] < df_merged['bid_price']),\n",
        "        (df_merged['price'] == df_merged['ask_price']),\n",
        "        (df_merged['price'] == df_merged['bid_price'])\n",
        "    ]\n",
        "    choices = ['buy', 'sell', 'buy', 'sell']\n",
        "    df_merged['side'] = np.select(conditions, choices, default=None)\n",
        "    mask_null = df_merged['side'].isnull()\n",
        "    if mask_null.any():\n",
        "        df_merged['prev_price'] = df_merged['price'].shift(1)\n",
        "        tick_sides = np.select(\n",
        "            [(df_merged['price'] > df_merged['prev_price']),\n",
        "             (df_merged['price'] < df_merged['prev_price'])],\n",
        "            ['buy', 'sell'], default=None\n",
        "        )\n",
        "        df_merged.loc[mask_null, 'side'] = tick_sides[mask_null]\n",
        "    return df_merged.dropna(subset=['side'])\n",
        "\n",
        "def calculate_order_flow_imbalance(df_trades_classified):\n",
        "    buy_vol = df_trades_classified.loc[df_trades_classified['side']=='buy','volume'].sum()\n",
        "    sell_vol = df_trades_classified.loc[df_trades_classified['side']=='sell','volume'].sum()\n",
        "    return buy_vol - sell_vol, buy_vol, sell_vol\n",
        "\n",
        "def analyze_contracts_flow(contracts_df, base_ticker, raw_dir, lit_ex_ids):\n",
        "    flow_results = []\n",
        "    contracts_total = len(contracts_df)\n",
        "    contracts_ofi_used = 0\n",
        "    contracts_no_quotes = 0\n",
        "\n",
        "    market_tz = pytz.timezone('America/New_York')\n",
        "    now_market_time = datetime.now(market_tz)\n",
        "    start_ns, end_ns, session_source = get_session_window(now_market_time)\n",
        "\n",
        "    for _, contract in contracts_df.iterrows():\n",
        "        options_ticker = contract.get('options_ticker')\n",
        "        if not options_ticker:\n",
        "            continue\n",
        "\n",
        "        logger.info(f\"Flujo → {options_ticker} ({session_source})\")\n",
        "\n",
        "        # Convertir los nanosegundos de get_session_window a datetime para usar el cliente\n",
        "        start_dt = datetime.fromtimestamp(start_ns / 1e9, tz=market_tz)\n",
        "        end_dt = datetime.fromtimestamp(end_ns / 1e9, tz=market_tz)\n",
        "        df_trades, df_quotes = get_option_trades_and_quotes_client(options_ticker, start_dt, end_dt)\n",
        "\n",
        "\n",
        "        if df_trades.empty:\n",
        "            contracts_no_quotes += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Normalizaciones\n",
        "        df_trades.rename(columns={'size':'volume'}, inplace=True)\n",
        "        df_trades['volume'] = pd.to_numeric(df_trades['volume'], errors='coerce').fillna(0)\n",
        "        df_trades['exchange'] = pd.to_numeric(df_trades.get('exchange'), errors='coerce')\n",
        "        df_trades['sip_timestamp'] = pd.to_numeric(df_trades.get('sip_timestamp'), errors='coerce')\n",
        "        df_trades = df_trades.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        if not df_quotes.empty:\n",
        "            df_quotes = _normalize_quote_cols(df_quotes.copy())\n",
        "            df_quotes['sip_timestamp'] = pd.to_numeric(df_quotes.get('sip_timestamp'), errors='coerce')\n",
        "            df_quotes = df_quotes.dropna(subset=['sip_timestamp']).sort_values('sip_timestamp')\n",
        "\n",
        "        # % Lit / Off / Unknown\n",
        "        total_vol = df_trades['volume'].sum()\n",
        "        lit_vol = df_trades[df_trades['exchange'].isin(lit_ex_ids)]['volume'].sum()\n",
        "        unknown_vol = df_trades[df_trades['exchange'].isna()]['volume'].sum()\n",
        "        off_vol = max(total_vol - lit_vol - unknown_vol, 0)\n",
        "\n",
        "        def pct(x): return round((x/total_vol*100) if total_vol>0 else 0.0, 2)\n",
        "        lit_pct, off_pct, unk_pct = pct(lit_vol), pct(off_vol), pct(unknown_vol)\n",
        "\n",
        "        imbalance, buy_vol, sell_vol = (np.nan, 0, 0)\n",
        "        quotes_present = False\n",
        "        if not df_quotes.empty and ('bid_price' in df_quotes.columns or 'bp' in df_quotes.columns):\n",
        "            quotes_present = True\n",
        "            cols_keep = ['sip_timestamp']\n",
        "            if 'bid_price' in df_quotes.columns: cols_keep.append('bid_price')\n",
        "            if 'ask_price' in df_quotes.columns: cols_keep.append('ask_price')\n",
        "            if 'bid_price' not in cols_keep and 'bp' in df_quotes.columns: cols_keep.append('bp')\n",
        "            if 'ask_price' not in cols_keep and 'ap' in df_quotes.columns: cols_keep.append('ap')\n",
        "            df_q_small = _normalize_quote_cols(df_quotes[cols_keep].copy())\n",
        "            df_merged = pd.merge_asof(\n",
        "                df_trades[['sip_timestamp','price','volume']].sort_values('sip_timestamp'),\n",
        "                df_q_small.sort_values('sip_timestamp'),\n",
        "                on='sip_timestamp', direction='backward'\n",
        "            )\n",
        "            df_classified = classify_trade_side_lee_ready(df_merged)\n",
        "            if not df_classified.empty:\n",
        "                imbalance, buy_vol, sell_vol = calculate_order_flow_imbalance(df_classified)\n",
        "                contracts_ofi_used += 1\n",
        "            else:\n",
        "                contracts_no_quotes += 1\n",
        "                quotes_present = False\n",
        "        else:\n",
        "            contracts_no_quotes += 1\n",
        "\n",
        "        flow_results.append({\n",
        "            'contract': options_ticker,\n",
        "            'type': contract.get('contract_type'),\n",
        "            'strike': contract.get('strike_price'),\n",
        "            'expiration': contract.get('expiration_date'),\n",
        "            'imbalance': imbalance,\n",
        "            'buy_volume': buy_vol,\n",
        "            'sell_volume': sell_vol,\n",
        "            'total_volume_trades': total_vol,\n",
        "            'Volumen_Lit_%': lit_pct,\n",
        "            'Volumen_Off_%': off_pct,\n",
        "            'Volumen_Unknown_%': unk_pct,\n",
        "            'quotes_present': quotes_present,\n",
        "            'session_source': session_source\n",
        "        })\n",
        "\n",
        "    meta = {\n",
        "        \"contracts_total\": contracts_total,\n",
        "        \"contracts_ofi_used\": contracts_ofi_used,\n",
        "        \"contracts_no_quotes\": contracts_no_quotes,\n",
        "        \"session_source\": session_source\n",
        "    }\n",
        "    return pd.DataFrame(flow_results), meta\n",
        "\n",
        "USGAAP_NS = {'us-gaap': 'http://fasb.org/us-gaap/2023'}\n",
        "\n",
        "def _download_lab_xml(index_url):\n",
        "    headers = {'User-Agent': 'FMP-DATA-GATHERER youremail@example.com'}\n",
        "    html = requests.get(index_url, headers=headers, timeout=30).text\n",
        "    m = re.search(r'href=\"([^\"]+_lab\\.xml)\"', html, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    xml_url = m.group(1)\n",
        "    if not xml_url.startswith('http'):\n",
        "        xml_url = \"https://www.sec.gov\" + xml_url\n",
        "    logger.info(f\"Descargando XBRL: {xml_url}\")\n",
        "    r = requests.get(xml_url, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def _extract_rpo_from_lab(xml_bytes):\n",
        "    if not xml_bytes: return 0\n",
        "    tree = etree.parse(BytesIO(xml_bytes))\n",
        "    def _sum_tags(tag_list):\n",
        "        total = 0\n",
        "        for tag in tag_list:\n",
        "            nodes = tree.xpath(f\"//us-gaap:{tag}\", namespaces=USGAAP_NS)\n",
        "            if nodes:\n",
        "                total += sum(float(n.text) for n in nodes if n.text is not None)\n",
        "        return total\n",
        "    rpo_total = _sum_tags(['RemainingPerformanceObligation'])\n",
        "    if rpo_total > 0: return rpo_total\n",
        "    rpo_current = _sum_tags(['ContractWithCustomerLiabilityCurrent','RevenueRemainingPerformanceObligationCurrent'])\n",
        "    rpo_noncurrent = _sum_tags(['ContractWithCustomerLiabilityNoncurrent','RevenueRemainingPerformanceObligationNoncurrent'])\n",
        "    return rpo_current + rpo_noncurrent\n",
        "\n",
        "def extract_rpo_from_filings(sec_filings_df):\n",
        "    if sec_filings_df.empty: return np.nan\n",
        "    for _, row in sec_filings_df.sort_values('acceptedDate', ascending=False).iterrows():\n",
        "        xbrl_link = row.get('linkToXbrl') or row.get('linkXbrl')\n",
        "        if not xbrl_link: continue\n",
        "        try:\n",
        "            lab = _download_lab_xml(xbrl_link)\n",
        "            if lab:\n",
        "                val = _extract_rpo_from_lab(lab)\n",
        "                if val > 0:\n",
        "                    logger.success(f\"RPO extraído de {row.get('type','N/A')} ({row.get('acceptedDate','N/A')}): ${val:,.0f}\")\n",
        "                    return val\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Fallo XBRL {xbrl_link}: {e}\")\n",
        "    logger.warning(\"No se pudo extraer RPO.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Métricas y helpers ---\n",
        "def calculate_realized_volatility(price_series, window):\n",
        "    log_returns = np.log(price_series / price_series.shift(1))\n",
        "    return log_returns.rolling(window).std().iloc[-1] * np.sqrt(252)\n",
        "\n",
        "def calculate_iv_rank_crosssection(iv_series):\n",
        "    if iv_series.empty or iv_series.isna().all(): return np.nan\n",
        "    iv_series = pd.to_numeric(iv_series, errors='coerce').dropna()\n",
        "    if iv_series.empty: return np.nan\n",
        "    current_iv = iv_series.iloc[-1]\n",
        "    min_iv, max_iv = iv_series.min(), iv_series.max()\n",
        "    denom = max_iv - min_iv\n",
        "    if denom < 1e-9: return np.nan\n",
        "    return 100 * (current_iv - min_iv) / denom\n",
        "\n",
        "def calculate_gini_index(series):\n",
        "    if series.empty or series.sum()==0: return 0\n",
        "    s = series.sort_values().cumsum()\n",
        "    n = len(series)\n",
        "    return round((2*s.sum()/(n*series.sum()) - (n+1)/n), 4)\n",
        "\n",
        "def _select_front_month(df_options):\n",
        "    if df_options.empty: return df_options, None\n",
        "    d = df_options.copy()\n",
        "    d['expiration_date_dt'] = pd.to_datetime(d['expiration_date'], errors='coerce')\n",
        "    d = d.dropna(subset=['expiration_date_dt'])\n",
        "    d['open_interest'] = pd.to_numeric(d['open_interest'], errors='coerce').fillna(0)\n",
        "    today = datetime.now().date()\n",
        "    future = d[d['expiration_date_dt'].dt.date >= today]\n",
        "    if future.empty:\n",
        "        fm = d\n",
        "    else:\n",
        "        grouped = future.groupby('expiration_date_dt')['open_interest'].sum().sort_index()\n",
        "        if grouped.empty:\n",
        "            fm = future\n",
        "        else:\n",
        "            idx = grouped[grouped>0].index.min() if (grouped>0).any() else grouped.index.min()\n",
        "            fm = future[future['expiration_date_dt']==idx]\n",
        "    expiry_used = None if fm.empty else str(fm['expiration_date'].iloc[0])\n",
        "    return fm, expiry_used\n",
        "\n",
        "def calculate_advanced_options_metrics(df_options, spot_price):\n",
        "    if df_options.empty or not spot_price:\n",
        "        return {}\n",
        "    d = df_options.copy()\n",
        "    for col in ['gamma','open_interest','volume','close','vega','theta','iv','delta','strike_price']:\n",
        "        d[col] = pd.to_numeric(d[col], errors='coerce')\n",
        "    d.dropna(subset=['strike_price','delta','iv'], inplace=True)\n",
        "    d['iv'] = d['iv'].apply(lambda x: x/100.0 if x is not None and x >= 5 else x)\n",
        "\n",
        "    d_fm, expiry_used = _select_front_month(d)\n",
        "    if d_fm.empty:\n",
        "        d_fm = d\n",
        "        expiry_used = \"N/A\"\n",
        "\n",
        "    puts = d_fm[d_fm['contract_type'].isin(['put','P'])]\n",
        "    calls = d_fm[d_fm['contract_type'].isin(['call','C'])]\n",
        "\n",
        "    total_put_vol = puts['volume'].sum()\n",
        "    total_call_vol = calls['volume'].sum()\n",
        "    put_call_ratio_vol = (total_put_vol / total_call_vol) if total_call_vol > 0 else np.inf\n",
        "\n",
        "    puts_premium = (puts['volume'] * puts['close']).sum()\n",
        "    calls_premium = (calls['volume'] * calls['close']).sum()\n",
        "    net_premium = calls_premium - puts_premium\n",
        "\n",
        "    d_fm['sgn'] = d_fm['contract_type'].map({'call':1,'C':1,'put':-1,'P':-1}).fillna(0)\n",
        "    total_gex_shares = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn']).sum()\n",
        "    total_gex_notional = (d_fm['gamma'] * d_fm['open_interest'] * 100 * d_fm['sgn'] * (spot_price**2)).sum() / 1_000_000_000\n",
        "\n",
        "    vanna_total = (d_fm['vega'] * d_fm['open_interest'] * 100).sum()\n",
        "    charm_total = (d_fm['theta'] * d_fm['open_interest'] * 100).sum()\n",
        "\n",
        "    d_fm['gamma_$'] = d_fm['gamma'] * d_fm['open_interest'] * 100 * spot_price\n",
        "    gamma_by_strike = d_fm.groupby('strike_price')['gamma_$'].sum().sort_index()\n",
        "    cum_gamma = gamma_by_strike.cumsum()\n",
        "    gamma_flip_candidates = cum_gamma[cum_gamma * cum_gamma.shift(fill_value=0) < 0]\n",
        "    gamma_flip_strike = gamma_flip_candidates.index.min() if not gamma_flip_candidates.empty else \"Sin cruce\"\n",
        "    if gamma_flip_strike != \"Sin cruce\":\n",
        "        logger.success(f\"Gamma Flip (front-month): {gamma_flip_strike}\")\n",
        "    else:\n",
        "        logger.warning(\"No se encontró Gamma Flip en front-month.\")\n",
        "\n",
        "    max_pain_strike = d_fm.groupby('strike_price')['open_interest'].sum().idxmax() if not d_fm.empty else np.nan\n",
        "\n",
        "    iv_rank_cs = calculate_iv_rank_crosssection(d_fm['iv'])\n",
        "    iv_skew_simple = (puts['iv'].mean() - calls['iv'].mean()) if (not puts.empty and not calls.empty) else np.nan\n",
        "\n",
        "    put_25d = puts.iloc[(puts['delta'] - (-0.25)).abs().argsort()[:1]] if not puts.empty else pd.DataFrame()\n",
        "    call_25d = calls.iloc[(calls['delta'] - 0.25).abs().argsort()[:1]] if not calls.empty else pd.DataFrame()\n",
        "    iv_skew_25d = np.nan\n",
        "    if not put_25d.empty and not call_25d.empty:\n",
        "        iv_put = put_25d['iv'].iloc[0]; iv_call = call_25d['iv'].iloc[0]\n",
        "        iv_skew_25d = (iv_put if iv_put < 5 else iv_put/100) - (iv_call if iv_call < 5 else iv_call/100)\n",
        "\n",
        "    return {\n",
        "        'Expiry_Used': expiry_used,\n",
        "        'Ratio_Put_Call_Vol': round(put_call_ratio_vol, 2),\n",
        "        'Net_Premium_Notional_$': net_premium * 100,\n",
        "        'Total_GEX_Shares': total_gex_shares,\n",
        "        'Total_GEX_Notional_$B': total_gex_notional,\n",
        "        'Vanna_Exposure_$': vanna_total,\n",
        "        'Charm_Exposure_$': charm_total,\n",
        "        'Gamma_Flip_Level': gamma_flip_strike,\n",
        "        'Max_Pain_Strike': max_pain_strike,\n",
        "        'IV_Rank_CrossSection_%': round(iv_rank_cs, 2) if pd.notna(iv_rank_cs) else np.nan,\n",
        "        'IV_Skew_Simple_Mean': round(iv_skew_simple, 4) if pd.notna(iv_skew_simple) else np.nan,\n",
        "        'IV_Skew_25Delta': round(iv_skew_25d, 4) if pd.notna(iv_skew_25d) else np.nan,\n",
        "    }\n",
        "\n",
        "# Historial para z-scores\n",
        "def load_historical_metrics(ticker, save_dir):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    if f.exists():\n",
        "        df = pd.read_csv(f)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        # Asegurar columnas esperadas\n",
        "        for col in ['iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "        return df\n",
        "    return pd.DataFrame(columns=['date','iv_skew_25delta','avg_vol_off_exchange_pct','backfill_method'])\n",
        "\n",
        "def append_historical_metrics(ticker, save_dir, new_data):\n",
        "    f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "    df = load_historical_metrics(ticker, save_dir)\n",
        "    new_entry = pd.DataFrame([new_data])\n",
        "    new_entry['date'] = pd.to_datetime(new_entry['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce', format='mixed').dt.normalize()\n",
        "        df = df[df['date'].dt.date != new_entry['date'].iloc[0].date()]\n",
        "    df = pd.concat([df, new_entry], ignore_index=True)\n",
        "    df.to_csv(f, index=False)\n",
        "    logger.info(f\"Historial actualizado: {f}\")\n",
        "\n",
        "def maybe_backfill_history(ticker, save_dir, current_skew, current_offx, target_days=10):\n",
        "    dfh = load_historical_metrics(ticker, save_dir)\n",
        "    if len(dfh) >= target_days:\n",
        "        return dfh, None\n",
        "    shortfall = target_days - len(dfh)\n",
        "    logger.warning(f\"Backfill suave del historial: +{shortfall} días (naive).\")\n",
        "    today = datetime.now().date()\n",
        "    rows = []\n",
        "    for i in range(shortfall, 0, -1):\n",
        "        d = today - timedelta(days=i)\n",
        "        rows.append({\n",
        "            'date': d.isoformat(),\n",
        "            'iv_skew_25delta': current_skew,\n",
        "            'avg_vol_off_exchange_pct': current_offx,\n",
        "            'backfill_method': 'naive_repeat'\n",
        "        })\n",
        "    if rows:\n",
        "        df_add = pd.DataFrame(rows)\n",
        "        dfh = pd.concat([dfh, df_add], ignore_index=True)\n",
        "        f = Path(save_dir) / f\"{ticker}_metrics_history.csv\"\n",
        "        dfh.to_csv(f, index=False)\n",
        "        return dfh, 'naive_repeat'\n",
        "    return dfh, None\n",
        "\n",
        "# Whisper / Consensus\n",
        "def calculate_whisper_consensus(df_analyst_est):\n",
        "    if df_analyst_est.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    date_cols = ['publishedDate','date','updatedFromDate','fiscalDateEnding']\n",
        "    estimate_cols = ['estimatedEpsAvg','estimatedEps','epsEstimated','estimate']\n",
        "    consensus_cols = ['consensusEps','consensus','epsAvg','epsMean']\n",
        "    dcol = next((c for c in date_cols if c in df_analyst_est.columns), None)\n",
        "    ecol = next((c for c in estimate_cols if c in df_analyst_est.columns), None)\n",
        "    ccol = next((c for c in consensus_cols if c in df_analyst_est.columns), None)\n",
        "    if not dcol or not ecol:\n",
        "        logger.warning(\"Analyst_Est sin columnas esperadas (fecha o estimate).\")\n",
        "        return np.nan, np.nan, np.nan\n",
        "    df_sorted = df_analyst_est.sort_values(by=dcol, ascending=False)\n",
        "    latest5 = df_sorted.head(5)\n",
        "    whisper = round(pd.to_numeric(latest5[ecol], errors='coerce').dropna().mean(), 4) if not latest5.empty else np.nan\n",
        "    consensus = round(pd.to_numeric(df_sorted[ccol], errors='coerce').dropna().iloc[0], 4) if ccol and not df_sorted.empty else np.nan\n",
        "    diff = round(abs(whisper - consensus), 4) if (pd.notna(whisper) and pd.notna(consensus)) else np.nan\n",
        "    return whisper, consensus, diff\n",
        "\n",
        "def calculate_sbc_penalty(df_cash_flow, df_income_stmt):\n",
        "    if df_cash_flow.empty or df_income_stmt.empty:\n",
        "        return \"Datos insuficientes\", 0\n",
        "    latest_cf = df_cash_flow.sort_values(by='date', ascending=False).iloc[0]\n",
        "    latest_is = df_income_stmt.sort_values(by='date', ascending=False).iloc[0]\n",
        "    sbc = latest_cf.get('stockBasedCompensation', 0)\n",
        "    revenue = latest_is.get('revenue', 0)\n",
        "    if not revenue:\n",
        "        return \"Ingresos son cero\", 0\n",
        "    sbc_ratio = (sbc or 0) / revenue\n",
        "    status = f\"{sbc_ratio:.2%}\"\n",
        "    if sbc_ratio > 0.25:\n",
        "        status += \" (ALTO - Penalización)\"\n",
        "    return status, sbc_ratio\n",
        "\n",
        "def scan_recent_filings_for_guidance(df_news, df_8k):\n",
        "    combined = []\n",
        "    if not df_news.empty:\n",
        "        dcol = next((c for c in ['date','publishedDate'] if c in df_news.columns), None)\n",
        "        ccol = next((c for c in ['text','content','title'] if c in df_news.columns), None)\n",
        "        if dcol and ccol:\n",
        "            combined.append(df_news[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not df_8k.empty:\n",
        "        dcol = next((c for c in ['fillingDate','filingDate','acceptedDate','date'] if c in df_8k.columns), None)\n",
        "        ccol = next((c for c in ['content','text','title'] if c in df_8k.columns), None)\n",
        "        if dcol and ccol:\n",
        "            # FIX: renombrar también 'content' para 8-K\n",
        "            combined.append(df_8k[[dcol,ccol]].rename(columns={dcol:'date', ccol:'content'}))\n",
        "    if not combined:\n",
        "        return \"No hay documentos recientes para analizar.\"\n",
        "    df = pd.concat(combined, ignore_index=True)\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    one_week_ago = datetime.now() - timedelta(days=7)\n",
        "    recent = df[df['date'] >= one_week_ago]\n",
        "    if recent.empty:\n",
        "        return \"No hay documentos en la última semana.\"\n",
        "    keywords = ['guidance','outlook','margin','forecast','expectation']\n",
        "    for _, row in recent.iterrows():\n",
        "        content = str(row.get('content','')).lower()\n",
        "        if any(k in content for k in keywords):\n",
        "            return f\"Guidance/Outlook mencionado el {row['date'].date()}\"\n",
        "    return \"Sin menciones de guidance en la última semana.\"\n",
        "\n",
        "# =========================\n",
        "# Celda 4: Orquestación\n",
        "# =========================\n",
        "def run_analysis_pipeline(tickers, config):\n",
        "    t_start = time.time()\n",
        "    persistent_dir = Path(PERSISTENT_DIR); persistent_dir.mkdir(parents=True, exist_ok=True)\n",
        "    raw_base_dir = persistent_dir / 'raw'; raw_base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # lit exchanges (dinámico con fallback)\n",
        "    lit_ex_ids = fetch_lit_exchanges_polygon()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        logger.info(f\"=== Iniciando {ticker} ===\")\n",
        "        raw_dir = raw_base_dir / ticker; raw_dir.mkdir(exist_ok=True)\n",
        "        output_excel_path = persistent_dir / f\"{ticker}_hyperion_report_{datetime.now():%Y%m%d_%H%M}.xlsx\"\n",
        "\n",
        "        # 1) Extracción base\n",
        "        tasks = _create_api_tasks(ticker, config[\"start_date\"], config[\"end_date\"])\n",
        "        data_sources = {}\n",
        "        for name, (url, params) in tasks.items():\n",
        "            df = fetch_data_block(name, url, params, ticker, raw_dir)\n",
        "            if not df.empty:\n",
        "                data_sources[name] = df\n",
        "\n",
        "        # 2) Dependientes\n",
        "        if '1_Profile' in data_sources and not data_sources['1_Profile'].empty and data_sources['1_Profile'].iloc[0].get('cik'):\n",
        "            cik = data_sources['1_Profile'].iloc[0].get('cik')\n",
        "            logger.info(f\"CIK encontrado: {cik}.\")\n",
        "            cik_url = f\"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}\"\n",
        "            cik_params = {'cik': cik, 'limit': 100}\n",
        "            df_cik = fetch_data_block('43_SEC_Filings_By_CIK', cik_url, cik_params, ticker, raw_dir)\n",
        "            if not df_cik.empty:\n",
        "                data_sources['43_SEC_Filings_By_CIK'] = df_cik\n",
        "        else:\n",
        "            logger.warning(\"Sin CIK; omitiendo búsqueda por CIK.\")\n",
        "\n",
        "        df_transcripts = fetch_historical_transcripts(ticker, raw_dir)\n",
        "        if not df_transcripts.empty:\n",
        "            data_sources['Earnings_Transcripts_Hist'] = df_transcripts\n",
        "\n",
        "        # 3) Snapshot opciones (intradía si aplica)\n",
        "        market_tz = pytz.timezone('America/New_York')\n",
        "        now_market_time = datetime.now(market_tz)\n",
        "        intraday_snapshots = []\n",
        "        df_history = load_historical_metrics(ticker, persistent_dir)\n",
        "\n",
        "        if (now_market_time.weekday() < 5 and now_market_time.hour == 15) or RUN_INTRADAY_TEST:\n",
        "            logger.info(\"Última hora detectada → snapshots intradía.\")\n",
        "            for i in range(4):\n",
        "                snapshot_time = datetime.now(market_tz).strftime('%H:%M:%S ET')\n",
        "                options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "                df_snapshot_raw_intra = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, f'Options_Snapshot_Intra_{i}')\n",
        "                df_options_chain_intra = flatten_options_details(df_snapshot_raw_intra)\n",
        "                if not df_options_chain_intra.empty:\n",
        "                    spot_price_intra = fetch_api_data(f'https://financialmodelingprep.com/api/v3/quote/{ticker}')[0].get('price', 0)\n",
        "                    metrics_intra = calculate_advanced_options_metrics(df_options_chain_intra.copy(), spot_price_intra)\n",
        "                    df_flow_intra, meta_intra = analyze_contracts_flow(df_options_chain_intra.nlargest(20, 'volume'), ticker, raw_dir, lit_ex_ids)\n",
        "                    avg_off_intra = df_flow_intra['Volumen_Off_%'].mean() if not df_flow_intra.empty else np.nan\n",
        "                    intraday_snapshots.append({\n",
        "                        'timestamp': snapshot_time,\n",
        "                        'spot_price': spot_price_intra,\n",
        "                        'iv_skew_25delta': metrics_intra.get('IV_Skew_25Delta'),\n",
        "                        'avg_vol_off_exchange_%': avg_off_intra,\n",
        "                        'session_source': meta_intra.get('session_source')\n",
        "                    })\n",
        "                if i < 3:\n",
        "                    logger.info(\"Esperando 2 minutos para el próximo snapshot...\")\n",
        "                    time.sleep(30)\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw_intra)\n",
        "        else:\n",
        "            logger.info(\"Snapshot de cadena de opciones (una vez).\")\n",
        "            options_snapshot_url = f'https://api.polygon.io/v3/snapshot/options/{ticker}'\n",
        "            df_snapshot_raw = fetch_paginated_data(options_snapshot_url, raw_dir, ticker, 'Options_Snapshot')\n",
        "            df_options_chain = flatten_options_details(df_snapshot_raw)\n",
        "\n",
        "        if not df_options_chain.empty:\n",
        "            data_sources['Options_Chain'] = df_options_chain\n",
        "\n",
        "        # 4) Métricas\n",
        "        metrics_dashboard = {}\n",
        "\n",
        "        if '3_Daily_Bars_5Y' in data_sources and not data_sources['3_Daily_Bars_5Y'].empty and 'c' in data_sources['3_Daily_Bars_5Y'].columns:\n",
        "            prices = data_sources['3_Daily_Bars_5Y']['c']\n",
        "            metrics_dashboard['RV10_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_SHORT)\n",
        "            metrics_dashboard['RV20_Anualizada'] = calculate_realized_volatility(prices, ROLLING_WINDOW_LONG)\n",
        "\n",
        "        # Flujo opciones\n",
        "        avg_off_exchange = np.nan\n",
        "        data_quality = {\"Trades_Total\":0,\"Trades_NoExchange_%\":np.nan,\"Contracts_Analizados\":0,\"Contracts_SinQuotes\":0,\"Session_Source\":\"N/A\"}\n",
        "        if not df_options_chain.empty:\n",
        "            df_options_chain['volume'] = pd.to_numeric(df_options_chain['volume'], errors='coerce').fillna(0)\n",
        "            contracts_per_expiry = 10\n",
        "            top_contracts = (df_options_chain.groupby('expiration_date')\n",
        "                             .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
        "                             .reset_index(drop=True))\n",
        "            if len(top_contracts) > TOP_N_CONTRACTS_OFI:\n",
        "                top_contracts = top_contracts.nlargest(TOP_N_CONTRACTS_OFI, 'volume')\n",
        "\n",
        "            logger.info(f\"Contratos para flujo: {len(top_contracts)}\")\n",
        "            df_flow_analysis, meta = analyze_contracts_flow(top_contracts, ticker, raw_dir, lit_ex_ids)\n",
        "            if not df_flow_analysis.empty:\n",
        "                data_sources['Options_Flow_Analysis'] = df_flow_analysis\n",
        "                avg_off_exchange = df_flow_analysis['Volumen_Off_%'].mean()\n",
        "                metrics_dashboard['Promedio_Vol_Off_Exchange_%'] = avg_off_exchange\n",
        "                data_quality[\"Trades_Total\"] = int(df_flow_analysis['total_volume_trades'].sum())\n",
        "                data_quality[\"Trades_NoExchange_%\"] = round(df_flow_analysis['Volumen_Unknown_%'].mean(), 2)\n",
        "                data_quality[\"Contracts_Analizados\"] = meta[\"contracts_total\"]\n",
        "                data_quality[\"Contracts_SinQuotes\"] = meta[\"contracts_no_quotes\"]\n",
        "                data_quality[\"Session_Source\"] = meta[\"session_source\"]\n",
        "                logger.success(\"Análisis de flujo listo.\")\n",
        "        else:\n",
        "            logger.warning(\"Sin cadena de opciones → flujo no calculado.\")\n",
        "\n",
        "        # Métricas avanzadas (front-month)\n",
        "        spot_price = data_sources.get('2_Quote', pd.DataFrame()).iloc[0].get('price', 0) if '2_Quote' in data_sources and not data_sources['2_Quote'].empty else 0\n",
        "        advanced_metrics = calculate_advanced_options_metrics(df_options_chain.copy(), spot_price) if not df_options_chain.empty else {}\n",
        "        if advanced_metrics:\n",
        "            data_sources['Options_Metrics_Advanced'] = pd.DataFrame([advanced_metrics])\n",
        "            metrics_dashboard.update(advanced_metrics)\n",
        "\n",
        "        # Historial y z-scores\n",
        "        df_history, backfill_method = maybe_backfill_history(ticker, persistent_dir,\n",
        "                                                             metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "                                                             metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'))\n",
        "        historical_shortfall = len(df_history) < 10\n",
        "        if not historical_shortfall:\n",
        "            h30 = df_history.tail(30)\n",
        "            skew_mean = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').mean()\n",
        "            skew_std = pd.to_numeric(h30['iv_skew_25delta'], errors='coerce').std()\n",
        "            off_mean = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').mean()\n",
        "            off_std = pd.to_numeric(h30['avg_vol_off_exchange_pct'], errors='coerce').std()\n",
        "            current_skew = metrics_dashboard.get('IV_Skew_25Delta', skew_mean)\n",
        "            current_off = metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', off_mean)\n",
        "            skew_z = (current_skew - skew_mean)/skew_std if (skew_std and skew_std>0) else np.nan\n",
        "            off_z = (current_off - off_mean)/off_std if (off_std and off_std>0) else np.nan\n",
        "            metrics_dashboard['Skew_ZScore_30D'] = round(skew_z, 2) if pd.notna(skew_z) else np.nan\n",
        "            metrics_dashboard['Off_Exchange_ZScore_30D'] = round(off_z, 2) if pd.notna(off_z) else np.nan\n",
        "        else:\n",
        "            metrics_dashboard['Alerta_Fallback_Umbrales_Fijos'] = True\n",
        "            logger.warning(\"Historial insuficiente → usando umbrales fijos.\")\n",
        "\n",
        "        # Whisper/Consensus\n",
        "        whisper_eps, consensus_eps, eps_diff = calculate_whisper_consensus(data_sources.get('17_Analyst_Est', pd.DataFrame()))\n",
        "        metrics_dashboard['Whisper_EPS_Last_5'] = whisper_eps\n",
        "        metrics_dashboard['Consensus_EPS'] = consensus_eps\n",
        "\n",
        "        # SBC\n",
        "        sbc_status, sbc_ratio = calculate_sbc_penalty(data_sources.get('7_Cash_Flow_Annual', pd.DataFrame()),\n",
        "                                                      data_sources.get('5_Income_Stmt_Annual', pd.DataFrame()))\n",
        "        metrics_dashboard['SBC_vs_Revenue_Ratio'] = sbc_status\n",
        "\n",
        "        # Guidance\n",
        "        guidance_status = scan_recent_filings_for_guidance(data_sources.get('52_News_PR', pd.DataFrame()),\n",
        "                                                           data_sources.get('40_SEC_Search_8K', pd.DataFrame()))\n",
        "        metrics_dashboard['Recent_Guidance_Status'] = guidance_status\n",
        "\n",
        "        # Confianza Global (trazable)\n",
        "        confianza_global = 90\n",
        "        razones = []\n",
        "        if historical_shortfall:\n",
        "            confianza_global -= 5; razones.append(\"Historial<10d\")\n",
        "        if sbc_ratio > 0.25:\n",
        "            confianza_global -= 5; razones.append(\"SBC>25%\")\n",
        "        if pd.notna(eps_diff) and eps_diff < 0.02:\n",
        "            confianza_global -= 5; razones.append(\"Whisper~Consensus\")\n",
        "        is_gex_negative = metrics_dashboard.get('Total_GEX_Notional_$B', 0) < 0\n",
        "        skew_z = metrics_dashboard.get('Skew_ZScore_30D', np.nan)\n",
        "        if pd.notna(skew_z) and skew_z > 2.0 and is_gex_negative:\n",
        "            confianza_global -= 5; razones.append(\"SkewZ>2 & GEX<0\")\n",
        "        metrics_dashboard['Confianza_Global_%'] = confianza_global\n",
        "        metrics_dashboard['Confianza_Razones'] = \", \".join(razones) if razones else \"Base\"\n",
        "\n",
        "        # Put Panic\n",
        "        if historical_shortfall:\n",
        "            is_skew_high = (metrics_dashboard.get('IV_Skew_25Delta', 0) or 0) > 0.025\n",
        "            is_off_high = (metrics_dashboard.get('Promedio_Vol_Off_Exchange_%', 0) or 0) > 50.0\n",
        "        else:\n",
        "            is_skew_high = pd.notna(skew_z) and (skew_z > 2.0)\n",
        "            off_z = metrics_dashboard.get('Off_Exchange_ZScore_30D', np.nan)\n",
        "            is_off_high = pd.notna(off_z) and (off_z > 1.5)\n",
        "        flip = metrics_dashboard.get('Gamma_Flip_Level')\n",
        "        is_below_flip = False\n",
        "        if isinstance(flip, (int,float)) and spot_price:\n",
        "            is_below_flip = spot_price < flip\n",
        "        put_panic_trigger = (is_skew_high and is_gex_negative and is_off_high and is_below_flip)\n",
        "        metrics_dashboard['ALERTA_PUT_PANIC_ADAPTATIVA'] = \"ACTIVADA\" if put_panic_trigger else \"Desactivada\"\n",
        "\n",
        "        # # 5) Guardado Excel\n",
        "        # logger.info(\"Guardando Excel...\")\n",
        "        # with pd.ExcelWriter(output_excel_path, engine=\"openpyxl\") as writer:\n",
        "        #     quote_data = data_sources.get('2_Quote', pd.DataFrame())\n",
        "        #     if not quote_data.empty:\n",
        "        #         q = quote_data.iloc[0]\n",
        "        #         metrics_dashboard['Precio_Actual'] = q.get('price')\n",
        "        #         metrics_dashboard['Cambio_%_Dia'] = q.get('changesPercentage')\n",
        "        #         metrics_dashboard['Capitalizacion_Mercado'] = q.get('marketCap')\n",
        "\n",
        "        #     key_metrics = data_sources.get('8_Key_Metrics_TTM', pd.DataFrame())\n",
        "        #     if not key_metrics.empty:\n",
        "        #         km = key_metrics.iloc[0]\n",
        "        #         metrics_dashboard['PER_TTM'] = km.get('peRatioTTM')\n",
        "        #         metrics_dashboard['Dividend_Yield_TTM'] = km.get('dividendYieldTTM')\n",
        "\n",
        "        #     metrics_dashboard['Session_Source'] = data_quality[\"Session_Source\"]\n",
        "        #     metrics_dashboard['Trades_NoExchange_%'] = data_quality[\"Trades_NoExchange_%\"]\n",
        "        #     metrics_dashboard['Contracts_Analizados'] = data_quality[\"Contracts_Analizados\"]\n",
        "        #     metrics_dashboard['Contracts_SinQuotes'] = data_quality[\"Contracts_SinQuotes\"]\n",
        "\n",
        "        #     df_dashboard = pd.DataFrame.from_dict(metrics_dashboard, orient='index', columns=['Valor'])\n",
        "        #     df_dashboard.to_excel(writer, sheet_name='Dashboard')\n",
        "\n",
        "        #     if intraday_snapshots:\n",
        "        #         pd.DataFrame(intraday_snapshots).to_excel(writer, sheet_name='Intraday_Trend', index=False)\n",
        "\n",
        "        #     for name, df in data_sources.items():\n",
        "        #         safe_sheet_name = name[:31]\n",
        "        #         df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "\n",
        "        # logger.success(f\"Reporte guardado: {output_excel_path}\")\n",
        "\n",
        "        # # 6) Guardar historial y resumen JSON\n",
        "        # today_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        # new_hist = {\n",
        "        #     'date': today_str,\n",
        "        #     'iv_skew_25delta': metrics_dashboard.get('IV_Skew_25Delta'),\n",
        "        #     'avg_vol_off_exchange_pct': metrics_dashboard.get('Promedio_Vol_Off_Exchange_%'),\n",
        "        #     'backfill_method': backfill_method or 'none'\n",
        "        # }\n",
        "        # append_historical_metrics(ticker, persistent_dir, new_hist)\n",
        "\n",
        "        # summary_path = persistent_dir / f\"{ticker}_summary.json\"\n",
        "        # summary_data = {\n",
        "        #     'ticker': ticker,\n",
        "        #     'timestamp': datetime.now().isoformat(),\n",
        "        #     'spot_price': metrics_dashboard.get('Precio_Actual'),\n",
        "        #     'confianza_global_pct': metrics_dashboard.get('Confianza_Global_%'),\n",
        "        #     'confianza_razones': metrics_dashboard.get('Confianza_Razones'),\n",
        "        #     'alerta_put_panic': metrics_dashboard.get('ALERTA_PUT_PANIC_ADAPTATIVA'),\n",
        "        #     'skew_zscore': metrics_dashboard.get('Skew_ZScore_30D'),\n",
        "        #     'off_exchange_zscore': metrics_dashboard.get('Off_Exchange_ZScore_30D'),\n",
        "        #     'whisper_eps': metrics_dashboard.get('Whisper_EPS_Last_5'),\n",
        "        #     'consensus_eps': metrics_dashboard.get('Consensus_EPS'),\n",
        "        #     'sbc_ratio_status': metrics_dashboard.get('SBC_vs_Revenue_Ratio'),\n",
        "        #     'session_source': metrics_dashboard.get('Session_Source')\n",
        "        # }\n",
        "        # with open(summary_path, 'w') as f:\n",
        "        #     json.dump(summary_data, f, indent=2)\n",
        "        # logger.success(f\"Resumen guardado en: {summary_path}\")\n",
        "\n",
        "        # logger.info(f\"=== {ticker} completado ===\")\n",
        "\n",
        "    logger.info(f\"Pipeline terminado en {time.time()-t_start:.2f} s\")\n",
        "\n",
        "# =========================\n",
        "# Celda 5: Entrada principal\n",
        "# =========================\n",
        "def flatten_options_details(df_options_snapshot):\n",
        "    if df_options_snapshot.empty:\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    for item in df_options_snapshot.to_dict(\"records\"):\n",
        "        details = item.get(\"details\", {})\n",
        "        greeks = item.get(\"greeks\", {})\n",
        "        day_data = item.get(\"day\", {})\n",
        "        last_quote = item.get(\"last_quote\", {})\n",
        "        records.append({\n",
        "            \"options_ticker\": details.get(\"ticker\"),\n",
        "            \"expiration_date\": details.get(\"expiration_date\"),\n",
        "            \"strike_price\": details.get(\"strike_price\"),\n",
        "            \"contract_type\": details.get(\"contract_type\"),\n",
        "            \"open_interest\": item.get(\"open_interest\"),\n",
        "            \"iv\": item.get(\"implied_volatility\"),\n",
        "            \"delta\": greeks.get(\"delta\"),\n",
        "            \"gamma\": greeks.get(\"gamma\"),\n",
        "            \"theta\": greeks.get(\"theta\"),\n",
        "            \"vega\": greeks.get(\"vega\"),\n",
        "            \"close\": day_data.get(\"close\"),\n",
        "            \"volume\": day_data.get(\"volume\"),\n",
        "            \"bid\": last_quote.get(\"bid\"),\n",
        "            \"ask\": last_quote.get(\"ask\"),\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PIPELINE_CONFIG = {\n",
        "        'tickers': TICKERS_A_PROCESAR,\n",
        "        'start_date': (datetime.now() - timedelta(days=HISTORICAL_DAYS)).strftime('%Y-%m-%d'),\n",
        "        'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    }\n",
        "    try:\n",
        "        import openpyxl, tenacity, loguru, lxml, pytz\n",
        "    except ModuleNotFoundError:\n",
        "        logger.info(\"Instalando librerías requeridas...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"pandas\", \"openpyxl\", \"requests\", \"tenacity\", \"loguru\", \"lxml\", \"pytz\", \"-q\"])\n",
        "    run_analysis_pipeline(tickers=PIPELINE_CONFIG['tickers'], config=PIPELINE_CONFIG)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-23 05:15:36\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAPI keys cargadas correctamente.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLit exchanges (Polygon): [300, 301, 302, 303, 304, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 320, 322, 323, 325]\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m=== Iniciando CVS ===\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 1_Profile\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:15:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 2_Quote\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 3_Daily_Bars_5Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 4_Previous_Close\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 5_Income_Stmt_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 6_Balance_Sheet_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 7_Cash_Flow_Annual\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 8_Key_Metrics_TTM\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 9_Financial_Growth\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 10_Dividends_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 11_Splits_Hist\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 12_Earnings_Cal\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 13_Institutional_Holders\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 14_Institutional_List\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15_Senate_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 15b_Senate_Trading\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:44\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para 16_House_Disclosure\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 17_Analyst_Est\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:15:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 18_Up_Down\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19_Price_Target\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 19b_Price_Target_Consensus\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21_ESG_Data\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 21b_ESG_Ratings\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 22_COT_Report\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 23_Peers\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 24_Short_Interest\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: SPY_Daily_1Y\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 50_News_Stock\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 51_News_General\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 52_News_PR\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 40_SEC_Search_8K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41_SEC_Search_10K\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 41b_SEC_Search_10Q\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 42_SEC_Search_By_Symbol\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCIK encontrado: 0000064803.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: 43_SEC_Filings_By_CIK\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo transcripciones históricas (últimos 8 trimestres)...\u001b[0m\n",
            "\u001b[32m2025-11-23 05:15:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSnapshot de cadena de opciones (una vez).\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "/tmp/ipython-input-3777174840.py:763: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.nlargest(contracts_per_expiry, 'volume'))\n",
            "\u001b[32m2025-11-23 05:16:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mContratos para flujo: 50\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00065000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00065000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00065000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00079000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00079000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00079000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00050000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251219C00050000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00050000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00076000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:50\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00035000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00040000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:51\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00040000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00040000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00082000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00082000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00080000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00057500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:54\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00057500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00057500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00090000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00090000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS261218P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:56\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS261218P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS261218P00022500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00081000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:58\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260320P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320P00067500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00027500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:59\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00027500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00027500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:16:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128C00076000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:16:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:00\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251128C00076000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128C00076000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205C00080000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00077500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00077000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00077000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00077000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219P00077500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251128P00071000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251128P00071000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251128P00071000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00042500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:07\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00042500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00042500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260320C00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260320C00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00060000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:09\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116P00060000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00060000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116P00075000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116P00075000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116P00075000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260515P00067500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260515P00067500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260515P00067500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00037500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:12\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260116C00037500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00037500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00070000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:13\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00070000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00070000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260618P00055000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS260618P00055000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260618P00055000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115P00022500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115P00022500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115P00022500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260116C00080000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260116C00080000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS270115C00035000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:17\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS270115C00035000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS270115C00035000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251205P00081000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:17\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se obtuvieron datos para trades_O:CVS251205P00081000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251205P00081000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00082500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00082500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251219C00085000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251219C00085000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251219C00085000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS260220P00077500 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS260220P00077500\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS260220P00077500\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212C00083000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212C00083000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212C00083000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFlujo → O:CVS251212P00078000 (RTH_PreviousDay)\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: trades_O:CVS251212P00078000\u001b[0m\n",
            "/tmp/ipython-input-3777174840.py:206: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
            "\u001b[32m2025-11-23 05:17:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExtrayendo: quotes_O:CVS251212P00078000\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:22\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAnálisis de flujo listo.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:22\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[33m\u001b[1mNo se encontró Gamma Flip en front-month.\u001b[0m\n",
            "\u001b[32m2025-11-23 05:17:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mPipeline terminado en 106.16 s\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}